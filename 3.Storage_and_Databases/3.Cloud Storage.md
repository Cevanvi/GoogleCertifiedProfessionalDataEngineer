## Resources

- [GCS Documentation](https://cloud.google.com/storage/docs/)
- [GCS Pricing](https://cloud.google.com/storage/pricing)
- [GCS Overview](https://cloud.google.com/storage/?hl=en)

## Lesson Transcript
```text
Hello, Cloud Gurus,
no matter what you do in the cloud,
you're going to need to store some data in buckets.
To do this in GCP,
we have the aptly named
Google Cloud Storage or GCS.
Here's the service icon for GCS,
which I like to think looks like
two drawers full of socks.
If you're familiar with AWS,
you can compare GCS roughly to S3 buckets.
So what does GCS give you,
Fully managed object storage.
You can store any files you want in GCS buckets,
images, videos, you name it,
and you can access them via an HTTP API,
or SDKs for any popular programming language.
GCS is a fully managed service that you just consume.
You don't need to provision anything
other than creating a logical bucket.
The only storage limits, are the physical
limits of Google's combined data centers.
GCS provides multiple storage classes,
which are a way to save money
on data storage that is infrequently accessed,
but all storage classes have the same
performance class.
You can also set up Automatic
Lifecycle management to move objects
between storage classes as they age.
This is a rock solid GCP service.
So you get secure identity and access management,
high availability and durability,
for your storage buckets.
Let's quickly go over some high-level GCS concepts.
A GCS bucket is a logical container
where you store objects or files.
You create buckets within GCP projects.
Every GCS bucket belongs to a project.
When you name a bucket,
the name has to be unique within a
global namespace, not just within your project.
It's a handy naming convention
to just use the name of your project
as a prefix for all bucket names.
GCS buckets can be Regional, Dual-regional,
or Multi-regional.
Regional is the default, and it's the
lowest cost solution, but it's still highly durable.
Objects written to regional buckets
are written in chunks to
multiple physical devices,
located in at least 2 different
availability zones, in a region.
Regional is a good choice
when the workloads that will consume data
also operate out of a single region.
Dual-region buckets will additionally
chunk this data between 2 different regions,
although they will be in the
same part of the world, roughly.
This gives you a high-availability solution
that would survive an issue isolated
to a single region.
You can also choose multi-regional,
which will use almost all of the
data centers available in a
given geographic region, such as Europe or the US.
This can be helpful if you have workloads
running in many different regions,
but you want the lowest possible latency,
to access your data.
Once you've chosen the availability
of your bucket,
you have 4 choices regarding
each storage class, which should be
based on how long you are going to store data
and how frequently you are going to access it.
Standard is the default storage class
that you will most commonly use.
Storage starts at just
2 cents per gigabyte per month.
And the standard class offers you 99.99%
monthly availability in a regional bucket,
or above this in a dual or multi-regional bucket.
The remaining storage options
are designed for less frequently accessed data.
Starting with Nearline.
This class is designed for data
that will be stored for at least 30 days.
And if you delete an object before this time,
you will still pay the equivalent
storage charge for 30 days worth of that object.
Overall, the storage price is lower though,
half that of standard, starting at just
1 cent per gigabyte per month,
but starting with Nearline,
there is also a data retrieval fee
of 1 cent per gigabyte.
Nearline gives you 99.9% regional availability
and 99.95% availability
in multi and dual-region buckets.
Next, we have Coldline,
where the minimum storage duration is 90 days.
Again, that means if you delete an object,
you still pay a minimum of 90 days
worth of storage for that object.
Coldline storage costs just 0.4 of a cent
per gigabyte per month.
But data retrieval is charged at
2 cents per gigabyte.
Availability levels are the same as Nearline.
And finally, we have the Archive storage class.
Designed for data to be stored for at least
a year at just 0.12, of a cent
per gigabyte per month, but with a more expensive
retrieval fee of 5 cents per gigabyte.
And again, availability is the same
as Coldline and Nearline.
Now as you can see,
you have a bunch of different options,
depending on the type of data you're storing
and how often you're going to access it,
allowing you to choose the right one
for you and hopefully optimize
your costs in the process.
Chances are you will use
Standard class for most cases,
with one of the other options
for backups or other long-term storage.
The nice thing is that each class
behaves exactly the same way in terms of access,
latency, and features.
It's just the costs that are different.
So here's some basics about Objects in Cloud Storage.
Objects are stored as opaque data.
They're just chunks of zeros and ones,
as far as Google is concerned,
and they're encrypted in flight and at rest.
This is useful to know if
you're handling sensitive data.
Although if that's the case,
you still need to be mindful of
security and access control around buckets.
More on that in a moment.
Objects are also immutable,
they can't be changed in place.
To change an object, it has to be overwritten,
and that overwrite is an atomic operation.
So until an upload of a new object is complete,
the object it is overwriting, is still available.
Optionally, objects can also be versioned,
so you can make previous versions available
after overwriting an object.
You can access Google Cloud Storage,
through the web based UI
of the Google Cloud Console,
simple HTTP requests to the API,
a number of different SDKs,
for your favorite programming languages,
or using the gsutil command line tool.
GCS also supports some advanced features.
If you have particularly large objects to upload,
you can split them into smaller pieces.
Then use parallel uploads to upload
multiple pieces at the same time,
which can then be reformed into a
composite single object in the bucket,
once all of the uploads are complete.
A very useful feature of GCS,
is the built-in integrity checking.
You can, pre-calculate an MD5 or
CRC32C hash for an object.
Then provide that hash along with the data
when you upload the object.
GCS will calculate its own hash,
once the upload is complete, and the
operation will only be deemed successful,
If the hash it calculates,
matches the one you supplied.
GCS also supports GZIP Transcoding.
So files can be stored in a compressed form
and decompress during download,
so the consumer receives the original object.
Another useful feature is Requestor pays.
If you have objects that may be useful
to provide to the public,
you can insist that they use their own
billing account to cover the operation,
network, and retrieval charges when
accessing those objects.
So just a quick note on those charges,
which you have to bear in mind
whenever you interact with GCS.
Operation charges apply when you perform
operations in GCS.
These can be things like making changes
or retrieving information,
about a bucket or object.
There are 3 classes of operation.
Class A, class B, and free Operations.
Class A are the expensive operations,
and include uploading data.
Downloading data is a class B operation,
and deleting data is free.
There's a comprehensive list of these
operations and charges, and the GCS documentation.
It's a good idea to read that before
you embark on any work that involves
lots and lots of files.
Network charges also apply
when there is data egress.
In other words, when you retrieve
an object from a bucket.
When accessing an object within
the same location, the egress is free,
but there is a small charge
for moving data out of a bucket
to somewhere in the same continent
and worldwide you will pay
the standard GCP network egress rates.
Additional data retrieval charges will apply when
downloading data from
Nearline or Coldline buckets,
in addition to the charges above.
This is why using the correct
storage class is important.
You can manage storage classes
with Lifecycle Management.
This is a configuration you can apply to a bucket
that contains a set of rules.
GCS itself will periodically check
this configuration, and look for matching rules
to apply to any objects in the bucket.
Lifecycle rules can be used to delete objects,
or set a different storage class for an object
after a certain period of time.
Here's an example,
Lifecycle Management configuration file.
As you can see,
it's a simple JSON file that we can apply
using the G-Cloud command line tool.
Alternatively, we could set these rules
through the Cloud Console UI.
In this example, we have 2 rules.
Each has an action and a condition.
If the condition is true, the action will be applied.
Here the condition is that an object
is at least 30 days old and matches one of these
storage classes, Multi-regional, standard,
or durable-reduced availability.
If this is true for an object,
the action Set Storage Class
will be executed.
Setting the object storage class to Nearline.
Our second rule matches Nearline objects,
and checks to see if they are 365 days or older.
If that's true, their storage class
will be set to Coldline.
Security and Access Control,
are very important considerations
for cloud storage.
Especially if you're dealing
with sensitive data.
You can use Google's IAM roles
to manage access on a broad bucket level,
for a project.
Members, assigned to these roles,
will have them applied across
all buckets and a project.
Alternatively, you can use Access Control Lists
or ACLs to configure more granular access.
If you need to grant temporary access to a file,
you create a signed URL.
This is a publicly accessible URL
that has a specified expiry.
You can also configure Signed policy documents
for your buckets, to be prescriptive over what
kinds of files should be allowed for upload.
Going back to our data pipeline illustration,
you'll generally automate the ingestion of data
in cloud storage, using Pub/Sub.
An event in a bucket, such as an atomic right,
can send a message to a Pub/Sub Topic.
This topic could have Cloud Data flow
as a subscriber, to further process
and transform any data.
Alternatively, you could configure
your own custom Cloud Function to be a
subscriber to this topic.
In this way, cloud storage becomes a data repo.
New files are commits that trigger a
continuous data pipeline.
Okay, Cloud Gurus, you should now know
how to use Cloud Storage as the
unstructured base layer of any Data Pipeline.
I'll see you in the next video.
```

## Resources

- [Cloud Spanner Documentation](https://cloud.google.com/spanner/docs/)
- [CAP Theorem](https://en.wikipedia.org/wiki/CAP_theorem)
- [Distributed database quorums](https://en.wikipedia.org/wiki/Quorum_(distributed_computing))
- [Regional best practices](https://cloud.google.com/spanner/docs/instance-configurations)
- [ANSI SQL best practices](https://cloud.google.com/spanner/docs/sql-best-practices)

## Lesson Transcript
```text
Hello, Cloud Gurus.
Welcome to this video where we'll have an overview
of Google's enterprise SQL database, Cloud Spanner.
So let's get straight into it.
Here's the service icon for Cloud Spanner,
which looks like 3 Spanners glued together.
And there's a clever reason for that,
which we'll come onto in the next slide.
And for the first time,
there's no AWS comparison service dimension,
Cloud Spanner is a leader in its field.
So what makes it so great?
Let's go through the top 3 features of Cloud Spanner.
Spanner is a managed SQL compliant database.
It's not MySQL or Postgres,
so it can't be used as a drop-in replacement
for those engines.
This is a proprietary database system created by Google,
but it is ANSI SQL compliant.
Unlike most SQL databases,
Cloud Spanner is horizontally scalable.
This is normally something you have to use
a NoSQL database for,
but you can horizontally scale nodes in Spanner,
to add more power to your database.
And Spanner is highly available
with the right configuration.
It will replicate your data across multiple regions
and promises of 5 nines SLA.
Now, this all sounds a bit too good to be true, doesn't it?
It certainly used to be too good to be true,
due to something called the CAP theorem.
The CAP theorem formulated by Eric Brewer
states that in a distributed system that stores data,
there are 3 fundamental principles:
consistency, availability, and partition tolerance.
Now, there are quite detailed explanations
of the meanings of these principles in different contexts.
But in a nutshell, consistency means database transactions
must only change data according to specific rules.
And in the context of Spanner,
it refers to the strong consistency
of a right transaction taking effect
before that data can be read from anywhere in the system.
Availability is pretty straightforward.
It means that the system will always be available
to serve queries.
And partition tolerance, means that the distributed system
needs to be tolerant of the loss
of any partition of its total system.
More simply that means the system
needs to tolerate failures
if a part of it goes offline unexpectedly.
In Dr. Brewer's paper, he posits that you can achieve
any 2 out of 3 of these principles.
You have to sacrifice 1 to achieve the other 2.
So, you can have consistency and availability, a CA system,
but you're going to lose the ability
to tolerate the loss of a partition.
You could guarantee consistency and partition tolerance,
a CP system, but you can't keep that system available
100% of the time.
Or you can guarantee availability and partition tolerance,
an AP system, but your data is not going to be consistent.
You may have to put up with an eventual consistency model.
So does Cloud Spanner and break the CAP theorem?
Well, it promises to be strongly consistent
with transactions and it's also highly available.
So in that way, Spanner is most like a CA system.
But in order to tolerate partition loss,
sometimes Spanner will choose consistency over availability,
making it a CP system.
Can it really promise all 3 then?
Well, thanks to Google's private global
super-fast fiber network,
and the way that the Spanner is engineered,
it can provide up to 5 nines of availability.
So maybe that's not 100%,
but for almost all use cases, it's going to be good enough.
5 nines is just over 5 minutes of downtime per year.
So remember that service icon?
Those 3 spanners represent
the 3 principles of the CAP theorem.
At least that's my theorem anyway.
Okay, let's dig into the architecture of Cloud Spanner.
To use Cloud Spanner, you create an instance.
But this isn't an instance as in a single machine,
it's an allocation of resources
that share the same instance configuration.
An instance can contain multiple databases.
An instance configuration mainly just defines 2 things.
Whether the instance is regional or multi-regional,
and we'll look at those differences in a moment.
And the initial number of nodes available
to power the instance.
So first let's have a look
at a regional instance configuration.
Here we have a region with 3 zones.
Cloud Spanner we'll place a read-write replica in each zone.
These replicas are not machines you can connect to,
they're simply part of the architecture
of your Cloud Spanner instance.
You always just connect to the instance
through the Cloud Spanner API.
Now, here's where the nomenclature gets a bit confusing,
or maybe it continues to be confusing.
When you create an instance,
you specify a node count with a minimum of one.
With a node count of one, each replica will be powered
by 1 virtual machine.
If you increase the node count,
you can see that each replica
gets additional virtual machines
to provide additional CPU and RAM.
The number of replicas always stays the same,
but you can scale this node number up and down.
So you can think about grouping these virtual machines
across zones to create a node.
It's helpful to do this because the capacity of replicas
is determined by the node count.
Each node can serve 10,000 queries per second of reads
or 2,000 queries per second of writes
and store up to 2 terabytes of data.
In a regional configuration, replication is achieved
by having 3 read-write replicas.
Every mutation that is every write or change of data,
requires a right quorum of the majority of replicas,
2 out of 3.
If you're not familiar with how replicated databases
use a quorum, I'll put some further reading
in the resources section.
Google recommends some best practices
for regional Spanner instances,
the first one is to design a performant schema.
The idea here is to avoid certain schema design patterns
that conflict with the architecture
of the Spanner system itself.
Remember Spanner is a distributor system,
so you'll get the best performance
by spreading reads and writes around.
If for example you use a primary key
that causes writes to always be sent
to the same end of the table in the database,
you'll get a write hotspot.
Designing a performance schema isn't too complex,
but it could warrant a video of its own.
So instead I'll link to Google's recommendations
in the resources section.
You should co-locate your compute workloads
in the same region as your Spanner instance.
This is just common sense.
For example, if you run your virtual machines
or containers in Europe,
run your Spanner instance in Europe as well.
Google also recommend provisioning enough nodes
to keep their average CPU utilization below 65%.
More than this, and performance will start to degrade.
You can make some back of an envelope calculations
to figure out how many nodes will be required
to achieving this, using the node stats
we saw in the previous diagram,
and you can always add nodes
if CPU utilization gets too high.
Okay, let's look at some multi-regional configurations.
Unlike regional configurations,
these are only available in 4 predefined setups
across specific sets of regions.
The first 1 spans europe-west1 and europe-west4.
With read-write replica's in europe-westone, B and C,
and europe-west4, A and B.
europe-west1 is designated
as the leader region for replication.
It's a good idea to co-located anything
that's going to be write intensive,
in the same region as the leader.
This multi-regional configuration
is referred to as the EUR3 configuration.
Next, we have a similar configuration
across us-east4 and us-east1.
With read-write replicas in us-east4-A,
us-east4-B, us-east1-B and us-east1-C.
us-east4 is the leader region.
This is called the NAM3 configuration.
Okay, now it gets a little more complex.
We start with read-write replicas in 2 different zones
inside each of us-central1 and us-central2.
Wait a second, us-central2,
that's not in the GCP website,
the Spanner configuration was it refers to this
as a private region.
I'm not entirely sure what that means in Google terms,
but they certainly don't advertise where it is.
In any case, this configuration also adds,
read only replicas in europe-west1 and Asia east one.
This setup is called the Nam EUR Asia1 configuration.
Then our final option is to span us-centralone
and us-east1 with read-write replica's
with only a single read only replica
in each of US west 1 and US west 2.
This is called the NAM6 configuration.
You might think it's odd
to have such prescriptive configurations,
but this is based on where the underlying architecture
of Cloud Spanner is available in Google's data centers.
Which 1 you use, will depend on where you need
read and write capacity and how much money
you want to spend on nodes.
If you can afford it,
there are benefits to using a multi-regional configuration.
This is basically how you qualify
for the Spanner 5 nines SLA,
which will financially compensate you
if availability dips below 99.999%.
Multi-regional setups also very useful
if you need to distribute data globally,
but have it served from specific regions
with the lowest possible latency.
Maybe you have compute workloads in the US and Europe
to serve different customers.
Using the Nam, EUR Asia1 configuration
would mean that your application in Europe
could access a local read-only replica
rather than jump all the way across the Atlantic
every time it needs to access data.
Finally, multi-regional configurations
provide something called external consistency.
This is concurrency control for transactions,
that's even stricter than strong consistency.
Guaranteeing that transactions are executed sequentially,
even across replicas in different parts of the world.
This is essential for things like
financial transactions in banks.
Replication in multi-regional setups
varies upon the specific configuration you've chosen.
But similar to regional replication,
every mutation to the database requires a right quorum.
That quorum must contain 1 replica from the leader region
and any 2 additional voting replica's,
which are any other read-write replicas
in the configuration.
In terms of best practices,
again, Google recommend designing
a performance schema to avoid hotspots.
Even if your instance spans multiple regions,
you should think about putting
the write-heavy parts of your application
in the same region as the leader.
But you can also spread compute workloads
across multiple regions for maximum availability.
Due to the extra overhead of running multiple regions,
Google recommend you keep your average
CPU utilization below 45%.
Running advanced setups like this needs a lot of horsepower.
So, enough about the architecture,
we're finally ready to talk about
the actual data model of Cloud Spanner.
As an ANSI compliant SQL database,
Spanner, as you'd expect uses relational database tables
made up of rows, columns, values, and primary keys.
Data in Spanner is strongly typed
and like most equal databases,
you must conform to a strict schema.
But in addition to the standard representation
of SQL tables, Cloud Spanner also offers
parent-child relationships.
Where rows in different tables, can be physically located
on the same Spanner node,
to make retrieval much more efficient.
You can declare these relationships with primary keys
and create an interleave table.
For example, you could have a table called ship
and an interleave table called crew,
defined as a child of the ship table,
reads across these tables to search
for the crew of a specific ship
will now become much more efficient.
There are a few types of transaction
supported by Cloud Spanner.
A locking read-write is the only kind of transaction
that will support writes to the database.
This is because any data read as part of the transaction
will also be locked during the write.
This design decision is again particularly useful
in financial use cases.
Read-only transactions are less prescriptive
and can be handled by any replica,
but still guarantee consistency.
They are not locking.
Spanner also offers transactions written
in partitioned DML or data manipulation language,
which is specifically designed for bulk updates and deletes.
Regular transactions are performed
either through a client library or using ANSI SQL.
This can be a little different
from flavors of SQL you're used to,
so I'll link to Google's best practices guide for this
in the resources section as well.
Finally, as I mentioned before,
you can connect to Cloud Spanner through its public API.
You can do this from a compute engine instance, for example,
using a client library for any popular programming language
or even third cloud function.
If you are setting up a data pipeline,
there is also a cloud data flow connector available.
Okay Cloud Gurus, you should now have an appreciation
of the power of Cloud Spanner
and where it would be a good fit.
I'll see you in the next video.

```

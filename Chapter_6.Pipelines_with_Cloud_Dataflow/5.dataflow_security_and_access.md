## Lesson Transcript
```text
Hey Cloud Grus.
In this video,
we'll be looking at some of the aspects relating
to security and access management for Cloud Dataflow.
This section is not only important for the exam,
but it's also good for you to know
when building real-world pipelines.
We'll be covering the different service accounts
that are involved in pipeline processing.
And we'll see how they are used
by the Cloud Dataflow service.
Let's get going.
So as we discussed previously,
Cloud Dataflow pipelines can be run in different ways.
Firstly, they can be run locally on your machine.
Generally, you'd run the pipelines locally
on small data sets in order to test your driver program.
In order to do this,
you'll need to install the Apache Beam SDK
on your local machine.
There's a start guide that is linked in the resources.
You can also submit your pipeline
to the GCP Cloud Dataflow managed service for processing.
You generally do this after testing on your local machine.
Once everything is successful,
you can start processing on larger data sets.
This submitted pipeline will become a Cloud Dataflow job.
When the pipeline is submitted
to Cloud Dataflow managed service,
service accounts are used to manage security
and permissions.
The Cloud Dataflow service itself uses
the Dataflow service account.
The worker instances created by the Cloud Dataflow service
will use the Controller service account.
We'll cover these service accounts in more detail shortly.
Let's consider the Cloud Dataflow managed service
at a high level.
Your driver program defines your pipeline,
as we said earlier,
the pipeline is submitted to GCP Cloud Dataflow service
for processing.
The Cloud Dataflow service will create
a Cloud Dataflow job.
The job creates and manages workers
that carry out various tasks
for executing the pipeline.
As part of the execution of the pipeline,
the workers need access to various resources
such as files stored on Cloud storage.
The Cloud Dataflow job can be monitored using
the Cloud Dataflow Monitoring Interface
or the Command-line Interface.
The Cloud Dataflow service account is used
by the Cloud Dataflow service.
This service account is automatically created
when a Cloud Dataflow project is created.
The Cloud Dataflow service account manages services
on your behalf.
An example of this would be the addition,
and removal of worker VMs during the processing
of your pipeline.
The Cloud Dataflow service account assumes
the Cloud Dataflow service agent role.
This provides the required set of permissions
to run the Cloud Dataflow jobs.
The Cloud Dataflow service account
has read and write access to project resources.
It is recommended that you don't change the permissions
for the Cloud Dataflow service accounts.
The Cloud Dataflow service account is named as follows
with your project number being inserted.
The Controller service account is used by the workers.
By default, the workers will use your project's
Compute Engine service account,
as the Controller service account.
The Compute Engine service account is automatically created
when you enable the Compute Engine API for your project.
Compute Engine instances execute your pipeline operations.
And as we said, the workers will use your project's
Controller service account to access pipeline files
and other resources.
The Controller service account is also used
for Metadata operations.
These operations that don't run on the local clients
or on the Compute Engine workers.
An example of a Metadata operation,
would be determining the size of a file
within Cloud storage.
You're also able to specify
a User-managed controller service account,
which would replace the Cloud Engine service account.
This allows you to create and use resources
with fine grained access control.
The Controller service account is named as follows
with your project number inserted.
The Cloud Dataflow service,
employs a number of security mechanisms
to ensure the safety and privacy of data.
Firstly, only users with the requisite permissions
are able to submit pipelines for processing.
A number of these security mechanisms also apply
during the evaluation of the pipeline.
Temporary data,
which is created during the evaluation of a pipeline
is encrypted.
It is also not persisted beyond
the evaluation of the pipeline.
All communication between the workers occurs over
a private network and it is therefore subject
to all Projects, Permissions, and Firewall rules.
You're able to specify the region and zone
in which the individual Compute Engine instances run.
Data being processed by the Compute Engine instances
does not leave the specified zone.
Access to telemetry or metrics is controlled
by your project's
permissions. Telemetry and metrics data
are also encrypted at rest.
You're able to use specific Cloud Dataflow
IAM roles to control access for users
to Cloud Dataflow resources.
Okay, Gurus,
that's it for the video on Dataflow security and access.
See you in the next video.

```

## Resources 

- [Feature engineering](https://developers.google.com/machine-learning/crash-course/categorical-data/one-hot-encoding) 
- [Cleaning data](https://developers.google.com/machine-learning/crash-course/numerical-data/normalization) 


## Lesson Transcript
```text
Well, hello again Cloud Gurus.
it's been found that data scientists spend
only 20% of their time on actual analysis
while spending the remaining 80% of time
finding, cleaning, and preparing data.
Of course, this is only an approximation.
Well, that sucks for data scientists
but being a data engineer,
that should be music to your ears.
In this chapter,
we'll be looking at some aspects of cleaning
and preparing data.
As with most things, this topic is deep.
So we'll need to focus on a few important aspects.
We'll start by looking at how we handle missing data
and then we'll look at some other aspects
of preparing your data for machine learning.
Let's get going.
So we start with raw data.
Raw data is the form the data takes as it is delivered
from various source systems.
This could be from on-prem systems,
other cloud environments or from SAS-based solutions.
Data in its raw state is almost never fit
for machine learning or other advanced analytics.
This data needs to be processed so that it can be used
in our key datasets: the training data, the validation data
and the test data.
As we have seen in previous videos,
these datasets are then used to train our model.
The process of preparing the data
involves 2 important steps:
constructing the dataset and performing feature engineering.
To construct the dataset
which is prior to our feature engineering,
we need to collect the raw data, identify features
and label sources
and which features and labels we want to use.
We need to select a sampling strategy
and this is the case where we have so much data
that we need to select a subset of that data
for our training.
And of course, we need to split the data.
All of this is very much relative
to the specific machine problem we're trying to solve.
So a label in one scenario may be a feature in another.
Then we have feature engineering.
This is what the rest of this chapter will be about.
Feature engineering involves transforming our data
in some way
so that it is fit for machine learning.
Let's consider a dataset with 3 features.
As you can see, some of the data is missing.
Now, there are different ways or strategies
of dealing with missing data.
And the first one is simply to ignore it.
This is not always a good idea
but it can be possible in some scenarios.
The second approach is simply to remove lines
that have missing data.
And a third approach is to impute the values
of the missing data.
In this example as you can see,
I've removed some lines and imputed the values of others.
This could be for example
because one of our features is critical.
So if one is missing, we simply cannot use it.
So the row has to be removed.
Whereas we can use other methods to impute the values
of missing values in other feature columns.
Imputation is effectively the process of assigning a value
when none was present initially.
We can classify missing data into 3 broad categories.
First we have missing completely at random or MCAR.
In this case,
the missing data has nothing to do
with its hypothetical value or with the value of other data
in our set.
In this case, missing data are simply a random subset
of our full dataset.
Next we have missing at random or MAR.
In this case,
the missing data is not related to the hypothetical value
of the data
but it is related to some of the other data
that we have available that is other observed to data.
So again, the fact that it is missing
does have to do with the values of other variables.
And finally, we have not missing at random or NMAR.
In this case, the data is not randomly missing.
So there is some cause for the data missing.
Here we would say that the missing values are not related
to the values of other variables.
The missing values are related to some other mechanism
and this mechanism is not visible to us.
So let's look at some ways that we can impute the values
of missing data.
First you would calculate the mean or median values
of non-missing values within that column
and then replace the missing values
with those calculated values.
Of course, this can only be used with numeric data.
Another approach is to replace missing values
with fixed constants.
A common choice for numerical data is 0 for example.
You could also replace missing values
with the most frequent values that occur within the column.
This works well for categorical data that is missing.
We can also use machine learning to impute the values
of missing data.
One approach is K-Nearest Neighbors.
And here, we use the K-Nearest Neighbors algorithm
to predict the value of the missing data.
Another approach which also uses machine learning
is deep learning.
This approach works really well with categorical
as well as with non-numerical data.
So that's it for handling missing data.
Let's look at some of the other things that we can do
with our features.
You may have a dataset that looks similar to this
which is nicely structured.
In some cases however, you will have extreme outliers.
Including these data points within our analysis
can skew our results.
One approach is to clip or remove values
that are outside of some specified range.
So for example, if a value is over 1000
in a particular range,
then we would remove it.
You generally apply clipping before moving on
to other types of feature engineering.
Let's now look at one-hot encoding
which is applied to categorical data.
Categorical data are datasets which can only take on
a predefined set of values.
So for example for a test,
the values may range from A to F.
Remember, machine learning is mathematical in nature.
And so in most cases,
we only want to pass numeric values
to the machine learning algorithm.
To do this, we can transform our categorical data
to numerical values.
In this case,
we have a column for each possible value
that the feature could take on,
so for all the categorical values.
In the type over here, we have A, B, and C
and on the right-hand side, you can see we have type A,
type B, and type C.
For our first row, we have a B.
We transform that B by placing a one in the type B column.
We place a 0 in the type A and type C columns.
Likewise for type C,
we place a 1 in the type C column
and 0s in the type A and type B columns.
Let's now consider linear scaling.
Linear scaling is where we transform values across one range
into values in another.
In most cases, we'll scale values to the range
1 to 1 or 0 to 1.
In this case, we have values ranging from 0 to 255.
We can transform that range to the range 0, 1
simply by dividing each value by 255.
There is an equation that we use full linear scaling.
So X prime is equal to X minus our minimum value of X
divided by a maximum value of X
minus our minimum value of X.
This transformation of values which lie in the range
0 to 255 to the range 0 to 1
is very useful when we are doing machine learning on images
because pixel values can range from 0 to 255.
Another approach to transformation is using the Z-score.
Suppose we have these data points
which have an average value of Mu.
We can transform these values so that they are clustered
around 0.
What I've tried to show here is that the basic structure
of the data remains unchanged.
The way that we transform each value is as follows:
X prime is equal to X minus Mu
which is the average of our data
divided by Sigma.
Here Sigma denotes the standard deviation
which needs to be calculated from our entire dataset.
Let's now take a look at log scaling.
Log scaling is useful where a small number of values
have many points
while the vast majority have few points.
This gives a distribution very similar to the one
that we can see here.
Distributions like this can be challenging to work with
in machine learning.
So we are able to transform the distribution in such a way
that it looks more like this
and the equation for that
is simply to create the X prime values
as the log of the original values.
Finally, let's consider bucketing.
This is useful where we have a distribution
where the data points are related to each other
but that relationship is not linear.
Bucketing essentially involves
creating a number of defined ranges.
All data points within each range are then assigned
to a single data point.
This would result in a distribution
that looks something like this for example.
Here we've taken a continuous distribution
and discretized it.
We have in effect created a categorical numerical variable
from what was a continuous numerical variable.
Well, that's it for feature processing and engineering.
I hope you've enjoyed this video.
See you in the next video.
```

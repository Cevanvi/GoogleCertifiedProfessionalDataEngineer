## Resources 

- [Hyperparameters and parameters](https://machinelearningmastery.com/difference-between-a-parameter-and-a-hyperparameter/) 
- [Hyperparameter tuning](https://jrodthoughts.medium.com/knowledge-tuning-hyperparameters-in-machine-learning-algorithms-part-i-67a31b1f7c88) 


## Lesson Transcript
```text
Well, hello, Cloud Gurus.
In this short video,
we're going to take a look at hyperparameters.
Hyperparameters are a big deal in machine learning
because they have a large effect,
not only on how good or bad our models are,
but they also impact the training process itself.
We'll look at how hyperparameters
fit into the overall learning process.
We'll then discuss a few key characteristics
of hyperparameters, and then we'll finish
by taking a look at a few example hyperparameters.
Let's get going.
So how do hyperparameters fit
into the overall learning process?
We know that we need data in order to train our models.
And of course, we also need a model to train.
These are 2 key inputs to the training process.
The output of the training process is a trained model.
However, in addition to the data and the model,
there is a third component
that goes into the training process.
This is the set of hyperparameters.
Hyperparameters are values that need to be selected
before the training process can begin.
Let's look at a few characteristics of hyperparameters.
So firstly, as I just mentioned,
a hyperparameter is a value that needs to be specified
before training can begin.
It can however be modified during training.
This is in contrast
to the normal machine learning parameters
that are derived or learned as part of the training process.
There are 2 basic types of hyperparameters.
So some hyperparameters are directly linked
to the model that is selected,
and these have a direct impact
on the performance of that model;
that is, how accurate it is.
These are the model hyperparameters.
On the other hand, there are algorithm hyperparameters,
and these do not affect directly
the performance of the algorithm,
but affect the efficiency and the rate of model training.
An example of this would be the learning rate.
In most cases, hyperparameters cannot be learned
using gradient-based methods, that is, using an approach
similar to the gradient descent approach
that we discussed earlier.
There are however heuristics, or rules of thumb,
that can be used in the selection of hyperparameter values.
The process of tuning hyperparameters
involves changing the hyperparameter values
and attempting to find those that yield the best results.
Some platforms allow for hyperparameter tuning,
where the space of possible values
is explored with respect to model performance.
Let's consider a few common hyperparameters.
Firstly, we have the batch size.
The batch size is the number of samples that are processed
during training, before the model is updated.
Batch size is an example of an algorithm hyperparameter.
Next, we have the number of training epochs.
This refers to the number of times
that we run through the full set of training data
while training our model.
Another example of a hyperparameter
is the number of hidden layers in a neural network.
This is an example of a model hyperparameter.
We also have the regularization type,
which could be L1 or L2, as we discussed earlier.
The regularization rate
is another example of a hyperparameter.
A final example is the learning rate.
The learning rate is an algorithm hyperparameter.
Remember, the learning rate is the distance we travel
when trying to find the optimal value for a parameter.
Okay, Cloud Gurus,
that's it for the video on hyperparameters.
See you in the next video.
```

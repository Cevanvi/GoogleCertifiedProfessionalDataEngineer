## Resources 

- [Monitoring a Cloud Bigtable Instance](https://cloud.google.com/bigtable/docs/monitoring-instance) 

## Lesson Transcript
```text
Hello Cloud Gurus,
we've now got a solid grounding in Bigtable,
and it's time to start looking at some advanced topics.
We've talked a lot about designing
the most performant database from the outset,
but data changes over time.
So, monitoring your Bigtable instances is essential.
You can monitor Bigtable using the built-in dashboards
in the GCP console or using numerous metrics in Stackdriver
from which you can drive your own graphs or alerts.
The key things to look out for in your clusters,
are the overall average CPU utilization of the cluster
and the average CPU utilization of the hottest node.
If there's a big disparity between these 2 metrics,
you likely have a hotspot somewhere in your table.
CPU overhead varies depending
on your instance configuration.
For a single cluster instance,
you should aim for an average CPU load of 70%
with the hottest node in your cluster,
not going over 90%.
For instances with 2 clusters and replication,
multi-cluster routing introduces additional overhead.
So, these figures drop to 35% and 45% respectively.
For instances with more than 3 clusters,
you'll need to refer to the documentation
for specific tuning advice and metrics.
I'll link to these docs in the resources section.
While monitoring,
you should also keep an eye on storage utilization,
which you should aim to keep below 70% per node.
You can go up to the hard limit of the storage capacity
of each node.
However, you can't go past it.
And if you try to, writes will fail.
To assist in monitoring application performance
for Bigtable,
you should isolate applications
with their own application profile.
So you can then observe an application's own charts
for throughputs and error rates.
One app profile per application is enough though.
Don't be tempted to use unnecessary extra profiles.
Generally, when your monitoring tells you
that your cluster needs more capacity,
you scale up the number of nodes manually.
You can also do this in advance if you know a big task
is on the horizon.
But it's also possible to use Stackdriver metrics
for programmatic scaling.
Now, this isn't a built in feature of Bigtable.
It requires running your own software
that will use the Bigtable client libraries
to query the current metrics
and then update the cluster node count via the API.
Bear in mind that this sort of retroactive action
won't immediately alleviate load.
Rebalancing of tablets across new nodes takes time.
So, cluster performance won't improve for around 20 minutes.
Adding nodes will only scale capacity
if your schema is well balanced.
If you're using a bad schema with row keys
that are likely to cause hotspots,
those hotspots will remain
regardless of how many nodes you have.
Next up, a few quick notes on replication.
When you add an additional cluster to an instance,
replication is started automatically
and data will be synchronized between the clusters.
In Bigtable, every cluster is a primary with equal weight,
which means that replication is eventually consistent.
Google's documentation says that changes to data
will replicate typically within a few seconds or minutes,
not hours.
But if you're writing large amounts of data
or your clusters are geographically quite distant
from each other,
it can take quite a while for data to catch up.
With multi cluster routing,
you can't assume that you're always reading the latest value
that has been written.
You may want to limit your application profile
to single cluster routing if this is going to be a problem.
Replication is typically used for availability
and fail-over.
If a cluster becomes unhealthy,
connections can automatically be failed over
to an alternative cluster.
If your app profile uses single cluster routing,
you will need to make this update manually.
Application isolation,
where perhaps you want to limit specific applications
to their own clusters based on demand or use case,
and global presence,
where your application may exist in multiple regions
around the world,
but still needs low latency access to its data.
To get the best performance out of Bigtable,
schema and row key design are essential.
I'm sure you know this by now,
as I've mentioned it a few times already.
But you also need to make sure that the workload
is appropriate for Bigtable.
It's designed for large datasets.
Data that totals less than 300 gigabytes,
is probably not going to see the benefit
of a system like Bigtable.
Likewise, data that is short-lived,
won't see the benefit either,
as if the database isn't around very long,
it won't experience any of the intelligent rebalancing
the Bigtable can do.
Correct sizing of rows and columns is also important
to make the most efficient queries possible.
Now you understand the concept of row keys
and how queries work.
Just think about how much data has to be read
for every query.
And this will help you optimize your design of columns
and column families.
When using multiple clusters,
bear in mind that replication can improve read throughput,
but has no bearing on write throughput.
Adding clusters,
doesn't add write capacity
because the additional cluster has to write
just as much data as its predecessor.
Where possible,
use of batch writes with bulk data for rows
that are close together.
If your data is roughly sorted,
inserting data this way,
will be more efficient than row by row.
If it's not sorted,
let Bigtable do that hard work for you.
And to maintain the best performance,
monitor your instances as we've described
and use the Key Visualizer tool to detect hotspots
and bad row keys.
Finally, as we've mentioned before,
Bigtable rebalances tablets,
the sorted shards of table data
across the nodes in a cluster.
It does this behind the scenes
to make your instance as performant as possible.
At first, tablets are handled by the first node
in the cluster.
But as the size of the table grows,
the tablets are continually rebalanced
to make the most efficient use
of the entire clusters resources.
Not shown in this visualization,
because my PowerPoint skills just aren't up to it.
Is that, these tablets themselves,
or also being split, merged, and rebalanced to maintain
the sorted order of rows.
Bigtable uses an intelligent algorithm to do this,
to keep the table as performant as possible.
When a hotspot occurs and sometimes they are unavoidable,
they will consume more CPU on a node
than the average tablet.
In this case,
Bigtable may rebalance tablets to other nodes
leaving only the performance impacting tablets
on a single node
so that they have more CPU available to them.
When this happens,
if you can't solve the hotspot,
you need to bear in mind,
that the total storage capacity of your nodes
may be affected as now some may be processing
more than their fair share of storage.
Okay Cloud Gurus,
I think it's fair to say that you are now Bigtable experts.
If you have any questions about this video,
please let me know in the course forums.
I'll see you in the next video.

```

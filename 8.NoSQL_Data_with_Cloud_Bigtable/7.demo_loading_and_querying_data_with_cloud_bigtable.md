## Resources

- [Cloud Bigtable examples](https://github.com/ACloudGuru-Resources/Course_Google_Certified_Professional_Data_Engineer)
- [Forest Fires dataset](https://archive.ics.uci.edu/dataset/162/forest+fires)
- [Fire Weather Index (FWI) System](https://www.nwcg.gov/publications/pms437/cffdrs/fire-weather-index-system)
- [UCI Machine Learning Repository](https://archive.ics.uci.edu/datasets)
- [forestfires.csv](resources%2Fforestfires.csv)
- [dataloader.py](resources%2Fdataloader.py)

## Cloud Shell Commands

```bash
sudo apt remove java* -y

sudo bash -c 'cat << EOF >> /etc/apt/sources.list
deb http://archive.debian.org/debian/ stretch main contrib non-free
deb http://archive.debian.org/debian/ stretch-proposed-updates main contrib non-free
deb http://archive.debian.org/debian-security stretch/updates main contrib non-free
EOF'

sudo apt-get update

sudo apt-get install openjdk-8-jdk-headless -y

java -version

echo project = YOUR_PROJECT_NAME > ~/.cbtrc
echo instance = labinstance >> ~/.cbtrc

cat ~/.cbtrc

cbt listinstances

cbt createtable fires
cbt createfamily fires fwi
cbt createfamily fires metric

cbt ls fires

# Upload dataloader.py & forestfires.csv files to Google Cloud Shell

curl https://raw.githubusercontent.com/ACloudGuru-Resources/Course_Google_Certified_Professional_Data_Engineer/master/Demos/dataloader.py -O

curl https://raw.githubusercontent.com/ACloudGuru-Resources/Course_Google_Certified_Professional_Data_Engineer/master/Demos/forestfires.csv -O

vim dataloader.py

# Update your project name:
# project_name = 'YOUR_PROJECT_NAME'

sudo pip3 install google-cloud-bigtable

python3 dataloader.py

git clone https://github.com/ACloudGuru-Resources/Course_Google_Certified_Professional_Data_Engineer.git cloud-bigtable-examples

cd cloud-bigtable-examples/quickstart/

./quickstart.sh

scan 'fires'

scan 'fires', {ROWPREFIXFILTER => '2#2#', COLUMNS => 'metric:area'}

scan 'fires', {ROWPREFIXFILTER => '2#2#aug#', COLUMNS => 'metric:area'}

scan 'fires', {ROWPREFIXFILTER => '2#2#aug#', COLUMNS => 'metric'}
```

## Lesson Transcript

```text
Hello, Cloud Gurus,
and welcome to this lab
where we're going to look at loading and querying data
with Cloud Bigtable.
The objectives of this lab are to design a Bigtable schema
and row key convention that will give us the maximum benefit
for the type of query we need to run.
We'll use what we've learned
about row key and schema design and query planning
to help us do this.
We're going to pass a data set that currently exists
as a CSV file into a Bigtable instance.
Then we're going to query that data using the HBase shell.
Now, it's surprisingly difficult to find data sets
that make for good tutorial labs, Cloud Gurus.
We can generate a dozen hello world rows ourselves
or we can find a giant data set
about every ant in the world.
There's not a lot in between,
but thankfully,
I did find a manageable data set we can play with
in a development instance.
We're going to be using a small data set,
just over 500 rows, that tracked forest fires
in northeast Portugal in 2007.
This dataset comes from the UCI Machine Learning Repository,
which you can learn more about through the links
in the resources section.
The data was originally constructed
to help create a learning model
that could predict the burned areas of forest fires.
It was a particularly difficult regression task
as the dataset was so small,
but it's a good size for us to play with.
The original attributes of the dataset were X and Y
which represented a spatial grid across the Montesinho Park
in northeast Portugal,
numbered one to nine across the X axis
and two to nine across the Y axis.
The month and day of the fire were also logged.
The next four columns are all attributes
of the Forest Fire Weather Index,
a system created in Canada and France in the 1970s
which mainly tracks moisture of forest litter
and decomposed organic material.
The remaining columns are simple attributes:
temperature, relative humidity, wind speed, outside rain,
and the total burnt area of forest.
To translate this into a good Bigtable schema,
we need to consider what kind of queries we'll be running.
Let's assume we want to be able
to retrieve the Fire Weather Index data and other metrics
for each recorded fire for a particular cross-section
of the forest.
This could be the start of some analysis
into why certain areas are more prone to fire, for example.
In our Bigtable schema,
we'll use field promotion on the X and Y axis
as well as the month and day.
This way we can use selective row prefixes
in our queries to get all the data for a particular region
and narrow it down by the month or the year.
Once again, to follow along with this lab,
all you need is a GCP project.
We'll do all our work in the Cloud console
and the Cloud shell.
The code you need
and a link to download the CSV file can be found
in the resources section.
Once you've grabbed those files, join me in the GCP console.
So in the GCP console,
the first thing to do is create a Bigtable instance.
We'll select Bigtable from the GCP menu.
Then click create instance.
Let's call this lab instance.
This name will be referenced in our code.
So while you can call this whatever you like,
it'll be easier to leave it as lab instance.
We'll change the instance type to development,
scroll down to the cluster settings,
and set the region and zone to somewhere near you.
The hourly cost should be around about a dollar
and we should be done within the hour.
So let's click create.
And within a few seconds,
our Bigtable instance is ready to go.
So let's open the Cloud shell.
I'll just disregard the survey for now.
I'm going to full screen the Cloud shell
and I'll zoom in a bit to make this easier to read.
You may already have the Google Cloud Bigtable examples repo
cloned in your Cloud shell from the previous lab.
If you don't,
grab the link from the resources section
and clone this repo.
We'll use it in a minute to spin up the HBase shell for us.
Okay, so we've got our Bigtable instance up and running.
The first thing we need to do is create a table
and our column families.
We'll use Google's CBT tool to do this.
To set up the CBT tool,
we need to create a local resource file to configure it.
We can do this very simply with a couple of echo commands.
First, we'll echo our project ID.
Don't forget to use your project here, not mine.
Then we'll use Echo to write the name
of the lab instance to the same file.
Your .cbtrc file should look roughly like this,
although with your project name instead of mine,
and we should now be able to access our instance with CBT.
Let's clear the screen and we'll test this.
Let's try listing our instances with cbt listinstances.
Okay, great.
Now let's create our table with cbt createtable
and the name of the table name, which we'll call fires.
Next, we'll create our two column families
with cbt createfamily,
the name of the table, and the name of the family.
FYI for the fire weather index.
Hit the up arrow
and change the column family name to metric
where we'll store all the other metrics.
Note that we don't have to create our individual columns.
They simply exist or they don't exist
when we write our row data.
We can test that we've successfully created
our column families by listing the table
with cbt ls fires.
Now that that's done,
let's upload the two files that we need to our Cloud shell.
These can be found in the resources section.
First, we'll upload the dataloader.py script.
Then the forestfires.csv file.
Let's quickly edit the data loader script.
I'm going to do this in Vim
but feel free to use the Cloud cell editor if you prefer.
On the fifth and sixth lines here,
just make sure you have the correct names
for your project and your Bigtable instance.
If you try to use my project,
you'll get a permission denied error.
Further down the script,
we can see that it simply connects a client to Bigtable
then iterates through the CSV file.
For every line in the CSV file,
we create a row key based on the attributes
we described earlier, X, Y, month, and day.
Now, as it turns out, this is the day of the week,
not the day of the month in this data set
so we may end up with duplicate row keys.
To get around this, we add a random eight character string
to the end of every row key.
Then we populate the cells with the remaining data
in the appropriate column families and columns.
As you can see in the last line, in the script,
this data is transformed in a local array
and then sent to Bigtable itself
in a single batch transaction.
Okay, let's quit out of our editor.
Let's just check that the CSV file is
in the same location as our script.
There it is.
Before we can run the script,
let's just check we have the necessary Python library
for Cloud Bigtable.
This may already be installed in your Cloud shell,
in which case this command will do nothing
but exit harmlessly.
To install it, run sudo pip3 install google-cloud-bigtable.
Now let's clear our screen, and now we can run our script
with python3 dataloader.py.
And it's done.
Our data is loaded.
Now let's run the HBase shell to query our new dataset.
As I mentioned at the start of the lab,
we'll use the Google Cloud Bigtable examples repo
because it has a really helpful HBase quickstart script.
Let's change into
the cloud-bigtable-examples/quickstart directory.
And then run ./quickstart.sh.
Maven will do its thing and after a short while,
it will drop us into the HBase shell.
That error is nothing to worry about
and you can safely ignore a lot of the other messages
that come up now as well.
Just pause the video here if you need to
until you get the HBase shell prompt.
Now, as we know, we can scan our entire table
with scan fires.
Don't forget these single quotes
or the command won't work.
But we also know that scanning tables takes time.
That took about six and a half seconds.
The whole point of designing our row key was to make
for low latency queries.
So thinking back to our query planning,
let's say we want to retrieve data
for the area of forest burnt in a cross-section of the park,
let's say X two and Y two.
We can retrieve our previous query by hitting the up arrow
and then add a row prefix filter
that will jump straight to rows that begin with two and two.
We can also ask for specific columns,
in this case, the area.
Much quicker.
So how about just that cross section
for the month of August?
If we hit the up arrow again,
we can simply add another component
to the row key filter, aug for the month of August.
And if we want to get all the metrics for that month,
we can also broaden our column filter
to the entire column family.
Hit the up arrow again and just delete the column area.
As you can see, these queries are very fast.
Great job, Cloud Gurus.
You've transformed a data set to work
with a performant Bigtable schema
and run some HBase queries.
Hopefully, this has given you a really good sense
of the sort of data and workloads
that Bigtable is suited to and how well it performs
when you've designed your row keys
around the queries you need to make.
To clean up our lab, all we need to do is go back
into the GCP console and delete our Bigtable instance.
Please don't forget to do this.
If you have any questions about this lab,
please let me know in the course forums.
If not, I'll see you in the next video.

```

## Resources 

- [Bigtable Application Profiles](https://cloud.google.com/bigtable/docs/app-profiles) 
- [Bigtable Quotas & Limits](https://cloud.google.com/bigtable/quotas) 
- [Bigtable IAM Roles](https://cloud.google.com/bigtable/docs/access-control) 
- [Instances, clusters, and nodes](https://cloud.google.com/bigtable/docs/instances-clusters-nodes) 

## Lesson Transcript
```text
Hello Cloud Gurus,
and welcome to this lecture
where we'll look at the architecture of Bigtable
and what we need to know when creating instances,
clusters, and nodes.
When you use Bigtable, you create an instance.
This can be thought of as a logical container
for your Bigtable clusters
that will share the same configuration.
The instance is defined by its instance type.
This can be development or production,
its storage type,
this can be HDD or SSD,
and its associated application profiles,
which describe the parameters for incoming connections.
When you connect a Bigtable from your application,
you will use an instance ID and an application profile.
All instances have a default application profile,
but it's good practice to create a custom profile
per application that will use your instance.
More on this later.
Inside your instance,
you will have one or more clusters.
Clusters themselves contain nodes,
which are the workhorses of Bigtable.
Each node handles the compute responsibility
of dealing with requests for Bigtable.
So, you can scale up your cluster
to deal with demand and scale it back down again,
when the extra capacity is no longer necessary.
As we learned in the previous video,
this flexibility comes from separating storage
from our cluster nodes and storing data in Colossus,
Google's internal global file system.
Each node will handle a number of tablets,
which contain sets of contiguous rows from your table.
A tablet can't be shared by more than one node,
but as a table grows,
the data in tablets can be split,
rebalanced ,and remerged as necessary.
And this happens automatically behind the scenes.
So, as we stated before,
there are 2 types of Bigtable instance,
a production instance,
which contains 1 or more clusters
with a minimum of 3 nodes per cluster,
and a development instance,
which is a single node cluster,
purely designed for development work.
A development instance cannot use replication,
does not have an SLA,
and has severe performance limitations.
However, to get started with Bigtable,
it's a much cheaper option
and instance can also be upgraded from development
to production at any time
although this change is permanent
and it does not work the other way around.
When creating an instance,
you also have 2 storage types to choose from,
SSD and HDD.
Let's look at SSD first.
SSD is almost always the right choice.
There may be initial cost savings in using HDD disks,
but the impact to performance means you will often end up
spending more money to overcome this.
SSD disks are the fastest and most predictable option
in terms of performance,
offering 6 millisecond latency for 99%
of read and write operations.
Now, one thing you need to consider,
is that even though your cluster nodes don't store data,
they need to be able to process data.
So, there is a calculation for how many nodes you need
based on how much data you have.
With SSD instances,
each node can comfortably process 2.5 terabytes of data.
With this in mind,
if you size your cluster incorrectly,
you will start to see performance impacted.
The other storage type is HDD.
And with HDD instances,
each node can process 8 terabytes of data.
On the face of it,
this seems like you would pay for fewer nodes,
but this is not usually the case
due to the fact that,
throughput of HDD disks is severely limited.
So, there is not as much IO overhead
on the processing nodes.
For example,
row reads for HDD instances,
are about 5% as fast as SSD instances.
To make up for this poor disk performance,
you may end up adding more cluster nodes
at which point,
you've probably sacrificed any initial cost saving.
That being said,
there are a few rare use cases
where HDD instances may work for you.
Google say that if you are storing at least 10 terabytes
of infrequently accessed data and latency
is not an issue for you,
HDD instances might be a cheaper option.
This would work perhaps for archival data
or data that is analyzed in batches,
where time is not an issue.
As we mentioned,
when we draw a diagram,
an instance also stores application profiles.
These are custom preferences created for each application
that will be making connections to Bigtable.
It's good practice to create a profile
for each individual application
as this will help with viewing connection metrics.
And it also allows you to define a couple of policies
regarding the application's connection.
The first, is whether your application should use single
or multi-cluster routing.
In single cluster routing,
an application's requests will only ever route
to a single cluster that you define,
even if you have multiple clusters in your instance.
If that cluster becomes unavailable,
you'll need to update your profile
to point to a different cluster.
With multi-cluster routing,
requests will route
to the nearest available Bigtable cluster.
If that cluster becomes unavailable,
connections will automatically fail over
to the next available cluster.
So, multi-cluster routing sounds great.
Why would you ever choose single cluster routing?
When it comes down to whether or not your application
needs to support single-row transactions.
These are a kind of transaction
that permit atomic updates to single-rows
in a way that could not be made strongly consistent
across multiple clusters.
Because of this,
if you need single-row transaction support,
you'll be forced to use single cluster routing
because there's simply no way to provide
both options safely together.
For more details on single-row transactions,
see the link in the resources section.
Just a few configuration boundaries
to bear in mind when setting up Bigtable.
A single instance can run up to 4 clusters
and the individual clusters themselves,
will each run inside a single zone.
So, the way to achieve multi-zone redundancy,
is to run additional clusters in the instance,
in different zones in a region.
Production clusters must run a minimum of 3 nodes each
and the default quota allows up to 30 nodes per GCP project,
not just per instance.
You can request a higher quota,
but at this point you might want
to consider multiple projects.
Each instance also supports a maximum of 1000 tables.
For the full list of quotas,
see the link in the resource section.
Finally, never forget security and access control.
As with all GCP services,
this is achieved for Bigtable through Cloud IAM roles.
These can be applied to the project or instance level,
but no deeper than that.
They are typically used for defining restrictions
between accessing instances
or just administering the creation of instances,
delineating between users who can write to an instance
or only read from one,
or limiting who can access development instances
versus production instances.
Again, I'll link to the GCP documentation
for Bigtable IAM roles in the resources section.
That about wraps up our deep dive
on Bigtable architecture, Cloud Gurus.
If you have any questions,
please let me know in the course forums,
and if not,
join me in the next video.

```

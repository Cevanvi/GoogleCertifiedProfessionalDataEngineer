## Resources 

- [Dataproc Cluster Properties](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/cluster-properties) 
- [Dataproc IAM](https://cloud.google.com/dataproc/docs/concepts/iam/iam)
- [Autoscaling Clusters](https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/autoscaling)



## Lesson Transcript
```text
Hello, Cloud Gurus.
Now, you've got your hands on Cloud Dataproc
and you've seen what it can do.
In this lecture,
we'll go over some of its advanced features.
Not all of these features are likely to pop up in the exam,
so we won't go into too much detail,
but it's good to be aware of these extra components
if you ever come to use Dataproc yourself.
So as we discussed previously,
we know that Dataproc nodes are pre-configured VMs,
built from a Google-maintained image
that includes Hadoop, Spark,
and all of the common tools you need
to process your big data jobs.
But if, for some reason,
you need completely different packages,
you can create your own custom image
that will be used in your cluster,
instead of the default image.
Google will provide a simple script
that will create a Compute Engine VM,
based on the default Dataproc image.
Then apply a customization script that you've written,
and finally, store the image of this new VM
in Google Cloud Storage,
ready to be used in your Dataproc cluster.
It's that easy.
There are a few other ways
you can customize a Dataproc cluster
without having to completely rebuild the image
it's running on.
The open-source components of Dataproc,
such as Hadoop and Spark,
all ship with numerous configuration files.
Using custom cluster properties,
you can apply changes to any of the values
in dozens of different configuration files,
when setting up your cluster.
There are too many to mention here,
so I'll add a link in the Resources section
listing them all.
Alternatively, you can leave these at the default
and add custom initialization actions to your cluster.
These are scripts that you can upload
to a Cloud Storage bucket,
which will be executed on all of the nodes in your cluster,
as soon as the cluster is set up.
These are commonly used for staging binaries
or pre-loading any binaries
that might be required for a job,
rather than submitting them as part of the job itself.
Another feature to be aware of,
specifically for Spark jobs,
is that you can specify custom Java and Scala dependencies
when submitting the job.
This saves you from pre-compiling these dependencies
on your entire cluster,
when they're only going to be needed by specific Spark jobs.
One of the most powerful features of Cloud Dataproc
is its ability to autoscale.
This allows you to create relatively lightweight clusters
and have them automatically scale up
to the demands of a job, then scale back down again,
when the job is complete.
Autoscaling policies are written in YAML,
and contain the configuration
for the number of primary workers,
as well as secondary or preemptible workers.
The basic autoscaling algorithm
operates on YARN memory metrics,
to determine when to adjust the size of the cluster.
After every cooldown period elapses,
the optimal number of workers is evaluated
using this formula.
If a worker has a high level of pending memory,
it probably has more work to do to finish task.
But if it has a high level of available memory,
it's possible that the worker is no longer required
and could be scaled down again.
The scale-up and scale-down factors
are used to calculate how many workers to add or remove
if the number needs to change.
The gracefulDecommissionTimeout is specific to YARN
and sets a timeout limit for removing a node
if it's still processing a task.
This is a kind of failsafe,
so you don't keep removing workers
if they're still busy.
As great as this is, there are some times
when you should not use Dataproc Autoscaling.
You ideally shouldn't use HDFS storage
when using Autoscaling. Use Cloud Storage instead.
If you are using Autoscaling and HDFS,
you need to make sure that you have enough primary workers
to store all of your HDFS data,
so it doesn't become corrupted by a scale-down operation.
Spark Structured Streaming
is also not supported by Autoscaling.
You shouldn't rely on Autoscaling to maintain idle clusters.
If your cluster is likely to be idle for periods of time,
don't rely on Autoscaling to clean up resources for you.
Just delete the idle cluster, and create a new one
the next time you need to run a job.
There is extensive guidance on Autoscaling
in the Dataproc docs that are linked to
in the Resources section.
To provide more automation of Dataproc clusters and jobs,
you can use a built-in feature called Workflow Templates.
These are templates written in YAML
that can specify multiple jobs,
with different configurations and parameters,
to be run in succession.
A template must be created first,
but won't execute any actions,
until it is instantiated using the gcloud command.
In this example, our template contains the specifications
for an ephemeral cluster,
which is created when our template is ins instantiated.
Jobs are submitted to the cluster,
and the cluster will be deleted,
once the jobs are complete.
We can alternatively configure our template
to use an existing cluster,
instead of creating a new one each time.
Because Dataproc runs on Compute Engine VMs,
there are some advanced features of Compute Engine
you can also take advantage of.
You can enable local SSDs for your master and worker nodes,
including preempt workers
to provide much faster read and write times
than persistent disk.
This is useful for short-lived clusters
utilizing HDFS storage.
You can also attach GPUs to your nodes
to assist with specific workloads, such as machine learning,
which are well suited to the instruction sets of GPUs.
Note, the default Dataproc image
doesn't contain the necessary drivers and software
to use GPUs, but you can apply these
with an initialization action,
or create a custom cluster image.
You also can't attach GPUs to preemptible workers.
Finally, one of the other major benefits of Cloud Dataproc
is the Cloud Storage Connector.
This allows you to run Dataproc jobs
directly on data in Cloud Storage,
rather than using the HDFS storage service
provided by your worker nodes.
Not only is this cheaper than persistent disk storage,
but you gain all the benefits of GCS,
such as high availability and durability.
This allows you to decouple your storage
from the lifespan of your cluster.
In other words, you can get rid of a cluster
as soon as its work is finished,
knowing your data is safely stored in Cloud Storage.
Okay, Cloud Gurus.
That's enough advanced theory on Dataproc.
We haven't even touched on security and IAM,
but as with most other GCP products,
there are some roles and permissions
to familiarize yourself with,
so please check out the IAM link in the resources section.
If you have any questions,
please let know in the course forum.
And if not, I'll see you in the next video.

```

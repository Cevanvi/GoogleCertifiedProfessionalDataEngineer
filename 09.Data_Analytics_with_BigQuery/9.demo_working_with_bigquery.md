## Resources

- [Dummy data files](resources/9.demo_working_with_bigquery)
- [Loading CSV data from Cloud Storage](https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-csv)
- [Loading JSON data from Cloud Storage](https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-json)

## Lab commands

```sql
-- Search for top 5 most expensive products
SELECT product, retailPrice
FROM `<YOUR_PROJECT>.<YOUR_DATASET>.products `
ORDER BY retailPrice DESC
LIMIT 5;

-- Search for happy customer comments
SELECT name, email, customer_id, comment
FROM `<YOUR_PROJECT>.<YOUR_DATASET>.comments `
WHERE rating = 5
LIMIT 5;

-- Compare unit and retail prices
SELECT SKU, supplier, unitPrice, retailPrice
FROM `<YOUR_PROJECT>.<YOUR_DATASET>.products `;
```

## Lesson Transcript

```text
Hello Cloud Gurus.
Now we know all there is to know about BigQuery.
Let's get hands-on with some labs.
We've already been exposed to BigQuery
a couple of times in our Pub/Sub and Dataflow labs.
But in this lab, we're going to go back to basics
and look at how we ingest data into BigQuery
from our local device or cloud storage
in CSV and JSON formats.
Then we'll run some basic queries on our ingested data.
And finally, we'll export queries back to Cloud Storage
and even to Google Sheets.
All you need to follow along with this lab is a GCP project
and access to the Cloud Console.
We'll use some dummy data files that I've generated,
which you can download from the resources section.
So when you're ready, join me in the GCP Console.
So here we are in the GCP Console.
If you followed along with previous labs,
you should already have the BigQuery API enabled
for your project.
If you haven't, you can enable it
by selecting APIs and services from the main menu,
then click Enable APIs and Services
and search for BigQuery.
If you see an option here to manage the API,
that means it's already enabled.
So let's select BigQuery from the GCP menu,
which you can find under the Big Data section.
With your project selected on the left,
click Create Dataset.
Datasets are top-level containers that are used to organize
and control access to your tables and views.
I'm going to call mine timlab1.
You can call yours whatever you like.
We can optionally choose the location of the data here,
which might be helpful for performance
or compliance reasons.
The default will be multi-regional in the US.
Click Create Dataset.
Now we have a dataset, we can create a table.
Click Create Table,
and you'll see this pop over
asking you to configure the table.
Now, when we did this in our Dataflow lab,
we had to define the schema for the table.
This is mandatory if you're creating an empty table.
But instead, we're going to upload a CSV file
when we create the table to pre-populate it with data.
At the top, we can change the source
from empty table to upload,
then click Browse and select the products.csv file
that you've downloaded from the resources section.
Note that the file format toggle now updates
based on the filename.
Under schema, we now have the option
to auto-detect the schema
from the file that we're uploading.
So let's tick that.
Finally, we'll name the table products.
Then click Create Table.
A BigQuery job is created to batch ingest all of the data
in our CSV file into our new table.
It's only 1,000 rows so the job completes very quickly.
Now let's take a look at our new table.
We can look at the details of our table
and see that it contains 1,000 rows
stored in around 77 kilobytes, not bad.
And we can preview the table here
without having to run any queries.
This dummy data contains a product identifier,
name, and supplier, as well as a stock level, unit price,
and retail price.
This is all randomly generated,
so don't worry if some of it makes no sense,
like shark loins being sold at a loss.
Before we get to running any queries,
let's look at one more way to ingest data into BigQuery.
Select Storage from the GCP menu.
We need a cloud storage bucket.
You can use any existing bucket for this
or create one temporarily.
I'm going to use a bucket I already have
that's named after my project.
Click Upload files and select the JSON file
that you've downloaded from the resources section.
Now let's jump back into BigQuery.
Select your dataset again and click Create Table.
Under Source, select Google Cloud Storage,
then browse to the JSON file that you just uploaded.
Again, the file format automatically updates.
It's really handy
that BigQuery can import directly from JSON
and convert it to SQL without any other processing required.
But you do have to make sure
that this is a new line delimited JSON file.
Check out the link in the resources section
for more information.
Let's set the table name to comments.
And once again,
we'll tick here to auto-detect the schema of our data.
Then click Create Table.
Let's look at the details of our new table.
We can see that the schema auto-detection
has correctly inferred the types on each of our columns.
Our rating and customer ID columns are integers
and the rest of the columns are strings.
Again, let's have a quick preview of the data
and we can see it's mostly just lorem ipsum nonsense.
Okay, now let's get to grips with running some queries.
We can run interactive queries
directly from the query editor using standard SQL.
Let's see what our top 5 most expensive products are.
We'll select product and retail price
from the full table name,
which includes the project and dataset in back ticks,
order it by the retail price
in descending order
and limit this to 5 results.
Then click Run.
Interesting.
Beer, energy drinks, cake, shrimp, not a bad meal.
Note that every time we run a query,
a temporary table gets created to cache the results.
This table will be deleted automatically after 24 hours.
Let's have a look at our other table
to see some of our happiest customers' comments.
We'll select the name, email, customer ID, and comment
from the comments table
where the rating is 5.
And again, we'll limit to 5 results.
Then click Run.
Ah, if only we spoke Latin.
Note that in both of these queries we've just run,
we've limited the output to 5 rows with a limited clause,
but I've only done this to make it easier
to read the output in this lab.
In fact, we're still querying all 1,000 rows.
You can see for yourself
when the amount of data processed remains the same
when we remove the limit clause.
This gives us 191 rows with 5 star ratings.
That's not bad. 191 happy customers.
Maybe we should share this information with the boss
so she knows that we're doing a good job.
So let's export this data,
but only the data that's come out of this query.
Click Save Results.
The default option here
is to save a CSV file to Google Drive
and that's exactly what we want,
then we can share the file with the boss.
The export job runs and when it's finished,
we get a link to open in Drive.
Here's the preview.
And from here, we can share the file
or do anything else we need to
just like any other Google Drive file.
So we know our customers are happy, but are we profitable?
Let's go back to BigQuery
and look at our products table again.
Let's write a query to export the SKU, supplier,
and prices from our product data.
This one's very simple.
We just select all of the columns we want
from the products table.
And then click Run.
Now, say we have a product analyst on our team
who wants to take a look at the differences
in our wholesale and retail prices.
But this analyst doesn't know SQL.
They're used to just living inside spreadsheets.
Click Save Results.
And this time, select Google Sheets,
then click Save.
An export job will run and in a moment,
we'll be given a link to our new data
saved directly into a new Google Sheets file.
Now our analysts can operate on this
just like any other spreadsheet.
They can create a new column called profit,
write a quick formula to work out
what the profit actually is
between the retail price and unit price.
Copy that formula to the whole spreadsheet,
then apply a filter
and sort by profit.
Oh wow, there's lots of products here
that we're losing money on.
Time to negotiate with some of these suppliers.
Okay, Cloud Gurus, that was a really quick tour
of loading and using data in BigQuery.
It's capable of really powerful things,
but it's also very intuitive
and should be easily approachable
for anyone who needs to crunch some numbers.
All you need to do to clean up this lab
is go back to BigQuery, select your dataset,
click Delete Dataset,
and enter the dataset name to confirm.
Thanks for watching Cloud Gurus. I'll see you next time.
```

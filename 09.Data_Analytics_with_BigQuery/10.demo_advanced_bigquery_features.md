## Lab commands 

```sql
-- Query public dataset for cycle hires in 2017, ordered by duration of hire
SELECT start_station_name, duration, start_date FROM `bigquery-public-data.london_bicycles.cycle_hire`
WHERE CAST(start_date AS DATE) BETWEEN DATE(2017, 01, 01) AND DATE(2017, 12, 31)
ORDER BY duration DESC;

-- Copy public dataset to create partitioned table
CREATE OR REPLACE TABLE eu_dataset.cycle_hire PARTITION BY DATE(start_date) AS
SELECT * FROM `bigquery-public-data.london_bicycles.cycle_hire`

-- Query partitioned table for cycle hires in 2017, ordered by duration of hire
SELECT start_station_name, duration, start_date FROM `eu_dataset.cycle_hire`
WHERE CAST(start_date AS DATE) BETWEEN DATE(2017, 01, 01) AND DATE(2017, 12, 31)
ORDER BY duration DESC;

-- Query partitioned table for cycle hires on Valentine's Day 2017
SELECT start_station_name, duration, start_date FROM `eu_dataset.cycle_hire`
WHERE CAST(start_date AS DATE) = DATE(2017, 2, 14)
ORDER BY duration DESC;

-- Query for highest pollutant reading from public dataset
SELECT location, city, country, pollutant, value, averaged_over_in_hours, timestamp
FROM `bigquery-public-data.openaq.global_air_quality`
ORDER BY value DESC;

-- Copy public dataset to create partitioned auto-expiring table
CREATE OR REPLACE TABLE us_dataset.air_quality PARTITION BY DATE(timestamp)
OPTIONS (partition_expiration_days=60) AS
SELECT * FROM `bigquery-public-data.openaq.global_air_quality`

-- Query for highest pollutant reading from auto-expiring partitioned table
SELECT location, city, country, pollutant, value, averaged_over_in_hours, timestamp
FROM `us_dataset.air_quality`
ORDER BY value DESC;

-- Query a shared authorized view from a different project
-- Replace <SOURCE_PROJECT> with your own source project name
SELECT location, city, country, pollutant, value, averaged_over_in_hours, timestamp
FROM `<SOURCE_PROJECT>.us_sharedviews.air_quality_shared`
ORDER BY value DESC;
```

## Lesson Transcript
```text
Hello Cloud Gurus.
In this lab, we're going to take a quick look
at some of the advanced features of BigQuery.
First of all, we're going to take a look
at partitioned tables.
Partitioning is a great way to logically separate tables
to allow for cheaper and more efficient queries.
Then we'll look at
how you can automatically expire partitions
when you don't need to store lots of historical data.
And finally, we'll look at how authorized views can be used
to share queries with other users,
without granting access
to the underlying tables and datasets.
All you need to follow along with this lab is a GCP project
and access to the Cloud Console.
We will be running lots of SQL commands in this lab,
but you can download a text file containing all of them
from the resources section.
So when you're ready, join me in the GCP console.
Okay, Cloud Gurus.
We're back in the GCP console.
Let's go straight to the BigQuery UI.
First, we'll create 3 different datasets
that we'll use in this lab.
With your project selected in the left-hand side,
click Create Dataset.
We'll name this dataset eu_dataset.
Under data location, change this to be the European Union.
Then click Create Dataset.
Then we'll create another dataset.
We'll name this one us_dataset,
and change the data location back to the United States.
Then click Create Dataset.
And we'll create one more.
We'll call this one us_sharedviews,
and leave the location as the United States.
Then click Create Dataset.
Now to help us demonstrate some of the concepts in this lab,
we'll use some of Google's public BigQuery datasets.
You can explore these by clicking Add Data,
then Explore Public Datasets.
There are dozens of freely available datasets here,
covering all sorts of domains
from climate science to healthcare,
financial information and general scientific research.
Many of the datasets are historical,
so they don't contain completely up to date information,
but they're still very useful for training models,
or just training yourself
on how to use BigQuery effectively.
Scroll down and click on Public Safety,
and you'll see a selection of datasets,
covering things like crime,
vehicle collisions, and bicycle hire.
There's probably an interesting correlation right there,
if you want to try to query all 3 of those,
but for now let's look at London bicycle hires.
This dataset contains details of trips
taken on hired bikes in London,
in the UK from 2011 to, well, it says to the present,
but the data actually runs out part of the way through 2017.
That's a shame, but it's still good enough for our lab.
So click View Dataset.
Now, rather unhelpfully all BigQuery does
is open up a new BigQuery UI tab
with the name of the dataset but no other information.
If you want to look at the schema or preview the tables,
you'll need to dig through the BigQuery public dataset
in the left-hand menu.
So let's do that.
Click BigQuery Public Data,
and we'll scroll down to find London bicycle hires.
Here we go.
We're going to use the cycle hire table.
This is the schema.
In the details tab,
we can see this table contains about 24 million rows.
Let's click on Preview,
and see what sort of data is contained within.
Each of these rows represents a hire of a bike in London
from the starting timestamp and location
to the end timestamp and location.
That means where the bike was hired from,
and where it was dropped off.
Like most major cities, London has connection points
for hire bikes dotted all over the city.
It even has 2 or 3 bike lanes as of this recording.
The data also includes the duration of the hire in seconds.
So let's say we're curious to see
what the longest hire period was
between a specific time window.
I'll write a query that finds the start station name,
the date of the duration of the hire
from all hires that took place in 2017,
and then sort them with the longest duration first.
Note that the query validator tells me
it has to process one gigabyte of data
to perform this search.
That's not a small query, but don't worry.
The BigQuery Free Tier lets you run
one terabyte of queries per month, free of charge.
Click run.
This should take 10 to 12 seconds for you to run the query,
but my results have already been cached,
and we can see that the longest hire
was from All Saints Church, Portobello,
just over 648,000 seconds, which is more than a full week.
I think somebody forgot to return their bike.
Now as we know by now,
where and limit clauses don't actually affect
the amount of data that needs to be searched,
which is a major consideration
when designing large data processing systems
and trying to be mindful of costs.
So rather than store
all of the historical data in a single table,
the creators of this dataset
could have used a partition table.
Partition tables let you logically separate data,
so you don't need to scan an entire table.
Let's make a copy of this table into our own dataset,
creating a partition table as we do so.
With this query we're processing 2.6 gigabytes of data,
as we're including all of the original columns
to create our new table.
But as part of the create statement we use partitioned by
and cast the start date timestamp to a date.
Behind the scenes,
BigQuery will then logically partition the table
based on the date in every row.
Click Run.
This query should take about 30 seconds to run.
The query creates a new table in our eu_dataset,
called cycle hire.
Note that we have to use a dataset in the same region
as the source dataset that we're copying from.
Here's our new table.
If we look at the details,
we can see that this table is partitioned by day,
which means we can be very granular
and cost-saving in our searches.
We could have optionally partitioned by month for example,
which might make our query slightly more performant
with less granularity.
But we've still got 24 million rows
and the same data as before,
which we can check by previewing.
Let's try our original query again,
but on our new partition table.
Our new query is projected to only process
198 megabytes of data,
compared to the 1 gigabyte previously.
Thanks to the partitioning,
all of the rows we don't need,
because they're outside of our date range
can be safely ignored.
Click Run.
The results will be the same as last time,
but the query was cheaper.
Because we partitioned by day,
we could also choose an arbitrary date range
and be assured that the query is as cheap as it can get.
Let's look at the bike hires on Valentine's day.
This query is only going to process
just over 1 megabyte of data.
Okay, that's enough bikes for today.
Let's look at another useful feature
that you can consider when designing your BigQuery tables.
If you're ingesting large volumes of data,
you will eventually start to run up some storage costs.
Although BigQuery storage is very cheap,
It might be the case,
that you just don't need to keep that much historical data.
Thankfully BigQuery has a built in mechanism
to automatically expire data that's no longer required.
Let's have a browse through the public datasets again,
and look for the openaq dataset which contains data points
on global air quality measurements.
This global air quality table
contains just under 2 years worth of data,
and it's constantly being updated.
But let's suppose we want to present a view
of the highest polluted readings from only the last 60 days.
The entire table itself is very small.
So really we won't be saving any money here,
but we're limited to the public datasets
that are still actively updated.
So we'll just have to use this one
to demonstrate our concept.
Okay, let's start by just querying the public data itself.
Here we'll get the location details,
the type of pollutant and its value,
as well as the time window that the data was gathered in,
in the averaged over in hours column,
which will help us decide on the quality of the data.
I'll run the query,
and here I can see the highest point of pollution.
But the timestamp is from July, 2019,
which is not very helpful in my use case.
Again, we could just use a where clause
as this table is so tiny,
but pretend that it's billions of rows
and we're trying to save money.
Let's create a partitioned copy of this table,
using the new feature of automatic partition expiry.
Here we create our own version of this table,
but partitioned by the timestamp,
and set our partitions to expire after 60 days.
Note again, that the location is important.
We have to use our us_dataset,
as the origin dataset is in the US region.
We now have our new version of this table in our us dataset.
In the details tab, we can see that it's partitioned by day,
and the partitions are set to expire after 60 days.
So let's run our query again.
That's more like it.
Rows where the timestamp is older than 60 days
have already been expired from the table,
and we get much more up-to-date information.
So finally,
let's say we need our data analysts to see this data,
but we don't want to give them access to our project
and the underlying datasets and tables.
How can we do that?
It's simple.
We just create an authorized shared view.
Where you already have the query in the editor,
add your project name to the start of the from clause.
Make sure you use your own project name here.
Then click Save View.
Change the dataset name to us_sharedviews,
and enter a table name
of air_quality_shared.
And click Save.
In your us_sharedviews dataset,
you can now see the shared view you just created.
To share this with other users,
we first need to add them to our project.
The next part of this lab is optional if you don't have
a secondary GCP account that you can test with.
Just watch what I do
and you'll still understand the concept.
I'm going to go to the IAM and Admin section
in my other tab in the GCP menu.
I'll click Add.
And now I'll enter the email address of the user or group,
that I want to grant the permissions to.
I need to add this person with the BigQuery user role.
And I'll click Save.
Then back in the BigQuery UI,
I'll select the us_sharedviews dataset,
and click Share Dataset.
Here I need to add that user
with the BigQuery data viewer role.
First click Add, then click Done.
One more thing to do.
Back on the source dataset, us_dataset in our case,
I'll click Share Dataset again,
and this time go to authorized views.
Now I select the shared dataset and the shared view,
to authorize it to use this source dataset.
Then I'll click Add and done.
Now, if you're thinking
this process isn't particularly intuitive,
then we're in agreement my friend.
Once this is done,
I can test the query from another GCP account.
This is what the shared view looks like for the other user.
I can run this query against the shared view,
but I'll have no other access
to the source project or dataset.
Note that this secondary user
would have to pay to run the query,
not the owner of the source project,
and that's exactly how all of the public datasets
we saw earlier are shared.
Okay, CloudGurus.
To clean up from this lab all we need to do,
back in our original project, is delete the 3 datasets.
First, the eu_dataset.
Then the us_dataset.
And finally the shared views dataset.
Don't forget to also delete the secondary user
from your projects IAM section, if you added one.
Thanks for watching CloudGurus and I'll see you next time.
```

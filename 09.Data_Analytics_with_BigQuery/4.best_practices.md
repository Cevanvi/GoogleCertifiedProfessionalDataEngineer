## Resources 

- [Controlling costs](https://cloud.google.com/bigquery/docs/best-practices-costs) 
- [Query performance](https://cloud.google.com/bigquery/docs/best-practices-performance-overview) 
- [Optimising storage](https://cloud.google.com/bigquery/docs/best-practices-storage) 


## Lesson Transcript
```text
Hey, Cloud Gurus.
In this video, we'll cover some of the best practices
relating to BigQuery.
There isn't time to cover every one of them,
but we'll cover the most important ones.
We'll also cover 2 related topics rather briefly,
slots and the query plan.
Unfortunately, there are many points to cover
and this video may feel a little like death by slideshow.
Luckily, my innovative use of arrows for bullet points
should help mitigate this.
Let's dive in.
From the documentation, a slot is defined
as a unit of computational capacity
required to execute SQL queries.
Slots are used when queries are executed.
They play a role in pricing and resource allocation.
So what determines the number of slots used by query?
Well firstly, the size of the query,
that is the amount of data that is processed by that query.
And secondly, the complexity of the query,
which is really the amount of information
that is shuffled during the steps of the query.
BigQuery automatically manages your slots quota.
You don't need to worry about it.
You don't need to do anything for this to happen.
In addition to consumption-based pricing,
there is also a flat rate pricing that can be organized.
And this is where a fixed number of slots are purchased
over some periods such as every month.
You can see your slot usage using Stackdriver.
For each query job, BigQuery will provide you
with a diagnostic query plan and execution timeline.
These can be super helpful
in terms of understanding slot usage
as well as execution times.
The reason BigQuery is so fast is because it leverages
a heavily distributed parallel architecture
to run your queries.
In order to do this, BigQuery needs to do a few things.
When you write an SQL query,
you provide a declarative SQL statement to BigQuery.
BigQuery converts this declarative SQL statement
into a graph of execution,
which consists of a series of query stages.
Each query stage is in turn
composed of a number of execution steps.
For each query stage within the overall query,
important bits of information are provided.
Firstly, there is a stage overview.
Then there is step information.
There is stage timing classification.
And finally, there is timeline metadata.
In the resources section, I will link to the documentation
where you can find out more about the information provided
in each one of these steps.
So now let's look at BigQuery best practices.
So the best practices are divided into 3 main topics.
These are controlling your costs,
running your queries with the optimal performance
and optimizing your use of storage.
We'll look at each one of these in some detail
in the upcoming slides.
Okay. Best practices for cost controls.
Here we go.
Firstly, avoid using SELECT*.
Remember, this will select all columns.
Columns are stored separately,
which results in more data being processed.
Use the preview option to sample data
rather than running query statements.
This is because you don't pay to preview the data.
Price your queries before executing them.
So remember queries are billed
according to the number of bytes that are read.
The pricing is based on the amount of data
that the query will process,
which is available before the query is executed.
You can convert the number of bytes
that will be read to a price
using the Google Cloud Platform Pricing Calculator.
Remember that using LIMIT does not affect costs.
The full columns are read and processed.
Set up a dashboard so that you can view your query costs
and you can query your audit logs.
Partition by date.
Remember, this allows you to query only dates
that are relevant to the query at hand.
Materialize query results in stages.
This will minimize data shuffling
during processing of your SQL statements
and it will lower costs.
Of course, consider the cost of large result sets.
And use streaming inserts with caution,
because they are costly.
If you don't really need to use streaming inserts,
go with bulk loads.
Okay, now we will be looking at best practices
for our second area, which is query performance.
The query performance is broken down
into a number of dimensions.
Firstly, we have input data and data sources.
We have shuffling.
This is how data is passed between
the different stages of a query.
We have query computation. Then there is materialization.
And finally, there are a number of SQL anti-patterns.
We're not going to look at all of these.
I'm going to focus in on the three
that I think are most important,
input data and data sources, query computation,
and the SQL anti-patterns.
As always, I will link to the documentation
in the resources section and you can do further reading
on shuffling and materialization.
Starting with input data and data sources.
The first thing is to prune partitioned queries.
Don't query partitions that you don't need to.
Secondly, denormalize data wherever possible.
Remember, this is more efficient for BigQuery processing,
not only in terms of cost,
but also in terms of overall performance.
Use external data sources appropriately.
Remember, there are no guarantees of performance
with external sources.
Avoid excessive use of wildcard tables.
Now for the second topic of query performance
that we'll be looking at, query computation.
Avoid repeatedly transforming data via SQL queries.
Remember to use materialization.
Avoid JavaScript user-defined functions.
They slow down overall performance.
Order your query operations to maximize performance.
You'll need to know SQL to do this properly.
Optimize your JOIN patterns.
The last two points are really about the way
you construct your SQL statements.
Unfortunately, we don't have time
to get into the details of the SQL in this course.
And now for the final area of query performance
that we will be looking at, SQL anti-patterns.
And, of course, these are things you should avoid doing.
So avoid self-joins.
Avoid data skew.
Avoid unbalanced joins.
Avoid joins that generates more outputs than inputs.
An example of this is the Cartesian product.
Avoid data manipulation language statements
that update or insert single rows.
This is really inefficient.
Finally, let's look at best practices
for optimizing storage in BigQuery.
Firstly, use expiration settings.
You're able to specify expiration time
ranges for tables, for data sets,
and you can set them for partition tables as well.
This allows you to control storage costs
as well as optimize the use of storage in BigQuery.
Take advantage of long-term storage.
The idea here is to keep your data within BigQuery.
Don't just move it into BigQuery to do processing.
You're able to load data into BigQuery at no costs.
There are a number of pricing benefits that do apply
if you keep your data in BigQuery
for longer periods of time.
And finally,
use the Google Cloud Platform Pricing Calculator
to estimate your storage costs.
That's pretty self-explanatory.
Okay, Cloud Gurus,
that's it for best practices for BigQuery.
See you in the next video.
```

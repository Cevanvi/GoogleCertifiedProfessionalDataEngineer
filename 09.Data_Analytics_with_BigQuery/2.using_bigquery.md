## Resources 

- [BigQuery documentation - table schemas](https://cloud.google.com/bigquery/docs/schemas) 
- [BigQuery documentation - working with tables](https://cloud.google.com/bigquery/docs/tables-intro) 
- [BigQuery documentation - BigQuery jobs](https://cloud.google.com/bigquery/docs/jobs-overview) 
- [BigQuery Capacitor](https://cloud.google.com/blog/products/bigquery/inside-capacitor-bigquerys-next-generation-columnar-storage-format) 
- [BigQuery documentation - views](https://cloud.google.com/bigquery/docs/views-intro) 
- [BigQuery documentation - loading data into BigQuery](https://cloud.google.com/bigquery/docs/loading-data) 
- [BigQuery documentation - working with external data](https://cloud.google.com/bigquery/docs/external-data-sources) 
- [BigQuery Transfer Service](https://cloud.google.com/bigquery/docs/dts-introduction) 

## Lesson Transcript
```text
Hello Cloud Gurus.
In this lecture, we'll take a deeper dive
into some aspects relating to using BigQuery.
We'll be covering tables, views
external data and data transfer service.
Let's get going.
A job is an action that BigQuery runs on your behalf.
Jobs are run asynchronously,
and the reason for this is that
jobs can take a long time to complete.
Jobs can be polled for the status
to determine if they've started,
if they're in progress or if they have completed.
Now, not all actions in BigQuery are executed as jobs.
Short running actions are not assigned a job resource.
There are four types of jobs.
There are load jobs, these load data into BigQuery.
There are export jobs, these export data from BigQuery.
There are query jobs, which allow you to query data
within BigQuery.
And finally, there are copy jobs.
These jobs are used to copy tables or data sets.
There are two priorities for jobs.
There is the interactive priority and the batch priority.
The interactive priority is the default.
When running interactively,
the query is executed as soon as possible.
Interactive queries do count towards
your concurrent rate limit.
That is the number of concurrent queries
running at any time and towards your daily limit.
That is the total number of queries
that you can run per day.
Query results for interactive queries
are always saved to a table
either as a temporary table or as a permanent table.
You are able to append or overwrite data
in an existing table
or you can push that information into a new table.
Batch queries on the other hand are queued.
The query is executed when there are idle resources
available in the big query shared resource pool.
If the query has not been executed within 24 hours,
the job priority is changed to interactive
in which case it'll run as soon as possible.
Materialization of query results can be used to lower costs
and to simplify complex queries.
Let's look at how data is stored within BigQuery.
BigQuery uses the capacitor columnar data format.
More on this in a later slide.
Tables can be partitioned within BigQuery.
As with a relational database,
individual records exist as rows
and each record is composed of a number of columns.
Table schemes can be specified
at the time of creation of the table
or when you load data into the table.
If we have a look here, it's as you would expect
from a relational database.
There are records and there are columns in this table A,
and we can have multiple partitions of the table
depicted here.
Let's look at the capacitor storage system in more detail.
So it is a proprietary columnar data storage
that supports semi-structured data.
And by semi-structured data,
we mean nested and repeated fields.
Data is converted from the input format
which could be CSV, JSN, to the capacitor format
when it is loaded into BigQuery.
Dremel stores data in its column storage
which means it separates each record into column values
and stores each record on a different storage volume.
Each column in addition to its value
also stores two numbers, the values repetition level
and the definition level.
This encoding allows the full
or partial structure of a record to be reconstituted
without reading the parent columns.
This is extremely efficient.
So here's a schematic representation of a table
and we can think of each column being separated
and stored on a separate volume.
Remember, each value is stored together
with its repetition level and definition level.
We can show the schematically by assuming
row two, column five has the value New York
with repetition level three and definition level five.
Remember, this is just schematic.
You need to go into the details
if you want to understand how the repetition level
and definition level actually work.
The definition and repetition level in coding
is so efficient for semi-structured data
that it has been used for other encodings
such as the open source parquet encoding.
In the resources, I'll link a blog post on capacitor
as well as the original 2010 Dremel paper.
You'll need to put your thinking cap on
because it is non-trivial to understand capacitor.
It is unlikely that you'll be tested
on the details of capacitor within the certification exam.
Let's look at de-normalization in more detail.
So BigQuery performance is optimized
when data is de-normalized appropriately.
And as mentioned above, de-normalization refers to
nested and repeated columns.
This allows data to be stored in a very efficient manner
and it is also then retrieved in an efficient manner.
If you want to use nested or repeated columns,
you use the record data type.
Let's look at this schematically.
So here we have a table with three columns
the name, the ID, and the address column.
Suppose we want to have the address column
be a nested column,
we would then assign it the record data type
and in that case, we can specify sub columns
such as the name, the street and the city.
We can also have repeated values for each record.
So in this example, for each name or person,
we have multiple addresses stored
and of course the repeated columns can also be nested.
Let's look at this example from the GCP documentation.
So here we have a schema.
We have information on a person.
So we have an ID, a first name, a last name, a date of birth
and now we also have the addresses column
which is nested and repeated.
So it's nested because you can see the sub values of status,
address, city, state, zip code, et cetera.
And it is repeated because we can have
multiple addresses for each ID..
So when we look at the JSN representation
of actual information, we can see for a single ID one,
we have the first address over here
and the second address here.
When importing data into BigQuery,
we need to keep in mind that
there are a number of supportive formats.
The first one is CSV.
This is the default source format
for loading data into BigQuery.
Next we have JSON.
It's important that each record is new line delimited.
Next we have Avro.
Avro is an open source data format
where the schema is stored together with the data.
Avro is the preferred format
for loading compressed data into BigQuery.
Then there is Parquet.
as I mentioned above, it is based on
the Dremel storage approach.
It's a good choice because parquet's encoding is efficient
and typically results in smaller files.
Then there is ORC or Optimized Row Columnar format.
ORC is an efficient way to store hive data.
Then we have cloud data store exports
and fire store exports.
All of the data from these formats
is converted to the columnar formats in BigQuery.
Okay, let's take a look at BigQuery views.
Now, a view is a virtual table that is defined
by an SQL query.
So let's think about how a view works.
We have a dataset.
Remember, all views exist within a dataset
and we have an SQL query, which defines the view.
That SQL Query will access data from a number of tables
and that will be used to create the view
within the data set.
When you have created a view, you can access that view
in the same way that you query a table.
So other SQL queries can then query that view.
Remember, a view is unmaterialized.
That means that the underlying query is executed
each time the view is accessed
that will have billing implications.
So what are some of the benefits of views?
Well, they allow us to control access to data.
When a user queries a view,
they will only have access to the tables and fields
that are part of the definition of that view.
Next, views can be used to reduce overall query complexity.
This is done by breaking a complex query
into a number of simpler queries.
Views can be used to construct logical tables.
Logical tables can be used to organize similar information
from different physical tables in BigQuery.
And finally have the ability to create authorized views.
Authorized views allow users to have access
to different subsets of rows from the view.
BigQuery views also have a number of limitations
that you do need to know about.
You cannot export data from a view because as we said
they are unmaterialized.
You cannot use the JSON API to retrieve data from a view.
You cannot combine standard and legacy SQL.
If you have defined a view using legacy SQL,
you cannot query that view using standard SQL.
The definition of a view cannot use user defined functions.
There can be no wild card table references
in view definitions.
And you are limited to 1000 authorized views per data set.
A federated or external data source
is a data source that you can query directly
even though the data is not held within BigQuery.
BigQuery offers support for querying data
from cloud big table, cloud storage and Google Drive.
So why would you want to use external data sources?
Well, there are two typical use cases.
First is where we want to load and clean data in one pass.
In this case, we change the data from the external source
before loading it into a BigQuery table.
The second use case is where you have small
frequently changing data joined with other tables.
An example of this is where you may be querying
geographical data that changes from time to time
and it is easier to change the locations
within a document in Google Drive
than it is to change the data within a BigQuery table.
There are a number of limitations
that apply to external data sources.
So firstly, there is no guarantee of consistency.
There is lower query performance in general.
You cannot use the table data list API method.
You cannot run export jobs on external data.
You cannot reference external data
in a wild card table query.
You cannot use parquet or ORC formats for external data.
The query results for external data sources are not caged
and you're limited to four concurrent queries
where external data is involved.
There are a number of other data sources
that are useful in BigQuery.
So firstly, there are public data sets.
These are accessible to everyone within BigQuery.
Then there are shared data sets.
These are data sets that have been shared with you.
And finally, you can also view
stackdriver log information within BigQuery.
BigQuery also allows you to bulk extract
and analyze data from a number of additional data sources.
Pulling bulk data into BigQuery is easily facilitated
using the data transfer service.
The data transfer service has a number of connectors
to Google services, to GCP services like cloud storage,
to AWS services like S3 and Redshift.
They're also connect us to other third party systems
such as Facebook, LinkedIn, and there are many others.
These bulk extractions can be one-off events
or they can be scheduled to run repeatedly.
The data transfer service allows
for the historical restatement of data
and has an uptime and delivery SLA.
You can access the BigQuery data transfer service
using the cloud console, the BigQuery online tool
or the BigQuery data transfer service API.
Okay, Cloud Gurus.
That's it for the video on using BigQuery.
See you in the next video.

```

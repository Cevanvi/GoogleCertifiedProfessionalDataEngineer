## Lab Commands

```bash
mvn archetype:generate \
      -DarchetypeGroupId=org.apache.beam \
      -DarchetypeArtifactId=beam-sdks-java-maven-archetypes-examples \
      -DarchetypeVersion=2.8.0 -DgroupId=org.example \
      -DartifactId=dataflow-lab -Dversion="0.1" \
      -Dpackage=org.apache.beam.examples -DinteractiveMode=false
```

```bash
mvn -Pdataflow-runner compile exec:java \
      -Dexec.mainClass=org.apache.beam.examples.WordCount \
      -Dexec.args="--project=${PROJECT_ID} \
      --stagingLocation=gs://${BUCKET_NAME}/staging/ \
      --output=gs://${BUCKET_NAME}/output \
      --runner=DataflowRunner"
```

## Lesson Transcript

```text
Hello Cloud Gurus,
in this lab will get hands-on with Cloud Dataflow
and the Apache beam programming model.
We had a brief experience with Dataflow
in one of our pub/sub labs,
where we used an off the shelf template
to create a processing job for BigQuery.
In this lab, we'll start looking at how we might program
our own jobs by creating a simple word count
using the Apache beam SDK.
Then we'll submit that job to Cloud Dataflow
and marvel as it takes care of
all the resources required for our job
without us having to lift a finger.
All you need to follow along with this lab
is a GCP project and access to the cloud console.
We'll run commands from the Cloud Shell
and take a quick look at some code
in the Cloud Shell editor.
So when you're ready, join me in the GCP console.
Okay Cloud Gurus, here we are in the GCP console.
The first thing we need to do is enable the Dataflow API.
Select APIs and services from the menu
then click Enable APIs and Services
and search for Dataflow.
As you can see, it's already enabled in my project.
Now we need to create a cloud storage bucket for this lab.
Let's go to storage in the GCP menu
and click create bucket.
I'm going to call my bucket, Tim a cloud guru D flow lab.
Make a note of your bucket name as we'll use it in a moment,
you can leave all of the other options as the default
and click create.
Now we're going to use the cloud shell
for the rest of this lab,
and let's open the cloud shell editor as well.
We're going to use a Java tool called Maven.
If you're not familiar with Maven,
it's used to automate builds and projects in Java.
We're going to use it
to create a Java project directory structure for us
and download some pre-written code.
Maven refers to these bits of code as artifacts.
Here's the command we need to run.
You can download the commands for this lab,
from the resources section.
Don't worry too much about the details here,
but there's two things worth noting,
the archetype artifact ID, that's where we specify
that we're going to pull some examples from Apache beam
and the local artifact ID, that's the local directory
where our project will be built.
If we hit return,
the command will run and create our project directory,
downloading the artifacts we've requested.
We can browse through this directory
in the cloud clamshell editor.
You can see that it contains
lots of examples for Apache beam.
These are the artifacts that Maven downloaded for us.
It's worth looking through some of these samples later on
as they are very well written and well commented
to explain the Apache beam programming model.
For now, let's have a look at wordcount.java.
If you're familiar at all with Java,
you know to look for a main method,
which is the first class that runs
when we execute this program.
Our main class, takes in any local pipeline options
then executes the run word count method.
If we take a look at the run word count method,
we can see here that it creates a pipeline.
Then it applies the various stages of the pipeline,
reading in the lines of the input file,
counting them by using the count words class,
then mapping them and finally
writing out the totals for each word.
Let's take a look at the count words method.
This class extends the P transform class,
which performs transformations on P collections.
Inside this class,
we convert lines of text into individual words,
and then count the number of times each word appears.
Note the use of ParDo here, which as we learned,
stands for parallel do,
a way to run a function
on each element of an input peak collection in parallel.
If we take a quick look at the extract words function,
we can see that it extends do FN.
Do FN is just a superclass
that defines the way to create a function
that can be used in a Pardo.
Again, if you're not familiar with Java,
it might feel
like we're starting to get into the weeds a bit here,
but don't worry,
you don't need to be a proficient Java developer
to pass this exam.
It's just worth knowing what these building blocks are,
like Pardo and do FN and how they're used
when building a pipeline.
I'll link to the Apache beam programming guide
in the resources section,
which is worth a quick read before you take the exam.
Okay, so now we hopefully know roughly
what this code can do, how do we run it?
Let's just clear my terminal.
First, we'll set some environment variables.
First I set the project ID,
don't forget to use yours here not mine
and then the bucket name,
and then the name of the bucket that you created earlier.
Now we'll use Maven again to execute our Java code.
I'll just clear my terminal again.
First, we need to change into the new project directory,
then run the following command.
Again, you can grab this command from the resources section.
Here we're simply specifying the location of our code,
our project and the staging and output locations
in our cloud storage bucket.
The input file is actually hard-coded in this example,
it's the text of Shakespeare's king Lear,
but we could override it if we wanted to.
The key thing here is when we specify minus minus runner
equals Dataflow runner.
This causes our code to contact the Cloud Dataflow service
to actually run the job.
When we run this command,
Maven will download all of the supported files
it needs to run our Dataflow job
and then compile them to create our project.
Next it will upload the compiled code to our staging bucket,
ready to run in Dataflow.
Then it will submit the Dataflow job.
To start with, it needs to create a worker
to do the necessary computation for our job.
Once the worker started successfully,
we can pop back into the GCP console
and go to the Dataflow section under Big Data.
Here's the jobs list,
so let's click on our running job.
Dataflow gives us a really nice visualization
of the stages of our pipeline as we saw earlier in the code.
Read the lines, count the words, map them,
and write out the results.
You can safely ignore this deprecation error,
it just means the examples need a bit of updating.
Dataflow shows us the current resource metrics
of its workers.
In this case, it looks like we have a single VCPU worker
with 3.75 GB of RAM.
We can also observe the job logs
and the logs of the workers themselves.
And we can look at a visualization of the job metrics.
Here for example,
we can see the current auto scaling graph of our job
with our single worker.
Helpfully, all of this information will persist
after the job has completed.
It will take a couple of more minutes
for the job to complete.
Our job is now complete and what's really great
is that we haven't had to worry about spinning up
scaling or tidying any of the compute resources involved.
Let's take a look at the output of our job
in our GCS bucket.
Go back into cloud storage
and find the bucket you created earlier.
And here we can see the output of the job,
three shouted output files.
Let's take a look at one of them.
And here are the word counts.
Okay Cloud Gurus, because as I said,
Cloud Dataflow has managed all of the resources for us.
There's no compute instances or other services to clean up,
to finish this lab.
You can optionally delete this bucket now,
or keep it a little while longer
if you plan to play some more with Dataflow.
Thanks for watching and I'll see you next time.

```

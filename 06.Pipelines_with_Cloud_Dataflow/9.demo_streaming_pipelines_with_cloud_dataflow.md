## Resources 

- [Beam Programming Guide](https://beam.apache.org/documentation/programming-guide/) 
- [Lab Code](resources%2Fdemo_streaming_pipelines_with_cloud_dataflow)

## Lab Commands

Remember to activate the virtual Python environment in any terminal you use:
```bash
cd tweeper
source bin/activate
```bash
Activate the Tweep feed:
```
```bash
  python tweeper.py
```
Test the pipeline locally:
```bash
  python pipeline.py --streaming
```
Run the pipeline with Cloud Dataflow:

  python pipeline.py --streaming --runner DataflowRunner \
  --project <YOUR_PROJECT_NAME> \
  --temp_location gs://<YOUR_BUCKET_NAME>/temp \
  --staging_location gs://<YOUR_BUCKET_NAME>/staging \
  --region us-central1 \
  --job_name tweeps

## Lesson Transcript
```text
Hello Cloud gurus.
Welcome to our next Cloud Dataflow lab
where we'll construct a transformational streaming pipeline
additionally, using Pub/Sub and BigQuery.
To see the power of streaming pipelines in Dataflow
we'll start by creating a fake social media feed.
This will be a quick Python script that generates tweeps
for our imaginary micro blogging site Tweeper.
Then we'll publish those random messages
continuously to Pub/Sub.
We've done enough Java for one day, so we'll use Python
to create a streaming Dataflow pipeline
that will consume the messages as they arrive
with a Pub/Sub subscription.
The pipeline will demonstrate how transformations
and other user functions can be useful when ingesting data.
First, by converting the JSON string of the Pub/Sub message
to an object that BigQuery understands,
then checking the message against a known list of bad words
that we'd rather not see on our social network.
If we spot one, we'll add an element to our data
to flag the message for further inspection.
Then we'll write the transform data to BigQuery.
All you need to follow along with this lab is a GCP project
and access to the Cloud Console.
We'll run commands from the Cloud Shell
and write our code in the Cloud Shell Editor.
This is quite a code heavy lab,
but feel free to download the pre-written code
from the resources section.
As soon as you are ready, join me in the GCP console.
Okay, Cloud gurus. Here we are back in the GCP console.
You should have the Dataflow Pub/Sub
and BigQuery APIs enabled in your project
to follow along with this lab.
You can check these by going
to the APIs and Services Dashboard in the menu,
selecting Enable APIs and Services,
and searching for the name of the API in the search bar.
If you get an option to manage an API here,
it's already enabled.
With that out of the way,
let's create some resources for our lab.
First, we'll go into the Storage menu
and create a Cloud storage bucket for Dataflow to use.
Click Create Bucket.
I'll call mine tim-acloud-guru-dflow-stream.
Make sure you name yours something else.
You can leave all of the other options at the default.
Make a note of this bucket name and click Create.
Next we'll create our Pub/Sub topic.
Go to Pub/Sub in the menu,
click Create Topic and we'll call our topic tweeps.
Now we'll create a subscription for our topic.
Dataflow can actually dynamically create a subscription
for you every time a pipeline runs.
But I prefer to create the subscription myself
so I'm always using the same one.
Let's call our subscription tweep reader.
You can leave everything else
as the default and click Create.
One more thing before we get to Dataflow.
Let's create the table in BigQuery
where all the tweeps will end up.
Select BigQuery from the big data menu.
Then with your project selected
on the left click Create Dataset.
We'll call the dataset tweeper
after the name of our pretend social network.
Then inside the data set, click Create Table.
We'll call the table tweeps.
Toggle the button to edit the schema as text,
then type in the following.
Each of our tweeps will contain a timestamp,
an arbitrary ID, the text of tweep itself,
the handle of the person who tweeped,
and we'll also have a column called flagged,
which will indicate if the message contains any words
which have flagged it for our attention.
Click Create Table.
And now that we've got our supporting resources set up,
let's open the Cloud Shell
and the Cloud Shell Editor to start creating
our fake tweep stream and our pipeline.
First, we'll create a directory for our code
and a runtime environment using virtualenv.
Virtualenv will keep an isolated Python runtime
and manage the libraries we need so we can keep
this separate from any other code that we are using.
You can activate the environment
by typing source bin/activate from inside the directory.
Let's create a file called requirements.txt
and list the Python libraries we're going to be using.
We'll need the Faker library to create some fake data,
the Google Cloud Pub/Sub library,
and the GCP version of the Apache Beam library.
Let's save that file.
We can install these Python libraries with pip
just type pip install -r requirements.txt.
Pip will download, compile
and install all of these libraries for us.
Once that's done, I'll just clear the terminal.
So now we need to write two Python programs.
One to create our fake social network
and one to run our Dataflow pipeline.
Let's start with our social network.
Create a new file called tweeper.py.
You can type in along with me.
Just pause the video whenever you need to catch up
or you can grab the complete code
from the resources section.
To start with, we just import the libraries we need.
Then we set the GCP project ID
and don't forget to change it to your project ID
and the name of the Pub/Sub topic we just created.
Next, we create an empty list for usernames.
We're going to create a finite list
of users for our platform rather
than generate a new one for every tweep.
This makes it slightly more realistic
and gives you another way to sort the data
in BigQuery at the end.
Next, we create an instance of the Faker Python library
which will generate random data for us.
Then we create an instance
of the Pub/Sub publisher client and define the topic path.
This short function will first UTF-8 encode
and then publish a message for us.
This next function creates a fake tweep.
The structure is loosely based
on another social media platform you may have heard of.
We take the time now, generate a random ID for the tweep,
a random sentence and pick a username
from our pre-generated list.
We haven't actually generated that list yet, but we'll do it
in our main method before this function gets called.
And here's our main function.
First, we populate that list
with 200 usernames using the random simple profile function
in the Faker library.
Once that's done we start an endless loop where we just keep
on tweeping with a half a second break in between each one.
And that's it for our fake social network.
When we run this in a moment
our Pub/Subtopic will soon fill up with fake tweeps.
Let's save the file.
And now we need to write our pipeline script.
Create a new file called pipeline.py.
Just like before, we start by importing the libraries
we need and setting some variables.
Don't forget to use your project ID here.
Then we set the subscription ID and the schema
of the BigQuery table that we set up earlier.
Now we'll write some user-defined functions
that we'll call in our pipeline.
This one is quite simple.
Pub/Sub messages are consumed as strings
even though we know they're actually JSON encoded,
so we just use the JSON library to convert them
to a Python dictionary object.
This makes it much easier to manipulate them
and Apache Beam also supports writing Python dictionaries
directly to BigQuery.
Note, we have to import the JSON library
inside this function, not globally.
That's because Beam may distribute the processing
of our functions, so we can't always assume
that any global definitions will be passed around.
Our next function deals
with the timestamp included it in our tweeps.
This is a nice little example of how user-defined functions
in Dataflow can be used to transform data
for the data sync it's being written to.
BigQuery expects timestamps to be in a specific format
which is slightly different from the one
that we have in our tweep JSON data,
so we just use some Python magic to change the format.
And again, we have to import the Datetime library
locally inside this function.
Our last user-defined function
as a new column of data called flagged
to the tweep that has come in and checks all
of the words in its text against a list of known bad words.
If it's spots one, the element flagged gets changed to true.
We are hard coding the list of bad words
that we want to flag up in our system here, pretending
that our site cares about its social responsibility
and wants to double check any
potentially dangerous tweeping that's going on.
It's all make believe, remember?
The list of words might also look quite innocuous.
That's because the Faker library we're using
will generate sentences that are made up
of the most common 1000 words
in the English language and nothing more.
Outta that list, these three are
about the most risky I could find.
Now that we've written all of our functions,
let's see how they come together in the pipeline.
In the main function,
we first pass any command line arguments
that have been invoked as part of the script.
We need to do this to read some extra options
that are required when running the pipeline
on the Dataflow service.
Then we create the pipeline and simply define each step.
First, we use read from Pub/Sub to grab messages
from our Pub/Sub subscription.
Then we decode the incoming message
and use our pass Pub/Sub function
to turn the JSON into a Python dictionary.
Then we run the fixed timestamp function
and finally the check tweep function.
Once all of those things are done
we write the output to BigQuery.
It's really that simple.
When we run this as a streaming pipeline in Dataflow
the Dataflow service will take continuous batches
of messages and push them into BigQuery,
distributing the work required to do this
amongst its workers.
Note that we're applying our functions
to the pipeline using beam.map
which is the most simple approach.
We could alternatively use pardu
to start parallel processing of elements
in each P collection,
but there are some additional programming considerations
for pardu that are outside the scope of this lab.
Let's save the file and see our pipeline in action.
First, we'll start our tweep feed.
Just run python tweeper.py.
We'll leave this running for about 10 seconds
or so and that should give us around 20 messages
in our Pub/Subtopic.
Hit Control + C to stop it.
Now we can test our pipeline locally
by running python pipeline.py --streaming.
Don't worry about this deprecation warning that comes up.
It can be safely ignored.
Note that the pipeline runs locally by default
because we haven't specified the Dataflow runner.
We'll leave this running for a few minutes.
Okay, now let's hit Control + C to stop the pipeline.
If we go back to BigQuery in the GCP console
we should now be able to preview the table
and see the tweeps that were published.
They've been transformed and cleaned up
and passed through our bad words checker
before being written to BigQuery.
There aren't many here yet, so nothing has been flagged up.
So now let's put our pipeline into production.
First, we'll start the tweep feed again
with python tweeper.py.
We'll leave this running and it will tweep twice a second.
Now open a new Cloud Shell.
We'll need to go back into our tweeper directory
and activate virtualenv with source bin/activate.
Now we'll run our pipeline again
but we'll direct Apache Beam to use cloud Dataflow
as it's backend for distributing the work.
We'll do this
with python pipeline.py --streaming --runner Dataflowrunner
to specify the backend.
Then we give it our project with --project
and don't forget to use yours.
Then --temp_location and the bucket we created earlier
appended with a temp directory.
Then --staging_location and the same bucket
but appended with a staging directory.
And finally --job_name to give our Dataflow job a name,
which we're calling tweeps.
Hit return, and again, don't worry
about these deprecation warnings that come up.
The job has been submitted to Dataflow
so let's take a look at it in the console.
Go to Dataflow in the big data menu
and then select the tweeps job.
Here we can see the visualization of our pipeline
and all of the stages we defined.
It will take Dataflow a few minutes
to scale up its workers and start processing data.
Let's go back to BigQuery
and look at the details of our table.
I'll just give us a bit more room and scroll down.
Note that incoming rows are contained
in the streaming buffer
before they are fully written to the table.
We'll leave this running for about five more minutes.
So we should now have a decent enough bunch
of messages in our table.
Let's refresh this page and see if we have.
Not bad. We now have 899 rows in our streaming buffer.
Let's hope everyone is tweeping nicely
and not using any of the bad words.
We can run a quick query to check.
In the Query editor type SELECT * FROM ` the name
of your project, then the name of the dataset, tweeper
and the name of the table, tweeps WHERE flagged = TRUE.
Click Run and it will show us all of the flagged tweeps.
Oh dear.
People on the internet just can't help themselves.
Okay, Cloud gurus.
You've successfully got a streaming Dataflow
pipeline running that's helping your social network
to transform and ingest data.
Let's clean up this lab before we finish.
Go back to the Dataflow window,
select the job and click Stop.
You have the option to do this gracefully
but it's fine to just cancel it immediately.
This will be the quickest way of shutting it down.
Go back to Cloud Shell
and in the terminal where the pipeline is running,
you'll soon see an error as Dataflow quits.
But don't worry about that.
Go to the other terminal and quit the tweeper program.
Be warned that if you leave this running,
you'll keep publishing to Pub/Sub and eventually
that will push you out of the Pub/Sub free tier.
To delete the Pub/Sub resources themselves
just go to Pub/Sub in the GCP menu.
First, delete the subscription,
then delete the topic itself.
Now we can go to Cloud Storage
and delete the bucket.
Finally, let's go back to BigQuery
and we can delete the entire data set.
And that's it.
Dataflow will clean up any
of the compute resources it created all by itself.
Thanks for watching Cloud gurus. I'll see you next time.

```

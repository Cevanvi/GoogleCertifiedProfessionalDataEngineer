## Resources 

- [Concepts](https://cloud.google.com/dataflow/docs/concepts/beam-programming-model) 

## Lesson Transcript
```text
Hey Cloud Gurus, in this video,
we'll be looking at some of the more advanced concepts
in cloud Dataflow.
First, we'll cover what is known as the event time,
we'll then look at windowing,
and some of the different types of windowing functions.
We'll look at the concept of a watermark
and finally we'll cover triggers.
Let's get going.
The time a data element occurs
is determined by the timestamp on the data element itself.
This should be contrasted with the time
the actual data element gets processed
at different points within your pipeline.
The event time is the time that a data element occurs
and it is determined by the timestamp
on the data element itself.
This should be contrasted with the times
that the data element is processed
at different points within your pipeline.
It takes time for the element to move within your pipeline.
The processing times refer to the different times
at which your element is processed
during its transit through your pipeline.
The processing times are greater than the event time.
Let's now consider windowing.
Imagine you have an unbounded PCollection,
that is a stream of events.
Windowing is particularly useful in this situation.
A windowing function is assigned to a PCollection.
A windowing function allows you
to subdivide the elements of a PCollection
according to their timestamps.
The point of windowing is to enable grouping
or aggregation operations over unbounded collections.
This is done by grouping elements into finite windows.
The unbounded PCollection is therefore processed
as a succession of finite windows.
Let's take a look at the different types of windows.
The fixed time window is the simplest type of window.
A fixed time window represents
a constant non-overlapping time interval.
Fixed time windows are for a constant duration.
For example, one minute.
Fixed time windows are also non overlapping.
So each element within the stream
is assigned to a single window.
Sliding windows also represent time intervals
within a data stream.
However, sliding windows can overlap.
Because of overlapping,
it is possible for an element
to belong to more than one window.
Sliding windows are particularly useful
for taking running averages of data.
For example, one could have 1 minute windows
starting every 10 seconds.
Next we have session windows.
A different session window is created in a stream
when there is an interruption in the flow of events,
which exceeds a certain time period
depicted here by the red arrow.
Session windows apply on the a per key basis.
Session windows are particularly useful
for irregularly distributed data, with respect to time.
By default, all the data in a Pcollection
is assigned to a single global window.
If you don't change the windowing function
on a PCollection, a single global window will apply.
The windowing function for a PCollection
is assigned by applying a window transform
to that PCollection.
When setting a windowing function,
you may need to also set a trigger for your PCollection.
We'll discuss triggers a little later on.
Let's talk about watermarks.
As we saw earlier,
there is a time lag between the event time
and the processing times within your pipeline.
Within your pipeline,
Beam needs to collect all the data
based on the event timestamp.
To assist with this beam tracks a watermark.
The watermark is the system's notion
of when all the data for a certain window
can be expected to have arrived.
Once the watermark moves past the end of a window,
any further data elements of that arrive
with a timestamp within that window
are considered to be late data.
You can determine how late data is treated.
Let's now consider triggers.
Triggers are what Beam uses to determine
when to give off aggregated results
from each window.
By default, eam outputs the aggregated results
when it assumes that all data has arrived.
You are able to set specific triggers
for your PCollections to override this default behavior.
There are a number of pre-built triggers that you can use.
There are event time triggers,
which operate based on the event time.
Beam's default trigger is event time-based.
There are processing time triggers,
which operates based on the processing time.
There are data driven triggers.
Data driven triggers are fired
when the data for a particular window
meets a certain criterion.
There are also complicit triggers.
Composite triggers combine the other triggers
in different ways.
Okay Gurus, that's it for this video.
On the more advanced topics within cloud Dataflow
See you in the next video.

```

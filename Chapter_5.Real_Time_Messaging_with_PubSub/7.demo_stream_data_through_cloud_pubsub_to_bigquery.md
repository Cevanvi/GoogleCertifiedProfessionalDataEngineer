## Resources

- [transactions](resources%2Ftransactions.py)
- [mock_data](resources%2F1628490330578-MOCK_DATA.csv)

## Demo Instructions

Demo Steps: Stream Data through Cloud Pub/Sub to BigQuery

1. Create a bucket matching your project name (e.g. gs://playground-s-11-78f5b410)
2. Leave other settings as default
   #UPDATED STEP#:
3. Go to IAM and copy the compute service account name to a text editor (e.g.
   526273785053-compute@developer.gserviceaccount.com)
   #UPDATED STEP#
4. Create a Pub/Sub topic named "purchases"
5. Copy the default subscription name to a text editor (e.g.
   projects/playground-s-11-78f5b410/subscriptions/purchases-sub)
6. Go to Big Query and create a dataset
7. Create a table and add the schema, and copy the table ID (e.g. playground-s-11-78f5b410:trends.purchases)
   UPDATED STEPS:
8. Create a data-flow job with max workers=1, number of workers = 1, Machine type n1-standard-1
9. The job will create, but then throw a credentials issue (this is because the Dataflow API had not finished enabling
   before the job tried to run)
10. Click on CLONE and then create another copy - the second job will run correctly.
    #UPDATED STEPS#
11. Open Cloud Shell, run sudo pip3 install google-cloud-pubsub and the open the Shell editor, to create transactions.py
    and copy in MOCK_DATA.csv as per the lesson video
12. Run python3 transactions.py
13. Use Big Query to query the data which will now have been synced into the purchases table.

## Lesson Transcript

```text
Hello Cloud Gurus and welcome to this lab
where we'll stream some data through Pub/Sub
to BigQuery for later analysis.
As we've discussed, Pub/Sub is perfect
for handling incoming data globally at massive scale
that can then be processed or dealt with later
by any number of subscribers.
In this lab, we'll create some simulated data
for shopping cart transactions.
We'll then publish this data to a Cloud Pub/Sub topic.
We'll configure a Cloud Dataflow job to process the data
from a Pub/Sub subscription
and write it to a table in BigQuery.
Then we'll view the data and interact with it in BigQuery.
Here's a simple architecture diagram
of what we're going to build.
Transaction data will be sent as messages
to a Pub/Sub topic.
From their data flow, we'll pull the messages
via subscription and write them into BigQuery
for later analysis.
Rather than write the data directly to BigQuery,
we gained some great advantages by using Pub/Sub
as a message bus.
First, if either end of the transaction
experiences a problem, Pub/Sub can buffer messages
so they're not lost.
Second, we can optionally transform the messages
in Dataflow.
This can be handy for things like time windowing.
And third, because we've decoupled the data,
we could use it with multiple subscribers.
In this case, we're just setting up BigQuery
to analyze some shopping trends,
but there are probably going to be other parts of our stack
that need this data as well.
We can easily achieve this with extra subscribers.
So back to this lab.
All you need to follow along is a GCP project.
We'll work inside the cloud console
with the Cloud Shell and Cloud Shell editor.
We'll need to write a small bit of Python code,
but you can also download the code
from the resources section, if you need to.
Okay Cloud Gurus,
get your GCP project ready and let's get started.
Okay Cloud Gurus, here we are in our GCP project.
Now as discussed,
we're going to see how some transactional data can stream
through Pub/Sub into BigQuery,
but in order to demonstrate that,
we're going to need a bunch of test transactional data.
To generate that test data,
we're going to use this site called mockaroo.com.
You can follow along with me here
and generate your own data, or if you prefer,
you can just download the CSV file I've already created
from the resources section.
So in Mockaroo, you can generate test data very easily
by just choosing what field you want in your data.
Let's leave ID.
We'll change first name to full name,
then click the type here,
and select the full name generator.
We'll delete the last name field.
Let's leave email address, but delete gender and IP address.
Now we'll add a few more fields just for fun.
This doesn't have to be a super accurate schema
as it's just for demo purposes.
Let's add a field for country and specify the country type.
We'll also add some countries to limit this to
so we can do some fun searches later.
Let's limit this to the United States,
the United Kingdom, and France.
Don't forget to separate these with commas.
Let's add a date field, change this to the Date type,
and we'll specify a range
from the start of October to the end of October.
We'll also change the date format
to make it compatible with BigQuery.
Then we'll add a card type using the Card Type generator,
and we'll add an amount.
We'll set this to the Money type.
We'll specify that we want values between 1 and 10,
but we'll remove the currency symbol
so we can keep this as a numeric field.
And finally, we'll add a product field.
We can look under the product section
to choose the type here.
The Product (Grocery) type will generate all sorts
of random products for us.
Again, this doesn't have
to be a super accurate representation.
It just needs to give us enough data to play with
as we see it stream through our system.
Leave these settings at 1000 rows in CSV format
and click Download.
Hang onto this file for now and we'll come back to it
in a moment.
Okay, so now we've got our test data.
Let's go back into our GCP console.
The next thing to do is create a GCS bucket
for some temporary files.
This will be used in a moment by our Dataflow job.
Select Storage from the menu and Create Bucket.
Don't forget the bucket name has to exist
in a global namespace.
So for simplicity, let's name this after our GCP project
with the suffix temp.
Just to clarify, use your project name here, not mine.
You can accept all the defaults and click Create.
Now we're going to copy the name of this bucket
to the clipboard and make a note of it.
We'll use this in a later step in the lab.
I'm going to copy mine to Google Keep
for the purposes of this lab, but you can use a text editor
or whatever note taking app you like.
Back in the GCP console,
we'll now create our Pub/Sub topic.
From the GCP menu, select Pub/Sub
under the big data section.
If this is the first time you've used Pub/Sub
in your project, it will enable the API for you.
Click Create Topic
and we'll call our topic purchases.
Let's scroll down on the topic page here
and click Create Subscription.
We'll call this subscription trends.
Remember we discussed the decentralized nature
of using a message bus.
So theoretically, if all purchases are published
to this topic, there could be multiple subscribers
that perform different functions,
such as billing or shipping.
This subscription is going to be for analyzing trends
in purchases.
This will be a pulse subscription,
and we can leave all the other settings as default
and click Create.
On the subscription details page,
click this icon to copy the full subscription path
to your clipboard.
Then make a note of this as well.
Next, we need to create our BigQuery dataset
and table to receive all of our data.
Let's go to BigQuery in the Big Data section,
click on your project name and click Create Dataset.
We'll call this dataset trends.
Then go into your dataset and click Create Table.
We'll call the table purchases.
Now it's very important
that we define the correct schema here,
which must match the one we created for our test data.
Add each of the following fields in this order,
id as an integer,
full_name as a string,
email as a string, country as a string,
date, but changes to the data type,
card as a string,
amount, but change this to the numeric type,
and product, but leave this as a string.
Then click Create Table.
Once that's done, click your new table,
go into the details tab,
scroll down and copy the full table ID.
Then we'll make a note of that as well.
Now we've got our BigQuery table ready to receive data.
Let's set up the Dataflow job
that will ingest the data for us.
We'll go to Dataflow in the Big Data section.
Select Create Job from template.
Let's call this job purchases-to-bq.
Dataflow has all kinds of templates for us
we can use off the shelf.
We're going to choose Cloud Pub/Sub subscription
to BigQuery.
This template will set up a job that continually pulls
from our Pub/Sub subscription
and writes the data into BigQuery.
Once we've selected that template,
the form opens up to ask us for a bit more information
and here's where we're going to use all that information
that we've been writing down.
Let's just close this pop-up.
First, copy the full path of the Pub/Sub subscription.
Then paste it in here.
This is the subscription that Dataflow will pull from.
Then copy the name of the BigQuery table
and paste that in here.
This is the output table where data flow
will write its data.
And finally, copy the name of the GCS bucket you created.
Dataflow needs some space to write temporary files
while it works.
You need to prefix this with gs://.
And it's normally a good idea to add a directory name
like /tmp, just to keep things neat and tidy in the bucket.
Once you filled in this form, click Run Job.
Dataflow will show you a visual representation
of the job you've just created.
Primarily, it's just converting JSON
to table rows and inserting them,
but it also contains various stages for error checking
and handling.
Now our Dataflow job is running.
Let's simulate some transactions.
For this, we'll need to use the Cloud Shell.
We'll open the Cloud Shell.
And then the Cloud Shell editor.
We're going to write a Python script called transactions.py.
You can follow along with me or download this script
from the resources section.
But if you do, make sure you update the reference
to your project name in a moment.
So let's create a file called transactions.py.
We start by importing some libraries, JSON, CSV,
and the GCP Pub/Sub library.
Then we set variables for the project name,
make sure you use yours here and the topic name.
Then we set the file name of the data file we downloaded
from Mockaroo.
We create the publisher client and set the topic path.
We open the CSV file.
Then we use Python CSV reader to grab the rows.
Then for each row,
we create a JSON representation of the data
and publish it to our topic, utf-8 encoded.
If you're comfortable with Python,
feel free to use the time library
and add some random delays here
to simulate the fluctuating flow of transactions.
For simplicity, I'll just the loop unhindered
and all of these transactions will be sent to Pub/Sub
one after the other.
Let's save that file.
And now we'll upload that CSV file that we downloaded
right at the beginning of the lab,
it will be uploaded to your Cloud Shell home directory.
So make sure that's the location of your script as well.
Just select Upload File from this menu
and choose the CSV you downloaded earlier.
Before we run the script,
we just need to install the Google Cloud Pub/Sub library.
We can do this with sudo pip3 install google-cloud-pubsub.
As we're just using one library,
we'll just use PIP directly
rather than set up a whole virtual environment.
Let's clear the shell, and now we can run our script
with Python transactions.py.
The script completes almost immediately
and has sent 1,000 messages to Pub/Sub.
Let's go back into the GCP console.
As quickly as we published those messages,
our Dataflow job has pulled them from its subscription
and written them to BigQuery.
So now let's go into BigQuery and do some simple analysis.
First, let's see how many individual items were bought
by each country.
With SELECT country, COUNT(country) as purchases
FROM trends.purchases and GROUP BY country.
This is why we limited our data
to just those 3 countries.
We can see that France was slightly ahead
of the United States as the top purchaser.
So let's try another example.
Let's see which items were bought in France
in the last 2 weeks of the month,
sorted by the most expensive first.
We'll delete the previous query
and type SELECT product, amount FROM 'trends.purchases'
where DATE is greater than the 15th of October, 2019
and country = 'France'
and ordered by amount in descending order.
And it looks like the most expensive item
was some sort of container followed by some pernod
and half a chicken.
Interesting purchases, France.
Great job, Cloud Gurus.
We've successfully demonstrated
how we can decouple data ingestion with Pub/Sub,
stream data through Dataflow and analyze it with BigQuery.
Now there's a few different resources to delete
to clean up this project.
First, let's stop the Dataflow job.
Go back to Dataflow in the Big Data menu.
While the job continues to run,
it will consume compute resources.
So it's really important that you stop this one first.
Just go into the job details page and click Stop Job.
You can choose to cancel the job immediately.
Next let's jump back to BigQuery and delete our dataset.
You can optionally keep this around
if you want to practice some further queries,
but please review the BigQuery pricing
if you're going to do this.
If you want to delete it, just click Delete Dataset.
Then in Pub/Sub, we'll delete our topic
and our dangling subscription.
Finally, let's remove the temporary GCS bucket we created.
Okay, Cloud Gurus.
If you have any questions about this lab,
please feel free to ask me in the course forums.
If not, thanks for watching,
and I'll see you in the next video.

```

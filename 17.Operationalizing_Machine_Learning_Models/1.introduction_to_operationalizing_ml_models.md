## Lesson Transcript
```text
Hello Cloud Gurus.
In this chapter,
we'll cover the high-level theory
of operationalizing machine learning models.
We're going to start with this introduction video,
where we'll discuss the objectives
of operationalizing ML models.
Then in the following videos,
we'll look at how we can use KubeFlow
to help us achieve some of these objectives.
And finally,
how we could also do the same thing
with Google's AI platform.
So, what exactly does it mean
to operationalize machine learning?
Broadly speaking,
it refers to the process of deploying predictive models
to a production environment
together with ongoing measurement, monitoring
and improvement of those models.
Operationalizing is basically a set of best practices
for managing a machine learning workflow or pipeline,
throughout its entire lifecycle.
Now, there are different definitions and perspectives
on the design of machine learning pipelines,
but broadly speaking,
you can group together the various considerations
into the stages of deployment, scoring, logging, monitoring,
and retraining.
Let's take a close look at each of these.
We start with deployment.
Regardless of the many different product
or infrastructure services at your disposal,
there are some high-level considerations
for how you should manage deployments.
In general, you should have separate infrastructure
for model training and model deployment.
The infrastructure required to train a model,
usually differs from the infrastructure required
to process requests anyway.
But in both environments,
the full data preparation pipeline
should be included in the deployment.
This way, requests that are sent to the model,
are constructed in the same way as requests
that were sent to the training data.
Requests sent to the production model,
should also be validated
wherever possible to ensure
that they meet the broad constraints and assumptions
that we used during training.
And wherever possible,
models should be lightweight and portable
and designed to run anywhere.
This can be achieved with KubeFlow,
which we discuss in the next video.
Once your model is in operation,
scoring engines can be used to evaluate its effectiveness.
To help enable this,
you should think about making your model interface flexible
so that it can support requests that are real-time
or streamed or offline via batch processing.
Models can obviously be produced using different frameworks,
which can also make scoring a challenge.
Consider the use of standardized frameworks
like the predictive model markup language
or the open neural network exchange.
To assist with model scoring of performance,
proper logging should be in place
to identify any issues with the model itself,
as well as for the evaluation of the model performance
over time.
Important metrics to capture,
are the original input request and output response,
the model version,
any errors validating the data contained in the request,
the predicted output and the overall latency
of response times from the model.
These metrics can be used in prior active monitoring systems
to keep an eye on how accurately a model is working
as well as if it's performing fast enough
to meet any necessary SLAs.
Metrics that can be separated and graft
to represent the predictive performance,
which would highlight any decay
in the model's predictive ability
and processing performance,
which could indicate issues
with the underlying infrastructure that forms the pipeline.
Finally, know that all models will eventually
need to be updated or retrained
as their predictive power will decrease due to changes
in the external world.
Some causes of this include data drift,
where there are unforeseen changes in the input data
that affect the performance of the model
and concept drift where the statistical properties
that our model is trying to predict,
have shifted in the external world to a point
where the model no longer applies.
Depending on how seriously a model has decayed,
there are different approaches
to dealing with these situations.
Models can be retrained with essentially
the same feature design,
but a new set of input data.
This retraining can even be automated
based on a threshold of the aforementioned metrics.
Alternatively, if a model is performing very badly,
it may need to be recreated with a modified
or new set of features,
based on an evaluation of those external changes.
Okay Cloud Gurus,
that's a really high-level overview
of the concepts involved.
Join me in the next video,
where we'll look at one way to accomplish
all of this with KubeFlow.
```

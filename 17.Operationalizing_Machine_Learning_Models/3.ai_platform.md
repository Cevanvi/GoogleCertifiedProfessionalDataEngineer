## Lesson Transcript
```text
Welcome back, Cloud Gurus.
So we've seen how Kubeflow works as a standalone
Kubernetes-based operationalizing platform for ML models.
Let's see how we can integrate
some of the features of Kubeflow with other GCP services
in AI Platform.
AI Platform, formerly known as Cloud ML Engine,
is a suite of additional APIs and services
combined with other big data and ML products in GCP
to make a single platform right inside the GCP console.
It provides all of the tools you need
for an operationalized ML pipeline,
including ingesting data, preparing and pre-processing data,
performing data discovery,
developing and training ML models,
testing them, and finally deploying them.
Operational models can be deployed to VMs,
to Kubeflow services in Kubernetes,
or to manage TensorFlow jobs in AI Platform itself.
Notebooks, modules, and pipelines can be templated
from the new community AI Hub,
Google's hosted repository of plug and play AI components.
Let's have a quick tour of each stage of this pipeline
and see what AI Platform offers us.
For storage ingestion,
we can simply use Google Cloud Storage
or the Cloud Storage Transfer Service.
For data preparation, we can use Cloud Dataflow
or Cloud Dataproc or even BigQuery.
We can also use Cloud Dataprep,
a third-party service provided by Trifecta
that provides a visual browser-based interface
for cleaning and preparing data ready for processing.
Dataprep recipes can be created without any programming
or SQL knowledge and exported as Cloud Dataflow jobs.
At the pre-process stage,
AI Platform's built-in data labeling service
can also label training data by applying classification,
object detection, and entity extraction
for images, video, audio, and text.
At this point, you could optionally import the labeled data
directly into AutoML to train a model.
To develop and train our own ML models,
AI Platform provides several options.
The Deep Learning VM is a pre-configured
compute engine instance optimized for data science
and machine learning tasks
with key ML frameworks and tools pre-installed.
It also supports GPU accelerated data processing
out of the box.
AI Platform Notebooks
are instances of the deep learning VM image,
which also include a Jupyter Notebook installation
made available through a public URL.
AI Platform Training provides a fully-managed platform
to run training resources for your ML model.
You simply build a Python application
that contains the necessary steps to train your model.
Then deploy it to the platform
within the resource constraints that you've configured.
This avoids you having to manually deploy any VMs yourself
as this is handled for you by the platform.
Alternatively, AI Platform supports Kubeflow
so you can take your ML pipeline
and run it in your on-premises Kubernetes environment.
You can perform extensive testing on your models
using the capabilities of the TensorFlow Extended Framework,
and then when you're ready to deploy your models,
you can do so with the same AI Platform API
that you used during training,
except now you're using AI Platform Prediction.
Or alternatively again,
you can simply export the model to Kubeflow.
Finally, AI Platform is linked with Google's new AI Hub,
a hosted AI repository of plug and play AI components,
including end-to-end AI pipelines
and out-of-the-box algorithms.
This service is currently in beta,
but provides a discovery platform
for one-click deployment of common pipelines
and other AI content developed by Google.
An Enterprise version also offers private team-based
sharing of workflows, models, and more.
Okay, Cloud Gurus, that about does it for AI Platform.
For the purposes of the exam,
AI Platform, or Cloud ML Engine as it was formerly known,
is probably the easiest way to quickly run
a ready-to-go model in a managed service.
Thanks for watching and I'll see you next time.
```

## Lesson Transcript
```text
Hello, Cloud Gurus.
Welcome to this video, where we'll take a quick look
at KubeFlow, a complete platform for developing,
deploying, and operationalizing machine learning models.
KubeFlow can be thought of as the ML toolkit for Kubernetes,
as that's the platform that it's built on top of.
It provides all of the tools you need
to support the phases of the ML operationalized workflow
from experimenting with data in Jupyter Notebooks,
to tuning and training models with TensorFlow,
and then serving the models with exported model containers
and monitoring their performance.
There are various ways to interact with KubeFlow,
depending on your requirements,
but the easiest way is through its own built-in web UI,
or you can use kfctl, the dedicated command line interface.
Various components of KubeFlow also offer
their own APIs and Python SDKs.
As I said, KubeFlow supports
the entire operationalizing pipeline,
starting with the experimental phase,
where you identify the problem you want to solve
and begin to build a pipeline in KubeFlow.
Then, you'll choose a machine learning algorithm
and begin to code your model.
KubeFlow provides the most popular frameworks,
such as PyTorch, scikit-learn, TensorFlow, of course,
as well as the XGBoost gradient boosting library.
Then you can experiment with training data,
using Jupyter Notebooks.
KubeFlow also provides a tool called Fairing,
which packages a Jupyter Notebook and deploys it,
either as a KubeFlow training job on Kubernetes
or straight to AI platform on GCP.
You can also tune the hyperparameters of your model
using, for example, the built-in Katib system.
Then moving onto the production phase,
you begin your pipeline to transform your data
ready for production.
Then start the formal model training
using built-in functionalities such as MPI
for all reduced style distributor training,
Apache MXNet, for deep neural networks
or PyTorch, for vision or natural language processing.
Next, you can serve the model by scheduling it
as a TensorFlow serving deployment,
as a KubeFlow serving objects on top of Knative,
or even to an NVIDIA TensorRT inference server.
KubeFlow then provides you with all the tools you need
to continually monitor your model, with built-in TensorBoard
and even an extensive Metadata store for metrics.
So the pipeline we've described
is a description of a machine learning workflow,
which includes all of the components
and how they relate to each other.
An individual pipeline component is a self-contained
set of user code packaged as a container
and performs one step in the overall pipeline.
Okay, Cloud Gurus, that's all you need to know
about KubeFlow for now.
It's unlikely to be covered in depth on the exam,
but it's definitely worth looking at for your own projects.
I'll see you in the next video.
```

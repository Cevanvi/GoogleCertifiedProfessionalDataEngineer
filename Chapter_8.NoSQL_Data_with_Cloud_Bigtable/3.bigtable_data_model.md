## Resources 

- [Bigtable Garbage Collection Defaults](https://cloud.google.com/bigtable/docs/garbage-collection#defaults) 

## Lesson Transcript
```text
Hello, Cloud Gurus.
Now we know a bit more about the architecture
of a Cloud Bigtable instance,
let's learn about how Bigtable actually stores your data.
Here's the high-level view of that table
we looked at in our first lecture.
Let's swap this out though
for something with some more realistic data.
So here's a table that may represent
some fictional public transport information.
Remember, this is the row key for each row
and the row key is the only thing
that gets indexed in the table.
Our table can have multiple column families,
which are logical groups of table columns.
Using column families can make it more efficient
to retrieve only the data we need rather than an entire row
and you can have up to about 100 column families in a table.
The name of the column itself
is also referred to as the column qualifier.
Let's look at just one row of this example table.
In a 2 dimensional table,
at the intersection of row and column, we find a cell.
However, Bigtable is a 3 dimensional table.
Every cell is written with a timestamp.
And if a cell has mutated, new values can be written
without overwriting the original values.
How much cell history you store
and for how long is configurable,
which we'll look at in a moment.
This makes it possible to retrieve data
with detailed granularity,
specifying a row, column, and timestamp.
The value of any cell you retrieve
is always an array of bytes.
We can sum up this model with this expression.
Your table will inevitably contain many, many rows,
perhaps thousands, possibly millions or billions.
As we've previously mentioned,
Bigtable will intelligently split tables into tablets,
constantly optimizing them for performance.
Within a tablet, rows are sorted lexicographically,
essentially alphabetically by row key.
This allows you to optimize searches
across specific ranges of row keys,
but this also means
that the design of your row key convention
is probably the most important part
of your overall Bigtable design,
so much so that we'll dedicate
most of our schema design lecture to row keys
later in this chapter.
Before we get to that,
some other features of the data model to be aware of.
Bigtable has atomic operations by row, but only by row,
as opposed to the model of some transactional databases.
This means, for example,
that if you update 2 rows in a table,
it's possible that one of the updates may fail
and leave a row unchanged.
The other row will be none the wiser.
This is something to bear in mind
when designing applications to work with Bigtable.
Bigtable is also a sparse table system.
That means it won't hurt
to have lots of column families and columns,
even if they don't apply to every entity in the table.
Empty cells won't consume any space in the database.
For optimal performance,
you should try to keep within recommended row sizes.
Individual cells should be no larger than 10 megabytes,
which includes all previous timestamp diversions
of that cell's value.
Overall, a row should be under 100 megabytes,
although Bigtable will support rows
up to 256 megabytes in size.
To help you keep these sizes under control,
Bigtable uses garbage collection
on timestamp data in cells.
So as we know,
every cell can have multiple versions of its data.
These can be stored with server-recorded timestamps
reflecting the time the data was written,
or you can optionally set a custom timestamp on a cell
when writing data.
You can also use sequential numbers instead of timestamps
to track previous versions of a sales data.
Bigtable can garbage collect older versions of this data
to keep the overall size of the table down.
Garbage collection is defined by a policy that is created
when you create the column family.
You have to choose between 2 extremes here.
Creating a column family with the HBase client
will set the policy to only retain
the latest version of a cell.
If you use any other client library,
it will set the column family to store infinite versions,
but you can then update a policy
to expire based on a specific age
or expire based on a specific number of versions
if using sequential numbering as opposed to timestamps.
There are some caveats
when making changes on replicated tables,
so it's best to define these policies
before adding additional clusters to your instance.
There are some caveats when making changes
on replicated tables,
so it's best to define these policies
before adding additional clusters to your instance.
Okay, Cloud Gurus,
we still need to deep dive on row key design,
probably the most important component of Bigtable
to consider as a data engineer
and something that is sure to pop up in the exam,
but let's take a break from all of theory for a moment
and join me in the next video
where we'll conduct our first lab
and get hands-on with Cloud Bigtable.

```

## Resources 

- [Apache Beam overview](https://beam.apache.org/get-started/beam-overview/) 
- [Apache Beam programming model](https://cloud.google.com/dataflow/docs/concepts/beam-programming-model)

## Lesson Transcript
```text
Hey Cloud Gurus.
In this chapter,
we'll be looking at Cloud Dataflow.
You should know Cloud Dataflow well for the exam,
particularly how it is used in real-world situations.
In this chapter,
we'll start by looking at an introduction to Cloud Dataflow.
We'll then look at a few things to consider
when constructing Dataflow processing pipelines
We'll then cover some basic Cloud Dataflow concepts.
We'll move on to some more advanced concepts.
We'll look at security and access management.
And finally, we'll look at some of the more
practical aspects of Dataflow.
Okay, let's get going.
You can think of Cloud Dataflow
as being a really powerful ETL tool
for transforming and enriching data.
Cloud Dataflow pipelines are executed as jobs
with one or more workers carrying out the specific tasks.
You have access to huge computing resources
to manage big data pipelines.
Cloud Dataflow automates the provisioning
and management of processing resources.
It handles the auto scaling of worker resources,
and it ensures that your pipelines are processed
in the most efficient way.
Cloud Dataflow uses the open source Apache Beam SDK.
The Beam model allows you
to easily define data processing pipelines.
The pipelines you define
are processed efficiently using parallel task execution,
Cloud Dataflow supports pipeline development
using expressive SQL,
Java,
and Python APIs for the Apache Beam SDK.
Cloud Dataflow supports both real-time streaming
and batch jobs.
Beams unify, and develop, and model,
allows you to reuse code across streaming
and batch pipelines.
And there is Stackdriver integration for logging
and monitoring the processing of your pipelines.
There are many use cases for Cloud Dataflow.
These include fraud detection,
personalization of user experiences, and IOT analytics.
The data we want to process will normally start off
in a data source.
We want to place the process data in the data sync.
We define a pipeline that will take data from the source,
process it, and then place it in the Sink.
Apache Beam connectors allow you to read data
into your pipeline
and then write your output data into the Sink.
Some common data sources include Cloud Pub/Sub, BigQuery,
and Cloud Storage.
The source could also be a system external to GCP,
such as Kafka.
Common data syncs include Cloud Storage, BigQuery,
and Bigtable.
Cloud Machine Learning can be applied to sync data.
On GCP, our pipelines are managed
and executed by Cloud Dataflow.
We'll now take a look at the Driver program,
and the associated Runner.
The Driver is the program you write using the Apache SDK.
This can be written in Java or Python.
The Driver program defines your pipeline.
The pipeline refers to the full set of transformations
your data undergoes from initial ingestion
to final output.
The Driver program is submitted to a Runner full processing.
The Runner is software that manages
the execution of your pipeline.
The Runner acts as a translator
for the Backend execution framework.
The Backend system is most often
a massively parallel processing system,
GCP Cloud Dataflow, for example.
The Runner can also manage local execution
of Driver programs for testing and debugging.
This would be on your local machine.
Cloud Dataflow represents both the Pipeline Runner
and the Backend on GCP.
The Driver program defines the pipeline.
PCollections are used in the pipelines
and represent data as it is transformed within the pipeline.
A PCollection represents a potentially distributed,
multi-element data set.
PCollections can represent both batch and streaming data.
When the data comes from a fixed source,
like a file,
the data set is Bounded,
and it is treated as a Batch.
When data comes from a continuously updating source,
like a smart device emitting events,
the data is said to be Unbounded,
and it is processed as a Stream.
Pipelines typically create an initial PCollection
by reading data from an external data source;
however, the initial PCollection,
can also be created from in-memory data.
A transform represents a data processing operation
or a step within your pipeline.
Being transforms,
use PCollections as inputs and outputs.
Each transform takes one or more PCollections as inputs
and generates 0 or more output PCollections,
multiple transformed steps will define your pipeline.
Okay, Gurus.
Hopefully this video gave you a useful understanding
of the Cloud Dataflow, big picture.
See you in the next video.

```

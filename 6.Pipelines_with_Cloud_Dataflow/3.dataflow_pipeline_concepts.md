## Lesson Transcript
```text
Hi Cloud Gurus.
In this video,
we'll be focusing in more depth on some
of the Dataflow Concepts.
We'll cover the generic ParDo Processing Transform.
We'll then take a look at Aggregation Transforms.
We'll cover some of the characteristics of PCollections.
And finally, we'll briefly cover the Core Beam Transforms.
Let's go!
ParDo or ParallelDo
is the generic parallel processing transform.
Let's consider PCollection 1
with the number of elements.
We want to execute a transform
on each element of PCollection 1
to create PCollection 2.
Each element of PCollection 1
is transformed and placed into PCollection 2.
ParDo is a transform that allows you to do this.
For each input element,
a ParDo Transform produces either one output element,
0 output elements, or in some cases,
a ParDo Transform can produce multiple output elements
from a single input element.
Consider a single element of an input PCollection.
A ParDo Transform will act on that element,
and produce an element in the output PCollection.
A user-defined function provides user-defined code,
which specifies the operation to apply
to each element of the input PCollection.
A user-defined function defines
a distributed processing function.
A single pipeline may contain user-defined functions
written in a different language
from that of your Runner program.
A pipeline can also contain user-defined functions
written in multiple languages.
Okay, let's consider Aggregation Transforms.
Aggregation is the process of computing
a single value from multiple input elements
In Apache Beam,
this normally means aggregating across all elements
with a common key and within a common window.
We'll discuss windowing in a later video,
if we think of the colors denoting different keys,
then the blue key elements will be aggregated
into a single element,
and the orange key elements will be aggregated
into an orange element.
user-defined functions can be used in aggregation.
Let's consider some of the characteristics of PCollections.
Firstly,
the elements of a PCollection may be of any data type,
but they must all be of the same type.
The Apache Beam SDK includes built-in encoding
for commonly used data types.
There is also support for specifying custom encodings.
PCollections do not support random access
to individual elements within the PCollection.
Transforms are applied to every element
within the Pcollection, individually.
PCollections are immutable or unchanging.
Once a PCollection is created,
elements cannot be added to
or removed from their Pcollection.
Being transformed, do not change PCollections.
They use the elements of the input PCollection
to create the elements of the output PCollection.
So Beam Transforms do not consume
or change the elements of the input PCollection.
There is no limit to the number of elements
that a PCollection can contain,
however, a PCollection can be Bounded,
in which case it contains a finite number of elements,
or it could be Unbounded,
in which case, it does not have an upper limit
on the number of elements.
A Timestamp is associated with every element
of a PCollection.
Each element's Timestamp is initially assigned
by the source that results in the creation
of the PCollection.
We'll now briefly consider the Core Beam Transforms.
Firstly, there is ParDo,
which is the generic parallel processing transform.
We've already seen how this operates on each element
of the input PCollection.
Then there is the GroupByKey Transform
for process and collections of key value pairs.
The GroupByKey is used to collect all the values
associated with a unique key.
CoGroupByKey is used when combining multiple PCollections,
it performs a relational join
of or more key value PCollections,
where they have the same key type.
Combine is unsurprisingly the Beam Transform
for combining elements.
A Combine Transform requires you to provide a function
that defines the logic for combining elements.
The combining function needs to be associative
and commutative.
There are a number of pre-built combined functions provided
by the Beam SDK,
these include combined functions such as sum, min, and max.
Flatten merges multiple input PCollections
into a single logical PCollection.
The Partitioning function provides the logic
that determines how the elements
of the PCollection are split up.
Okay, Gurus.
That's it for the video on Dataflow Pipeline Concepts.
In this video, we looked at ParDo;
we looked at the Aggregation Transform;
We looked at the characteristics of PCollections,
and we also covered the Core Beam Transforms.
In the next video,
we'll be looking at some more advanced concepts.
See you in the next video.

```

## Resources 

- [DAG (Directed Acyclic Graph)](https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/dags.html) 
- [Writing DAGs (workflows)](https://cloud.google.com/composer/docs/how-to/using/writing-dags)

## Lesson Transcript
```text
Hey Cloud Gurus.
In this video, we'll be covering
the basic pipeline development lifecycle.
We'll look at a few things to consider
when designing your pipelines.
We'll then cover some common pipeline structural elements.
And finally, we'll cover the basic flow
for creating a pipeline in your driver program.
Let's get going.
The lifecycle starts with designing your pipeline.
This includes designing your pipeline structure,
thinking about the transformations that will be applied
within the pipeline, and determining your input
and output methods.
Once you have designed your pipeline,
you would then normally go ahead and create it.
A Beam program generally starts by instantiating
a pipeline object.
Creating the pipeline involves implementing
the transformations that were identified
during the design phase.
And finally, there is testing.
Debugging a failed pipeline execution on a remote system
can be very challenging.
It is suggested that you perform local unit testing
of your pipeline on your local machine
where it's much easier to pick up potential errors.
So running the pipeline on your local machine is a great way
to identify and fix bugs before it is remotely executed.
Let's take a look at a few things you should consider
when designing your pipelines.
Start with the location of your data.
Where will your data be sourced?
Where is the input data for your pipeline stored?
How many different sources do you have?
You should then consider the input data structure
and format.
What does your source data look like?
This will give you some idea of your initial transforms.
You should then think about your transformation objectives.
What are you trying to do with your data?
This will determine the set of transformations
that needs to be applied within your pipeline.
And finally, you should think about
the output data structure and location.
Where does your output data need to end up,
and how should it be structured?
Okay, let's look at some basic pipelines
and pipeline structures.
The most basic pipeline is a linear structure
from input through to output.
Pipelines can also branch.
A branch can be formed where 2 different transforms
are applied to a single PCollection
resulting in 2 different output PCollections.
For example, T1 here operates on positive numbers
and T2 operates on negative numbers.
Branching can also take place at a transform.
Here we have a single transform
that outputs multiple PCollections.
This gives a main output and an additional output.
As an example, the transform operates on positive numbers
and generates the main output PC1.
The transform also generates PC2,
which contains the negative numbers.
Pipeline branches can also be merged.
Generally, you need to merge all the branches
of your pipeline at some point.
This is achieved with a Flatten or Join transform.
We'll cover the different types of transforms
in a later video.
A pipeline can have multiple sources.
The data from these sources can be independently transformed
before being merged with other branches
using a Merging transform.
So we often end up with a nonlinear pipeline
consisting of a number of branches and mergers.
A data flow pipeline represents
a directed acyclic graph, or DAG.
A DAG is a graph with a finite number of vertices and edges.
In a DAG, there are no directed cycles.
That is, it is not possible for the output of a transform
to become the input for the same transform.
You don't need to know this in detail, but for interest,
I'll provide a link to more information on DAGs
in the resources.
Regardless of what language you are using,
the process of creating a pipeline in your driver program
is basically the same.
First, create a pipeline object.
Then create one or more PCollections
using a read or create transform.
These are the input PCollections.
Then apply the required set of transforms.
PCollections are implicitly created
to manage the transfer of data
between your specified transforms.
Finish up by writing the final PCollection
to the pipeline sync.
And finally, execute the pipeline using a pipeline runner.
And voila, you have something beautiful.
Okay Gurus, that's it for the video
on the data pipeline lifecycle.
In this video, we looked at the different ways
of structuring a pipeline using branching,
merging and by using multiple inputs.
We also looked at the concept of a DAG.
Finally, we looked at the basic flow
for creating and executing a pipeline
using a driver program.
I hope you found that interesting.
See you in the next video.

```

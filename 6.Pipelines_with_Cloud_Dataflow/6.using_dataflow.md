## Resources 

- [Deploying a pipeline](https://cloud.google.com/dataflow/docs/guides/deploying-a-pipeline) 
- [Using Flexible Resource Scheduling in Cloud Dataflow](https://cloud.google.com/dataflow/docs/guides/flexrs) 
- [Using Cloud Pub/Sub Seek with Cloud Dataflow](https://cloud.google.com/dataflow/docs/concepts/streaming-with-cloud-pubsub) 
- [Use Dataflow SQL](https://cloud.google.com/dataflow/docs/guides/sql/dataflow-sql-intro) 

## Lesson Transcript
```text
Well, hello again Cloud Gurus.
In this video, I want to cover some of the practical aspects
of using Cloud Dataflow.
We'll start by discussing the use of regional endpoints
and we'll take a high-level look
at how Cloud Dataflow can be used for machine learning.
We'll very briefly look at some other topics
relating to Cloud Dataflow,
and we'll finish off
by looking at Cloud Dataflow SQL, let's go.
You are able to select a regional endpoint
for your Cloud Dataflow jobs.
The regional endpoint provides a number of functions.
A regional endpoint manages metadata
about your Cloud Dataflow jobs.
The regional endpoint also controls
your Cloud Dataflow workers.
It will also automatically select the best zone
within the selected region
to run your Cloud Dataflow workers.
If you do not specify a regional endpoint, Cloud Dataflow
we'll use us-central-1 as the default region.
There are a few good reasons to specify a regional endpoint
as part of submitting a Cloud Dataflow job.
Often there will be regulatory or compliance constraints
that will require a company starter
to remain within a specific geographic region.
Specifying a geographic endpoint
means that Cloud Dataflow workers
will only be created in the region specified
and so pipeline processing
will obviously remain within the specified region.
Network latency and network transport costs
can both be minimized by ensuring that worker instances
execute pipeline tasks
in the same region as the data sources,
syncs, and staging locations.
The ability to distribute workloads
across different geographic regions
adds to overall resiliency
and it can be a very good idea if you accept the trade-offs.
Pipeline execution can also be placed
in a specific geographic region
to isolated from your other workloads.
This leads to overall higher resiliency.
The GCP documentation has a good tutorial
on doing machine learning with Apache Beam and TensorFlow.
The tutorial also uses Cloud Dataflow, obviously,
and cloud machine learning engine.
I have linked to the tutorial in the resources.
For this video, I'm not going to go into the details
of the exercise,
but I'll cover the high-level structure,
so you get an idea of how machine learning could be done
using Cloud Dataflow.
The tutorial example comprises 4 steps.
Step 1 handles data extraction from cloud storage.
The specifics are not important
suffice it to say that the data source is read
and decompressed in that step.
Step 2 is for data pre-processing.
This is achieved using an Apache Beam pipeline
deployed on Cloud Dataflow.
Step 2 carries out a number of transforms on the data.
The TensorFlow API is used to normalize some values
between 0 and 1.
The Beam partition transform is used to split the data set
into the training data set and the evaluation data set.
In step 3, TensorFlow is used to train a model.
The model can be trained locally on your machine
or using cloud machine learning engine.
This step does not use Cloud Dataflow.
Step 4 is used to make predictions.
Here, an Apache Beam pipeline
uses the model trained in step 3.
The pipeline can be used to make predictions on batch
or streaming data.
In the provided example,
data is read from a cloud Pub/Sub topic,
and the predictions are made.
The predictions are then written out
to another Pub/Sub topic.
I hope this gives you some idea
of how Cloud Dataflow
can be used as part of a machine learning solution.
Let's now cover a few additional things you may come across
when using Cloud Dataflow,
you are able to use customer-managed encryption keys
with Cloud Dataflow.
This allows for the encryption of your pipeline data
at rest with the key that you provide and manage.
Batch pipelines can be processed in a cost-effective manner
using flexible resource sharing or FlexRS.
Flexible resource sharing
reduces overall costs by using advanced scheduling
by using the Cloud Dataflow Shuffle service.
This improves the overall efficiency
of processing the pipeline.
Flexible RS also uses preemptible VMs
in conjunction with your normal VMs to reduce costs.
You can get more details on FlexRS,
check out the resources section for relevant links.
Cloud Dataflow is excellent for MapReduce.
On-prem MapReduce jobs can be rebuilt on Cloud Dataflow.
Unfortunately, they cannot be migrated to Cloud Dataflow
as easily as they can be migrated to cloud.proc.
You can and probably should
migrate any App Engine MapReduce workloads
to Cloud Dataflow.
Cloud Pub/Sub Seek allows you to replay and reprocess
previously acknowledged messages.
It also allows you to acknowledge messages in bulk.
You are able to use Pub/Sub Seek
in conjunction with Cloud Dataflow pipelines.
It is recommended that you do not allow direct access
from a running pipeline to Cloud Pub/Sub Seek.
Again, check out the resources
for more about using Cloud Pub/Sub Seek
with Cloud Dataflow.
Let's finish off by discussing Cloud Dataflow SQL.
So you are able to develop and run Cloud Dataflow jobs
directly from the BigQuery web UI
using Cloud Dataflow SQL.
Cloud Dataflow SQL is a variant of ZetaSQL
and it integrates with Apache Beam SQL.
Apache Beam SQL can be used in Apache Beam pipelines.
It can be used to query bounded and unbounded PCollections.
Each Apache Beam SQL query is converted to an SQL transform
within the pipeline.
Cloud Dataflow SQL is the GCP version of Apache Beam SQL.
Some of the benefits of using Cloud Dataflow SQL include
the ability to utilize existing SQL skills,
the ability to join streams with BigQuery tables,
the ability to query streams or static data sets,
and finally, the ability to write your output
to BigQuery tables or views
for further analysis or visualization.
Okay Cloud Gurus, that's it for the video
on using Cloud Dataflow.
I hope you've enjoyed this chapter on Cloud Dataflow.

```

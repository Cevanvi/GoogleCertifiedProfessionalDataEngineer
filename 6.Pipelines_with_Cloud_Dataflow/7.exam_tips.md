## Lesson Transcript
```text
Hello, Cloud Gurus.
Let's go over some tips
to help you with any exam questions about Cloud Dataflow.
My first tip is Go with the Flow.
Dataflow is an ideal managed solution
for customers using Apache Beam.
So look for references to the Beam SDK in exam questions.
Quite often,
Dataproc and Spark may be suitable for batch workloads,
but Beam and Dataflow
are the preferred solution for streaming data.
There's some key Dataflow terminology you need to remember.
The Pipeline represents the complete set of stages
required to read data, perform any transformations,
and write data.
You define this Pipeline in Python or Java code
using the Apache Beam SDK.
A PCollection represents a multi-element dataset
that is processed by the Pipeline.
While you don't need to be a coding wizard to pass the exam,
you need to understand some of the functions
of the Beam SDK.
ParDo is the core parallel processing function
of Apache Beam,
which can transform elements of an input PCollection
into an output PCollection.
DoFN is a template you use to create user-defined functions
that are referenced by a ParDo.
Sources and Sinks.
This one's nice and simple.
Sources where data is read from,
Sinks are where data gets written.
Multiple I/O transforms are supported for Sources and Sinks,
including Google Cloud Storage,
Cloud Bigtable, Cloud Datastore, Pub/Sub, and BigQuery.
Windowing allows streaming data
to be grouped into finite collections
according to time or session-based windows.
This is very useful when you need to impose ordering
or constraints on Pub/Sub data
as Pub/Sub messages themselves
are not guaranteed to be delivered in order.
A watermark indicates
when Dataflow expects all data in a window to have arrived.
Data that arrives with a timestamp that's inside the window
but past the watermark is considered late data.
And finally,
in the exam you may have to choose the right solution
for orchestrating a pipeline,
and Dataflow is normally the preferred option
for data ingestion pipelines.
Cloud Composer may sometimes be used
for ad-hoc orchestration
or to provide manual control
of Dataflow pipelines themselves.
Okay Cloud Gurus, that about wraps it up for Cloud Dataflow.
Please feel free to reach out to me in the course forums
if you have any questions.
And if not, I'll see you next time.
Keep being awesome, Cloud Gurus.
```

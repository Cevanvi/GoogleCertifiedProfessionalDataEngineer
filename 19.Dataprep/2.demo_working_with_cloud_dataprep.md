## Resources 

- [Mock data files](resources) 

## Lesson Transcript
```text
Hello, Cloud Gurus.
In this lab, we'll get hands-on with Cloud Dataprep.
The objectives of this lab
are to explore the Dataprep user interface,
then import 2 datasets from Google Cloud Storage.
We'll create a recipe that transforms one of our datasets,
cleaning up a bunch of malformed JSON
into something that we can then use
to join with our other dataset,
making it into something useful for our data analysts.
All you need to follow along with this lab is a GCP project
and access to the Cloud Console.
We'll use some mock data that I've already generated,
which you can download from the resources section.
So grab those files now and when you're ready,
join me in the GCP Console.
Okay, Cloud Gurus, so here we are in the GCP Console.
We're going to be using 2 mock data files
that I've created,
which you can download from the resources section.
To use them in this lab,
you need to place them in a GCS bucket.
I'm going to use a bucket I've already created
that's named after my project.
Feel free to use any bucket in your project,
or just create a new one.
I'm sure by this stage in the course,
you're comfortable managing your own buckets and uploads.
These mock data files contain fake customers and comments
for a fictional car rentals company.
Once you've uploaded both of these files,
let's find Dataprep
in the big data section of the GCP menu.
As I said, Dataprep is an integrated partner service
that lives outside the normal GCP Console.
So the first time you use it,
there will be a few hoops to jump through.
First, you'll need to agree
to share your account information with Trifacta
and you'll need to allow Trifacta
to access your project data,
although you can disable this later
in your project settings.
Next, we need to choose
a default staging bucket for Dataprep.
We can let Dataprep create one for us
and just click Continue.
Finally, you'll arrive at the Dataprep dashboard.
Let's have a quick look around.
On the left-hand menu here, we have a few options.
This is our project browser,
so we can easily switch between projects
without leaving the Dataprep UI.
This icon will take us to our flows,
but we haven't created any yet.
Next, this is our library where we can browse datasets
that we've imported into Dataprep.
And finally, this icon would show us our jobs
where we can see scheduled and automated flows,
but we're getting ahead of ourselves.
Let's go back to the library section.
We can import data in 3 different ways,
either directly through an upload, from a GCS bucket,
or from a table in a BigQuery dataset.
Click Import Data,
and let's select GCS on the left-hand menu.
Now we can browse to our bucket
and find the 2 files that we uploaded earlier.
Click the + symbol next to the comments.json file.
On the right-hand side, you'll see a preview of the dataset.
Click the + next to customers.csv as well.
And we'll see another preview.
Now click Import Datasets.
So now we've got our data into Dataprep.
Let's create our first flow.
Select flows from the left-hand menu
and click Create Flow.
We'll call this flow Car Rentals.
To start with, we just have an empty flow.
We need to add some datasets to this flow
so that we can start wrangling data.
So click Add Datasets,
and here are the 2 that we imported into our library.
Let's tick both of them
and click Add to add them to this flow.
Our customers.csv file is selected first,
and we can see a preview of the data on the right,
but let's work with our customer comments first.
Click on the comments.json and we can see a preview
where Dataprep has roughly sorted
our JSON key value pairs into columns,
but there's still quite a bit of tidying up to do.
We'll do this by adding a recipe.
Click Add New Recipe
and this will add a blank recipe to this dataset
as part of our flow,
but it won't do anything until we add some steps.
So click here again to edit the recipe.
This will open up a new interface
where we can browse the full dataset
and create steps for our recipe.
We'll start with a really simple step.
If we scroll to the right, we can see our comments column.
These are the actual comments left by our customers,
or at least they would be
if this wasn't fictional Latin data.
You can spot just by browsing this column
that often the value of comment is mull.
That means the customer didn't leave a comment.
So these rows are pretty useless to us.
Let's add a step to our recipe to delete them.
Dataprep will always try to help us
by suggesting transformations.
So let's help Dataprep
by pointing out what we're looking to do.
We'll highlight the complete text from one of the rows
where comment is null.
A suggestion box pops up with things we can do
based on what we've highlighted.
We could count matching rows, split them,
maybe extract these values and create a new column.
What we want to do is the suggestion at the bottom,
which is to delete any rows
where comment null exists in the column.
If you hover over this suggestion,
you get a tiny preview of what's going to happen
if we add it to the recipe.
Then if we click it,
all of the affected rows in our current view
will be highlighted in red.
Let's click Add to add this to our recipe.
Our dataset view updates to take into account
the step we've just added
and the recipe view has popped up showing us the step itself
written in Wrangle, the language of Dataprep.
We can close the recipe view for now by clicking the X.
If we ever want it back, we just click this icon here
that sort of looks like a set of instructions.
So onto our next transformation.
Let's make a cleaned up column
to represent our customer IDs.
I'll just expand this column a bit.
All we need to do is highlight an example of a customer ID.
Dataprep's top suggestion is to extract values
that match 7 digits in a row,
which we know in this case
is the format of our customer IDs.
Let's add that to our recipe by clicking Add.
Now we can manually rename our new column to customer ID
by right-clicking on the column.
We can delete the original column as we no longer need it.
Right-click on the top of the column and click Delete.
Let's delete a few more.
All we actually need from this dataset is the customer ID,
the comment and the satisfaction rating we were given.
First, we'll select the name column,
then scroll to the right and holding down the Shift key,
select the timestamp column,
then just right-click and go ahead and select Delete.
Next, we'll transform these last 2 columns
just as we did with the customer ID.
First, the comments.
This time, we'll add a step to the recipe manually.
Open the recipe,
which you can now see contains 5 different steps so far.
Click New Step.
Search the transformations for extract
and select Extract Text Or Pattern.
Select column 7, which is where the comments are.
We can leave the text to extract box blank,
but we'll specify a pattern for the start extracting after
and end extracting before sections.
So we want to start after comment in quotes colon
and the first double quote,
but the whole string needs to be in single quotes.
Make sure you enter this exactly as I have here.
And we want to end before the last double quote.
Dataprep will try to add additional quotes
and single quotes as you type this.
So again, make sure you're entering it exactly as I do.
We can see a preview of what our step will do
and that looks good.
So let's add this step to our recipe by clicking Add.
Now we can rename our new column to comments
and delete the source column.
Finally, let's extract the satisfaction ratings.
Just go to the first row
and highlight a single digit number,
but choose the second suggestion here
which will match any sequence of digits, not just one.
Then click Add,
then we'll rename this column Satisfaction
and delete the source column again.
Now we've got our complete prepared dataset
of customer comments and satisfaction ratings
matching the ID.
Let's take a look at the complete recipe again.
There are 11 different steps here.
The neat thing is that we can reuse this recipe
on any future data
that matches the same format as this dataset.
Okay, let's leave the comments for now.
Right up in the top left,
click where it says Car Rentals in small writing
to take us back to our flow view.
We can see in this view our comments recipe
attached to our comments dataset.
Now let's take a look at our customers' data.
Click Add New Recipe,
and then again to edit the recipe.
So this data actually looks quite nicely formatted.
It's just a list of customers that gets appended to
whenever that customer rents a car.
There's not much for us to do here,
but wouldn't it be neat if we could somehow merge
that comment data into this table
so we could see how satisfied each customer was
with their rental.
This is actually surprisingly easy to do.
First, click on the Join icon here.
We'll select comments from the recipes in our current flow,
and then click Accept.
Now we just need to define how we'll merge the 2 datasets.
Let's edit the join keys
and change both the current and joined-in keys
to customer ID.
Then click Save and Continue.
Click Next
and now we just need to make sure
that we're adding the extra columns to our complete dataset.
Click Review
and then click Add To Recipe.
And voila, we now have a complete dataset
that contains details of each rental, the customer,
their comment, and satisfaction rating.
Don't worry that we now seem to have more rows
than we do original customers or comments.
That's unfortunately a side effect
of the way this pseudo random data was produced.
So this may not make much sense,
but the important thing here
is that you've seen how easy it is
to visually manipulate the data.
Click on Car Rentals one more time to go back
to the flow view
and you can see how the recipes on both datasets
have merged to produce our joined dataset.
We could now optionally schedule this flow
to be executed in GCP by Cloud Dataflow.
Okay, Cloud Gurus.
There's nothing to clean up for this lab
except the GCS bucket if you created a new one.
Feel free to explore around the other transformations
available in the recipes you've created,
or if that's quite enough Dataprep for you, don't worry,
it's only covered very lightly in the exam.
Thanks for watching Cloud Gurus and I'll see you next time.

```

## Lesson Transcript
```text
Hello, Cloud Gurus.
Welcome to this lecture
where we'll look at the origins of Hadoop,
Apache's open-source implementation of the MapReduce model
and cluster framework,
and HDFS, the Hadoop File System.
So I'm sure the question you're most interested
in having answered is: where did the name Hadoop come from?
The answer is a toy elephant,
specifically a toy elephant
belonging to the son of Doug Cutting,
who along with Mike Cafarella
was inspired by Google's published papers
on the Google File System and MapReduce.
Doug was working at Yahoo at the time
and he and Mike started the Apache Nutch project
as an attempt to create an open-source project
capable of indexing the entire worldwide web.
The code for the MapReduce model and its cluster management
were then moved out of Nutch
and the Hadoop subproject began in 2006.
Since then, Hadoop has been widely adopted
by many household names in the tech industry
and continues to be developed and maintained
by an active open-source community.
Hadoop now comprises 4 core modules.
These are Hadoop Common.
This is the base framework of Hadoop
containing all of the libraries,
operating system abstractions,
and startup scripts required to run the rest of Hadoop.
Hadoop Distributed File System or HDFS.
HDFS was designed to be a distributed
fault-tolerant file system that runs on commodity hardware
as part of a Hadoop cluster.
Google already had their own Global File System
to build MapReduce on top of,
but Apache had to start from scratch and build their own.
Hadoop YARN, which handles the tasks of resource management,
job scheduling, and monitoring for Hadoop jobs.
And of course, Hadoop MapReduce,
Hadoop's own implementation of the MapReduce model.
Hadoop MapReduce includes libraries
for map and reduce functions,
as well as partitioning, reduction,
and custom job configuration parameters.
Let's take a look at a high-level overview
of a Hadoop cluster.
We'll start with the components of HDFS.
In this example, let's say we have 2 physical servers.
HDFS uses a typical master-worker cluster architecture.
On our first server, which we can consider our master,
we run what's called the name node.
The name node manages access to files by clients
and stores all of the necessary metadata
for the file system.
In addition to the name node or master,
we have multiple data nodes or workers.
HDFS is designed to reliably store
very large files across a cluster.
So on each data node,
files are stored as a series of blocks.
HDFS also has the concept of racks
designed to help utilize the shortest network path possible
between nodes in a physical data center
in proximity to other racks.
Using multiple data nodes means that file blocks
can be replicated for fault tolerance.
A client can make requests to the name node
to read or write a file,
but the actual request can be fulfilled
by different data nodes.
The cluster design of YARN is quite similar.
It's YARN's function to manage MapReduce jobs
that are given to it across a cluster of machines.
It does this with a master resource manager.
The client sends jobs to the resource manager
and on individual workers,
a node manager process runs to handle local resources,
request tasks from the master, and return the results.
Now, as I said, these are very high-level overviews.
You will not be required to know HDFS and YARN
in depth in the exam,
but a basic understanding of these technologies
will help you when we start looking at Cloud Dataproc
a little later in the course.
To help put all of this together,
let's have a quick look at running a MapReduce job
on a single node Hadoop cluster.
Don't worry, this isn't a follow-along lab.
This is just to help you see some of the topics
we've been discussing in action.
Here, I have a compute engine VM
on which I've installed Hadoop as a single node cluster.
We can see that the HDFS services are running,
our name node and data node,
as well as the YARN services,
resource manager and node manager
for handling the execution of jobs
that we send to the Hadoop service.
We can query our local Hadoop File System
with the hadoop fs command.
I've uploaded an input file.
So let's use hadoop fs -ls
and the name of the file to see it.
The input file here is the text of Bram Stoker's Dracula
from Project Gutenberg.
I'm going to run a word count MapReduce program
on this input text.
Now, I could write this in Java
and use the built-in map and reduce functions
provided by the Hadoop library.
But to give you a more low level view of what's going on,
I've actually written very basic versions
of these functions in Python.
First, let's have a look at our map function.
As you can see,
this script simply takes a line read on stdin
and then iterates through every word in that line.
We do some basic checking
to make sure the word is a normal word
by removing any special characters.
Then we output a key value pair of that word
and the number one.
Now let's take a look at our reduce function.
Our reduce function simply adds the counts for the same word
and prints out a new key value pair.
As each set of mapped intermediate key value pairs
is sent through this function,
it will eventually map them down
into our final set of counted words.
So if I've written these 2 functions,
you may be wondering why I'm still using Hadoop.
Well, don't forget Hadoop isn't just a programming library.
I'm going to run this job using my Python functions,
but using Hadoop to schedule its execution
across my cluster.
This would be particularly useful if I had a much larger
or more complex job for it to run.
I'm also going to feed it an input file from HDFS
and ask it to write its output to HDFS as well.
So let's run this job.
First, I'm going to change directory
and I'll just clear my screen.
Then I'm going to run this rather long command.
I'm invoking the Hadoop streaming library here
so I can stream in my text.
Then I'm telling Hadoop which files it needs to use
to run my job, the mapper and reducer scripts,
and then I need to tell it which one is the mapper
and which one is the reducer.
Finally, I tell it the location of my input file
and where I would like the output written.
Both of these locations exist in HDFS,
not on the local file system.
So let's submit this job.
The MapReduce job is submitted via YARN.
First, the map function will run.
This will complete after a few seconds
and then the reduce functional run.
Then our job will complete and show us lots of metrics.
We can scroll back up quickly
to view the actual job completion notice.
I'll just clear the screen.
We can see our output files have been written into HDFS
by using hadoop fs minus ls on our output directory.
Just to test how successful our word count was,
let's see how many times the word blood appears in the book.
Wow, 102 times along with some blood curdling,
blood dripping and a few blood stains.
So this was a really simple task,
but hopefully that cemented some of the theory
we went over earlier in the lecture.
If you have any questions,
please let me know in the course forum.
And if not, I'll see you in the next video.

```

## Lesson Transcript
```text
Hello, Cloud Gurus.
In this video,
we're going to learn about
the next item in the Apache Hadoop tool bag.
And this one is called Apache Pig.
And unfortunately this time,
I can't tell you where that name came from.
So what is this oddly named thing in our big data tool bag?
And why does the Pig look slightly sinister?
I can't answer that one I'm afraid,
but I can tell you that Apache Pig is a platform
for analyzing large data sets.
It's basically a language called Pig Latin,
which can be used to define complex analytics jobs
that may include the merging, filtering
and transformation of data.
The key point is that it's a high level language
comparable to SQL in its simplicity
that can be used to define these jobs
without having to write complex code.
So you can think of Pig as an abstraction for MapReduce.
It's particularly useful for writing ETL
or extract, transform and load jobs,
because it's also a procedural language,
making it easy to specify the flow of data
through various transformations and functions.
Let's look at an example,
as I said,
rather than writing actual code,
that needs to be compiled and run for data analytics tasks.
We can use Pig Latin,
a much higher level language than something
like Java or Python.
So in this example,
let's perform the word count task on the text
of George Orwell's Animal Farm.
Something about that sinister Pig made me pick this.
We can create objects like words
by reading all of the lines in the text file
with simple iterators like foreach
we can then use filters, in this case,
we'll remove any word that just matches all white space.
Then we can group words together
and count them again using very simple high level terms
like group and count.
Then using some more SQL like terms
we can create a list in descending order
and store it into our partition files.
To execute this job,
Pig will compile our instructions into MapReduce jobs,
which are then sent a Hadoop
for parallel processing across the cluster.
Pig Latin has various built in functions
to enable you to write reasonably complex analytics tasks
with relatively simple SQL-like statements.
These include functions for
evaluation, including average, concatenation,
counting max, min and sum.
More advanced math,
including absolute, sine, cosine, tan, square root,
rounding numbers, and many more.
There are also functions for handling strings,
including splitting substrings,
performing regular expressions
and casting to upper and lowercase.
And there are also further functions
for datetime objects, tuples, and maps.
If those built-in functions aren't enough,
you can write your own user defined functions
in any supported language,
which includes Java, Jython, JavaScript,
Ruby, Groovy, and Python.
Or the other way around,
you can also embed Pig
into the normal code of several of these languages.
Okay, Cloud Gurus, to sum up,
Pig is a high-level framework for running
MapReduce jobs on Hadoop.
You're not going to be tested on using Pig during the exam,
but it's good to know where all these tools fit in,
in the open source ecosystem
and how that translates to using them in GCP.
Join me in the next video
where we'll move on to some more Apache goodness.
```

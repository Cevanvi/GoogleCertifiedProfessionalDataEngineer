## Lesson Transcript
```text
Hello, Cloud Gurus.
In this video, we're going to learn
about the next iteration of big data tools
that came out of the Apache software foundation,
Apache Spark.
So the MapReduce model was a notable advancement
in computer science and Hadoop enabled people
to take advantage of this new programming paradigm
to perform some impressive data analytics.
These were the early days of big data,
but as that data got even bigger,
some limitations of MapReduce became apparent.
Primarily the issue was the linear flow of data.
Every MapReduce job performed the same functions in order,
read some data, map functions across that data,
reduce the results and write them to disk.
Even though you could achieve some impressive results
by combining multiple MapReduce jobs,
don't forget this was the foundation
of Google search and page rank,
this linear constraint made it harder to run complex
calculations without waiting a long time,
even on large clusters of machines.
In response to these limitations,
Matei Zaharia, while studying at UC Berkeley,
created a new project called Spark,
which was then open sourced and later donated
to the Apache foundation in 2013.
Spark is designed to be a general purpose
cluster computing framework,
allowing for concurrent computational jobs to be run across
massive data sets.
Spark uses the concept of resilient, distributed data
multi sets, an extension of distributed storage,
where data is replicated across multiple machines
in a cluster, but can be accessed as working sets of data
by multiple jobs, without the iterative reading
and writing to disk of the MapReduce model.
In this way, you can think of Spark storage model
as a kind of distributed shared memory.
Spark supports 4 standard modules,
although there are also third-party projects
that extend Spark's functionalities.
The four standard modules are Spark SQL.
This module is designed for working with structured data,
using a familiar SQL syntax.
Structured data in Spark is stored in an abstraction
called a data frame.
For programmatic querying, developers can access
the data frames API rather than embed SQL
into their code.
Spark Streaming.
This module allows you to configure
streaming data ingestion in addition to batch processing.
In reality, Spark streaming uses very small batches
to achieve this.
The nice thing about this module is that you can use
the same code for batch processing,
join streams against historical data
or run ad hoc queries on the current state of a stream.
MLLib.
This is Sparks machine learning library,
which supports all the machine learning algorithms
you'd expect, classification, regression, decision trees,
recommendation, and more.
This is one area where Spark's iterative
cluster computing stands out.
In benchmarks, a logistic regression was 100 times faster
on Spark than when using MapReduce on Hadoop.
And finally GraphX.
A module specifically designed
for iterative graph computation to save you
the hassle of stitching together
numerous MapReduce jobs.
Spark supports the most common programming languages
for big data with libraries for Java, Scala,
Python, and R, as well as direct SQL support.
To run Apache Spark, you need two things,
a cluster manager and a distributed storage system.
You have quite a bit of choice here.
Spark will run as a standalone cluster using yarn on Hadoop,
or even in Kubernetes.
For distributed storage, Spark supports HDFS,
as well as some structured storage systems
like HBase and Apache Cassandra.
To get a feel for how Spark is different
to plain old MapReduce on Hadoop,
let's take a quick look at a PI Spark example.
Once again, this isn't a follow along lab,
just watch what I do and it should hopefully help
some of the theory we've been learning so far.
Here I'm logged into a compute engine VM
where I've installed Spark as a single node cluster.
Even running on this single VM,
I can configure Spark to run a single master
and four worker processes.
We can see this from the handy web UI
that comes built in with Spark.
Let's run a Spark job that gives us a bit
of a glimpse of how powerful it is compared to a standard
iterative MapReduce job in Hadoop.
Spark comes with lots of example job scripts,
including a basic implementation
of Google's page rank algorithm.
Let's take a look at it in the Spark examples directory,
I'll scroll down a bit to find the main function.
So here we can see the script starts a session with Spark.
The calculations that formed the PageRank algorithm
are then defined on the following lines.
It's not too important to understand what's going on here,
just that we're describing a set of steps to perform
on the data in memory.
When this job is submitted via Spark,
the work will be distributed to all of the workers
for parallel computation.
So let's see it in action.
I'm going to use a dataset that was released by Google
as part of a coding competition, way back in 2002.
Let's have a look at the data.
Each number in the first column represents the ID
of a webpage that has been linked to.
And the number in the second column represents the ID
of the webpage where that link was found.
The original dataset had over 5 million lines,
but I've reduced it to 1 million for the purposes
of this demonstration.
It's still a sizable amount of data to map and graph.
I'll just clear my screen.
And now I can submit my job to the Spark master
with the Spark submit command.
I specify the location of the master,
the job script,
the name of my dataset file
and the number 10,
which represents the number of iterations
the algorithm should run on each webpage ID.
Each iteration will expand the number of neighbors
in the map that its PageRank value as compared with.
Now we've submitted the job,
we'll get a lot of information on the console
about its progress.
We can also refresh the master webpage
to see our job as the current running application.
While applications on Spark are running,
they also present their own webpage,
which gives you a lot more detail
and even some visualizations of the work in progress.
Here we can see all the stages of our job.
Those that are active, as well as pending.
We can see that this job comprises several functions,
such as collect, reduce by key, and join.
If we look at the active stage,
we can see a graph visualization of the steps in this stage.
Let's look at the executor's page.
Here, we can see we're using four RDD blocks.
This essentially means we're keeping
all of our data in memory,
while we continually go over all of the stages on the data.
This is a huge leap forward to the original MapReduce
implementation of constantly writing intermediate data
to disk and reading it back again.
Let's have another look at the stages in our job.
If we scroll down,
we can see that some of the stages have completed now.
Let's go back to our console and wait
for our job to complete.
A few minutes later, the job completes successfully,
and the results are written to the terminal.
Of course, if we were going to use this data
for anything properly, we'd write it to a file
or feed it straight into another kind
of structured database.
Here are all the calculated page rank values
for all of those webpages,
although unfortunately, we've got no way of telling
what all these IDs actually refer to.
There are some here with quite high numbers,
so it would be interesting to know what they were.
So to summarize, the main difference between Hadoop
and Spark is the way that data is processed
during computations.
In Hadoop, data is stored in blocks on disk
before, during, and after a computation,
which means the complex tasks involving multiple MapReduce
jobs can be slowed down by lots and lots of disk IO.
Spark, on the other hand,
stores computations in memory where possible,
which as well as being much faster,
also permits multiple operations to be carried out
on that data in parallel.
In terms of latency, this makes Hadoop
a high latency system and Spark a low-latency system.
There are a few cases where to save costs
and if time is not an issue,
you could use Hadoop for batch processing
because disk is cheaper than RAM.
But if you need streaming support, use Spark.
In general, if you can afford it, use Spark.
It's been shown to be 100 times faster
at in-memory calculations and 10 times faster
at on-disk calculations than Hadoop.
Having this high level knowledge of these differences
will help you later in the course,
where we come to look at cloud Dataproc,
which is essentially a managed Spark service.
Well that's it for this video, Cloud Gurus.
If you have any questions,
please let me know in the course forum.
I'll see you next time.

```

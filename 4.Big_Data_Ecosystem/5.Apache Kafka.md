## Lesson Transcript
```text
Hello, Cloud Gurus.
Let's round off this chapter with an exploration of Kafka,
Apache's distributed streaming platform.
So what is a distributed streaming platform?
Well, it's a distributed system that allows you
to publish or subscribe to streams of records.
You can think of it like a message bus,
but specifically designed for data and lots of it.
Kafka is designed for high throughput and low latency
for when you have multiple connected systems
and need a way to shuffle data around them.
This could be for ingesting millions of events from devices
or moving terabytes of data through various stages
of a data analytics pipeline.
Before being donated to the Apache project,
Kafka originated at LinkedIn,
where it's moving roughly 800 billion messages per day,
over 175 terabytes of data across 60 Kafka clusters.
Now just bear in mind that
each one of those individual messages
is just some tiny piece of information
being passed between different systems.
They're not all nagging recruiters.
There are four main APIs provided by the Kafka service.
These are producer,
which allows an application to publish
a stream of records to a Kafka topic;
consumer, which allows an application
to subscribe to one or more topics
and process the stream of records contained within;
streams, an API designed to allow an application
to act as a stream processor itself.
This is useful when you wish to transform some data
and feed it straight back into Kafka as an outgoing stream;
And the connector API,
which allows you to extend Kafka
by connecting producers or consumers to external systems,
such as relational databases.
In this simple diagram,
we have a Kafka cluster, producers, and consumers.
Producers produce messages,
which are sent to Kafka via the producer API
and consumers, guess what, consume those messages
via the consumer API.
Under the hood, it's a little more complex than that.
A Kafka cluster achieves replication
and fault tolerance by using partitions.
Partitions themselves are distributed across
multiple nodes in a cluster.
Each partition requires a leader node in the cluster
and can use other nodes to act as followers,
which replicate the partition data.
Topics span one or more partitions for fault tolerance.
The configuration and management of a Kafka cluster
is also managed by a separate system
called Apache Zookeeper.
Zookeeper is a standalone system
designed to coordinate highly reliable clusters,
And it also has this amusing cartoon as its logo.
So if you've ever encountered Google's cloud pub/sub,
this is probably all sounding a bit familiar,
and there's a good reason for that.
Pub sub provides much the same functionality,
but we'll get into that later
in our dedicated pub sub chapter.
In the meantime, there are some differences to be aware of.
Kafka guarantees the order of messages within a partition,
although not within a topic across multiple partitions,
however pub subs provides no guarantee of ordering.
Kafka, as a fully configurable system,
has tuneable parameters, like storage duration for messages,
which is really only limited
by the amount of disk you can throw at your cluster.
In pub sub, the maximum retention is always 7 days.
Kafka only supports polling as a delivery method
or pull subscriptions in the pub/sub world.
Pub/sub also supports push subscriptions.
And overall, the biggest difference
is managed versus unmanaged.
Kafka is completely tuneable,
giving you access to all
those dials and leavers under the hood
to make it do exactly what you want it to do.
With the managed pub/sub service,
you don't have access to that level of configurability,
but the silver lining to that
is that you won't be able to break it either.
Let's have a quick demo of Apache Kafka in action.
I'm back on my compute engine VM,
where I have 3 Kafka processes running
to simulate a 3 node cluster.
This is going to allow me to have my topics replicated
across the 3 nodes for fault tolerance.
I've already created a topic for this demo called hot topic.
Now I'm going to quickly use Python
to generate some messages in an infinite loop.
First, I'll import some libraries.
Then I'll create a producer object using the Kafka library
and point it at the first node in my pretend cluster,
which is really just a process running on port 9 0 9 2.
I'll initialize a count variable,
then I'll start the infinite loop.
And every time it goes round,
I'll send a message with a count to the topic,
then sleep for 2 seconds and repeat.
When we start the loop,
we can see the message objects being created.
In a separate terminal window,
I can use a Kafka utility called the console consumer
to start consuming messages from this topic,
but I'll point the consumer at a different node
in my pretend cluster,
in this case, a different process on my VM.
I'm going to use the service on port 9 0 9 3.
This is looking good. The messages are rolling in.
So let's test the fault tolerance built into Kafka.
I'm going to kill the service that's running on this port.
I'll need to get the process ID of the service
using port 9 0 9 3 with the netstat command.
The process ID is 1 7 6 7.
So let's kill that process.
If we go back to our consumer window,
we should expect to see some errors
as we've killed the broker we were connected to,
but we're still getting messages through no problem.
Our Kafka client knows about all the brokers in the cluster,
so when the broker on port 9 0 9 3 stops responding,
it just switches to the next available broker.
If we were sending messages more frequently
than one every 2 seconds, maybe a thousand a second,
we may have lost a few while we reconnected,
but in our case, we didn't see any interruption.
Let's just confirm there's definitely no broker process
running on port 9 0 9 3,
where we originally connected our consumer.
And that's it for this demo Cloud Gurus.
I don't want to go too deep into the workings of Kafka
because it's going to benefit you more
to learn about cloud pub/sub later in the course.
In the exam, Kafka may come up as an element
of a customer stack who wishes to migrate to GCP.
So by the time you've learned pub/sub,
you'll see where it will be a good fit
for moving from Kafka,
but you still need to bear in mind
the differences and limitations.
Thanks for watching Cloud Gurus and I'll see you next time.

```

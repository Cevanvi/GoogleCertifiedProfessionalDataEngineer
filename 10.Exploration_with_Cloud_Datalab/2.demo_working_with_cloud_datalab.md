## Lesson Transcript
```text
Hello, Cloud Gurus.
Welcome to our Cloud Datalab lab.
There's probably an easy way to say that,
but I'm sticking with it.
In this lab, we'll create a Cloud Datalab instance
and play with some sample Jupyter notebooks.
We'll also see how Cloud Datalab can persist data for us
after we've deleted an instance,
which we can then reuse by creating a new instance.
All you need to follow along with this lab is a GCP project
as everything takes place inside the GCP Console
using Cloud Shell and the built-in Web Preview
to access our notebooks.
So let's get started.
So here we are in the GCP Console.
Before we do anything with Datalab,
we need to enable an API that Datalab will use.
Select APIs &amp; Services from the menu.
Then click ENABLE APIS AND SERVICES.
Search for Source,
and you should find the Cloud Source Repositories API,
then just click ENABLE.
We're now ready to use Datalab.
First, we'll need to open the Cloud Shell
to access the datalab command.
I'm going to full-screen the Cloud Shell and zoom in
so you can see what I'm typing a bit more easily.
Now, to create our Datalab instance,
all we need to do is type datalab create
and the name we want to give our instance,
which I'm going to call my-datalab.
Note that this isn't a subcommand of gcloud.
The datalab command is a standalone utility.
Hit Return, and let's see what happens.
First, we'll be asked to choose a zone
where our datalab instance should be created.
You might at some point want to choose a location
that's closest to your data sources,
but for this lab, just choose the zone closest to you.
I'm going to pick europe-west2-c, which is option 23.
Then the first thing that happens
is a new VPC network will be created
if it doesn't already exist
called datalab-network.
This helps to isolate your datalab instance
from other VPCs in your project.
It might feel like a bit of overkill,
but the next thing the script will do
is add some firewall rules,
and it's easier and safer to do this
on a brand new VPC network
rather than risk messing around with existing VPCs.
Next, the command creates some more resources for us:
the aforementioned firewall rule,
a persistent disk to attach to our instance,
a source code repository where we'll store our notebooks,
and finally the Compute Engine VM
that will actually run Datalab for us.
Inside this VM,
the Jupyter notebook system will be run as a container.
Finally, the script will attempt to create an SSH tunnel
to the instance
to make its web server available
using Cloud Shell's Web Preview.
At this point, the script may generate a new SSH key pair
if one doesn't already exist
and prompt you to optionally enter a passphrase.
Now, here's the interesting thing about the datalab command:
It's a bit buggy.
Your command might complete and open the SSH tunnel,
or you might experience what I have here
where it just appears to stall.
If this happens to you,
just hit Control + C to get back to the terminal
and then retry the connection with datalab connect
and the name of your instance.
After a few moments, the script finally opens the tunnel
and prompts us with what to do next,
which is to use the Web Preview feature of Cloud Shell.
Jupyter runs on Port 8081,
so we'll click the Web Preview icon but select Change Port,
then enter Port 8081, and click Change and Preview.
A new tab will open up, and we'll be connected
to our datalab instance running Jupyter notebooks.
This is the dashboard of our Jupyter system.
It's basically a file browser that shows us the contents
of the persistent disk that Datalab created for us.
The notebooks directory is a clone of the Git repo
that was also created for us.
Let's go into the notebooks directory
and create a new notebook by clicking Notebook.
First, we'll add some text by selecting Add Markdown.
This is the same markdown format you'll have seen used
to write a million README files in Git repos.
So let's just write a heading and some brief text.
When we finish writing this,
it doesn't render straight away.
Note that if we click out of the cell and back into it,
there's a green line next to it.
To run the cell, click Run at the top,
and the markdown becomes rendered.
Now, if we click back into that cell,
it has a blue line next to it
because it's already being executed.
Now, let's see how easy it is
to add some Python code to our notebook.
Click Add Code, then write the following.
Here I'm just importing some Python libraries
and then I'm applying a setting
that will suppress some irritating warning messages
that come out of Matplotlib.
Click Run again to execute this cell.
This time there's no output,
but we can see our cell has been executed again
because of the blue line next to it.
Now, let's do something a bit more interesting
by plotting a simple sin wave.
Enter the following code.
Now, before we run this, stop and think.
We've already run the first block of code,
so if we run the cell on its own,
will it still have access to the libraries it needs?
And, of course, the answer is yes.
The Python kernel for this notebook is still running.
We're still inside its session, so we can run this code
and see the sin wave plotted for us.
We can reset the current Python kernel
by clicking the Reset button.
Then we can clear what we've rendered so far
by clicking Clear All Cells under the Clear menu.
Now, starting from scratch,
if we try to plot our graph straight away, it will fail
because we've skipped importing the correct libraries.
Let's run the first code cell again,
then rerun the second, and it should work again.
This is a really simple example, but you can start to see
how different elements can be combined
in a really interesting way to present and visualize data.
Click the current name of the notebook, Untitled Notebook,
to change the name to something else.
I'm going to call mine Example 1.
We can now save it
by selecting Save and Checkpoint from the Notebook menu.
You can also see in this menu
that we have options for reverting to previous versions
or converting our notebook to different formats.
Now, saving the notebook
will store it on the persistent disk.
We can confirm this
by going back to our notebooks dashboard page.
But if you're planning on collaborating with other people,
you also need to commit your changes
to the source code repo that Datalab configured for you.
Go back to your notebook and click this Git icon.
Datalab will open a new web UI called Ungit,
which will show you the current status of the repo.
It's showing us here
that we have one file ready to be committed.
First, we'll need to add a commit message,
then click Commit.
Of course, this is Git, so we're not quite done.
We need to push our changes to the origin repo as well.
For some reason, Ungit hides some options in its UI.
But if you click the name of the master branch here,
you'll see an option to push your changes.
Jump back to the GCP Console and find Source Repositories
in the TOOLS section of the menu.
This will open a new UI for you,
then you can find your datalab-notebooks repo
and see your Example 1 notebook has been committed
and pushed to the repo.
Okay, let's go back to Cloud Shell
and Control + C out of this SSH tunnel.
You can always use datalab connect
to reconnect to it in future as we did earlier.
For now, let's delete our instance.
I'll just clear my screen first.
We can delete the instance
with datalab delete and the name of the instance.
The command prompts us
to confirm that we're happy to go ahead.
And note, it tells us
that the persistent disk will not be deleted.
We'll just enter Y,
and it will take a few minutes to delete our instance.
If we go back to the Compute Engine section
in the GCP Console,
and select Disk,
we can see that the my-datalab-pd disk still exists.
So let's create a new Datalab instance
that reuses this disk.
Back in the Cloud Shell, we can type datalab create,
a new instance name, I'll call mine new-datalab, --disk-name
and the name of the existing persistent disk.
Once again, we need to specify the zone,
and make sure you use the same zone as last time
so that the instance has access
to your existing persistent disk.
Then the command will quite quickly create a new instance
and try to set up an SSH tunnel.
And now we have our second bug.
Datalab is confused about our SSH keys,
and it's getting a permission denied error
when trying to set up the SSH tunnel.
To fix this, hit Control + C,
and go back to the Compute Engine section
of the GCP Console.
Then select Metadata
and SSH Keys.
Click Edit, and delete the datalab key, then click Save.
Back in the Cloud Shell,
we should be able to connect to our new instance
with datalab connect and the name of the instance.
This will force Datalab to reconfigure the SSH key pair.
Our new instance is now ready to go,
so let's click the Web Preview icon again
to open the dashboard.
In the notebooks folder,
here's the notebook we saved to our persistent disk
when we were using our previous instance.
We can open it up and check it's all there. Looks good.
Great job, Cloud Gurus.
Feel free to play around some more
with your Datalab instance.
But remember to stop or delete the Compute Engine VM
when you're finished as it is a billable resource.
That's it for this lab. I'll see you in the next video.
```

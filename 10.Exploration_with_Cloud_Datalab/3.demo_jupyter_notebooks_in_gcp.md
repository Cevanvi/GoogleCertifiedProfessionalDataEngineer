## Lesson Transcript
```text
Welcome back Cloud Gurus.
We're going straight into another lab here
to learn more about what Jupyter Notebooks can do in GCP.
In this lab, we'll take a tour of some
of the sample notebooks included
with Datalab to learn how easy it is to interact
with GCP services like BigQuery,
cloud storage, and even Cloud ML Engine.
Just like the last lab, all you need to follow along,
is a GCP project and access to the cloud console,
where we'll use Cloud Shell
and the built-in web preview to access our notebooks.
So let's get started.
Okay, Cloud Gurus.
Back in the cloud console, let's open the Cloud Shell.
Once again, I'll fullscreen mine, and zoom in
so you can see what I'm typing a bit more easily.
We know the command to create a new Datalab instance,
datalab create, and I'll call this one datalab-gcp.
But I'm actually not going to run this.
In my project I still have the persistent disk
from the last lab, and you may have that as well.
We can reuse that disc when we create our new instance
by running datalab create, the name of the new instance,
and then the option --disk-name,
and the name of the existing persistent disk.
If you followed the previous lab,
it should be called my-datalab-pd,
but if it's called something else,
you can always look for the name in the disk section
of the compute engine console.
Just like before, we'll be asked to pick a zone.
And again, I'll pick europe-west2-c, which is option 23.
After a while, our Datalab instance will be created.
But it looks like we've hit that metadata bug again.
I'll hit control C here, and we'll need to go back
into the console and delete the broken SSH key.
Select Metadata from Compute Engine in the menu.
Then click SSH keys, Edit, and Delete the Datalab key.
Then click Save.
Back in the Cloud Shell, we can run data lab connect
and our new instance name.
Now that that's ready, we can click on the web preview icon
and change it to port 8081.
And we should arrive at our Jupyter Notebooks dashboard.
So in this lab, we're going to have a look
through some of the tutorial and example notebooks
that come bundled with Datalab,
to show off how they can interact with our GCP services.
Let's go back up to the Datalab folder
and browse to Docs, Tutorials, and BigQuery.
There's a bunch of sample notebooks in here
that demonstrate things like accessing the BigQuery API
using DML, or Data Manipulation Language,
and even how to write UDFs, or User Defined Functions,
for BigQuery from within a Jupyter Notebook.
Let's start with something simple.
We'll open the Hello BigQuery notebook.
Before we start looking through this notebook,
click the dropdown next to clear, and click Clear All Cells.
This deletes any preloaded calculated output,
so we can start with a fresh notebook.
Now in this very basic example,
you can see how notebooks can be used
to give context around data.
We've got some mark down cells here,
which describe what data we're going to present.
Then we have some embedded code running the BQ command
to access BigQuery.
In this case, it's querying a public sample data set
of Wikipedia articles,
ordering them by the total number of characters,
but limiting the results to the top 10.
Before we run this,
let's add something to this BQ statement.
You can click in the cell,
and I'm going to add offset 10 just after limit 10
so that it skips the first 10 results.
This shows you just how easy it is for you
or one of your collaborators to change the code
in a notebook and rerun the calculations
to get their own results.
Click Run to run this code block.
You can click it alongside the block
or use run in the top menu.
There's also an option there to run all the cells at once.
And now our results become part of the notebook.
They will be cached here,
so if we save the notebook and share it with someone,
they can see these cached results
without having to run the query themselves.
Okay, let's move on to something a little more advanced.
We'll open the SQL and Pandas DataFrames notebook.
Again, the results of all of the compiled cells
are already cached as part of the book, but that's no fun.
So let's clear all the cells
and walk through them one by one.
As stated in the opening paragraph,
this notebook demonstrates how you can combine SQL queries
against BigQuery with declarative code in Python,
using libraries such as numpy, pandas, and matplotlib.
So the first thing we do,
is import the Python libraries we need.
And of course, once we run this,
our Python kernel session persists them for us to use later.
Next, we'll use the BQ command to create a new query.
This query is going to look up some sample HTTP logs.
I'm just clicking the run icon
next to every component in turn.
We can sample the query and get the first 5 rows
just to check that we're happy
with the data that's being returned.
Next, we create an object called DF,
which executes the full query
and stores it in our object variable as a data frame.
The len statement shows us that we have 8,392 rows
in our data frame.
We can now experiment on our data frame with Python,
using a bunch of different functions.
For example, head(5) will give us the first 5 rows.
Dtypes will show us the types or schema of the data frame.
And we can create other Python objects using our data frame.
Here, we'll create the object called groups,
using the data frames group by function,
to sort our results on the endpoint column.
We can iterate over our group's object
and use other Python functions to analyze the data.
The describe function can be used
to give us a mathematical breakdown
of the values in our data.
You can even run complex aggregation functions
or Python Lamdas.
Don't worry too much about these warnings.
They're just telling you
that some functions might be deprecated
in future versions of Python.
So let's move on to the data visualization stage,
where we can do some even more advanced transformation,
creating a new time series table by pivoting the data frame.
Once again, len shows us
that we still have 8,392 rows of data.
It's now super easy to plot the new time series data
because DataFrame objects have a built-in function,
called plot, which calls matplotlib.
Again, don't worry too much about these warnings.
Now, as is argued here,
the total data is not very meaningful
so we can use the built-in resample function
to provide time windowing.
So now we've worked on two different notebooks.
Before we move on, click this icon in the menu at the top.
A new tab will open showing you all of the active notebooks
that currently have a running kernel.
This is a good way to keep track of your resources.
For example,
if we accidentally left the Hello BigQuery notebook running,
we can shut it down here.
Okay, let's look at one more example notebook
so we can see how Datalab helps us integrate
with backend GCP services.
Back in the Dashboard, Open Up Docs, Samples, ML Toolbox,
Image Classification, Flower,
and the notebook called Service end to end.
Start by clearing all of the existing cells.
This notebook demonstrates how you can do simple preparation
of a machine learning task inside a notebook,
then use the cloud ML API for pre-processing, training,
and prediction with a full ML model.
The model itself is concerned with image classification,
using different types of flowers as examples.
Now, rather than have me walk you through this one,
I'll encourage you to read each cell step-by-step
and run all of the code in turn.
This notebook will make use of the ML API
as well as data flow and BigQuery,
so may cost a few dollars in resources,
but you should complete it in under an hour.
As you can see, the entire power
of the Python data science toolbox,
as well as GCP's suite of ML services,
is at your disposal here,
all of which can be used and shared in an interactive way.
Now don't worry if a lot of this is going over your head.
Really, I'm just trying to show you the power
of Jupyter Notebooks running in Cloud Datalab.
Data scientists and analytics folks love this stuff,
but as a data engineer
and someone who wishes to pass the data engineer exam,
you just need a working knowledge of what Datalab can do
and where its place is in the overall ecosystem.
If you are interested
in pursuing Jupyter Notebooks further, I'd highly recommend
our Introduction to Jupyter notebooks course,
right here on A Cloud Guru.
That's it for Datalab, cloud gurus.
If you have any questions,
please let me know in the course forums.
And if not, I'll see you next time.
```

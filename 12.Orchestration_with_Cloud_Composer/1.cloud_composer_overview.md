## Lesson Transcript
```text
Hello, Cloud Gurus.
Welcome to our chapter on Google's Cloud Composer.
A fully managed workflow orchestration
service built on Apache Airflow.
That's right.
It's yet another Apache big data service
adopted by Google Cloud.
Cloud Composer is built using Apache Airflow.
This should emphasize the importance of
the Apache ecosystem in the world of big data.
But once again, using Apache tools is made
easier by GCP offering them as a managed service.
Saving you all of the toil of setting
things up for yourself.
There are also 2 further benefits to this
crossover between GCP and the open source world.
Google are actively contributing back
to the Airflow project and there's no risk
of vendor lock-in because you can always
migrate your work to other Airflow environments.
Now let's have a quick overview
of the Apache Airflow system.
Airflow is a task orchestration system designed
to automate complex interdependent tasks
into pipelines or workflows.
It was created at Airbnb in late 2014
as a solution to address the problem
of managing increasingly complex workflows
authored by multiple data science teams.
Airbnb's data engineers were dealing with
multiple large data sets and writing scripts
to automate batch processing jobs.
These jobs in turn will often dependent
on other data sets and jobs.
They assess the current software landscape
and decided that there was no real solution
out there that would provide the level of
mission-critical scheduling and management that
they needed for their data pipelines.
So they created Airflow based
on some basic principles.
Each stage of the pipeline is created in code.
Airflow itself is written in Python and
each workflow is also written in Python
allowing it to immediately benefit from Python's
close relationship with the big data community
as well as making it highly extensible.
Airflow provides central management and
scheduling of all workflows, managing
their tasks on an array of distributed workers.
A bit like compute nodes in the other
distributed platforms we've been looking at.
Airflow also provides an extensive CLI tool
as well as a comprehensive web UI
for creating and managing workflows.
At the heart of an Airflow workflow is
the directed acyclic graph or dag,
which has its origins in math and graph theory.
But don't be put off by this eclectic term.
It's actually a very simple concept.
Let's have a super quick dag 101.
A dag is basically a graph consisting
of nodes connected by edges.
In this graph, you can think of the edges as how
we traveled from one node to another.
Because it's directed we always
travel in one direction and it's acyclic.
Which is just a fancy way of saying
that we never circle back.
It's impossible to reach the same node
more than once by traversing along the edges.
With these features it's possible to use dag's
that represent dependencies between nodes
that must be traversed in a specific order.
Airflow uses dag's to represent
a collection of all the tasks in
a workflow organized in a way that shows their
relationships and dependencies.
And the dag itself is represented in Python code.
We'll take a closer look at what
that code looks like in the next video.
The important thing about the dag is it's purely
about when the tasks should be aligned to execute.
It makes sure that whatever they do happens
at the right time and in the right order.
Inside the tasks of the dag we find operators
which are used to actually specify what is to be done.
Operators perform an action such as execute some
code or read and write from storage or
interact with another GCP service.
In addition to the dag,
our workflow can contain parameters
about when it should run, what its dependencies are
and who should be notified once it has been completed.
Cloud Composer or Airflow under the hood manages
the resources required to ensure that the
workflow completes successfully.
At this point, you might be thinking isn't Dataflow
the workflow tool that we use for processing data pipelines?
And it is definitely one of the tools in our tool bag.
But while Dataflow is specifically about processing batch
or streaming data using the features of the Apache Beam SDK.
Airflow or Cloud Composer is an orchestration tool that
can use any Python code at any stage of the pipeline.
You can think of it more as a scheduler than as a
transformative part of the pipeline itself.
In fact,
orchestrating Dataflow with Cloud Composer
is a common pattern.
Let's look at some simple examples
of Cloud Composer workflows.
In this example, a workflow runs daily, sets up
a Dataproc cluster, performs some Spark analytics,
writes the results to GCS and emails an administrator
and then deletes the Dataproc cluster.
In another example, a weekly scheduled workflow
could query an external data source and then
use the insights to generate and send out a
marketing email or newsletter
again by contacting an external system such as SendGrid.
Maybe you have an hourly schedule to check
for new data dumps in GCS which can then be fed
through a data flow instance and out into Bigtable.
And in fact, while Airflow is ideally suited for
data pipelines it can also be used for many other
scheduled automation tasks outside of big data
like operational infrastructure automation.
You could use Airflow to run some
deployment manager templates to stand up
a test GCP environment for example.
Again, the limit is really your imagination
as each workflow is entirely customizable
with the full back catalog of supported
Python modules and features.
Okay, Cloud Gurus.
Now we've got a good understanding of what Airflow is for.
In the next video we'll understand how
it is managed for you by the Cloud Composer
service and what resources Cloud Composer
creates and manages.
```


## Resources 

- [Blocked Configurations](https://cloud.google.com/composer/docs/concepts/airflow-configurations) 
- [Airflow Plugins](https://airflow.apache.org/docs/apache-airflow/stable/authoring-and-scheduling/plugins.html) 


## Lesson Transcript
```text
Hello Cloud Gurus in this lecture we'll look
at some of the advanced features
of Apache Airflow and Cloud Composer.
As we mentioned in our first video,
when you create a composer environment
you can specify some custom Airflow parameters.
These get written to the Airflow CFG file
that is used to configure services when
Airflow is run for the first time.
However, changes to some of these Airflow settings
are blocked by the Cloud Composer service.
And some can only take specific modifications.
These constraints are common when
dealing with a managed service.
There's a link to all of the blocked
configurations in the resources section.
When creating or updating an environment
you can also add environment variables
which will be passed by Cloud Composer
to elements of Airflow, such as the scheduler,
web server, and worker processes.
Environment variables are defined in a section
which are then simply key value pairs.
Once your Cloud Composer environment is up and running,
it needs to be able to authenticate with
other services in your GCP project.
Composer does this with a feature
of Airflow called Connections.
A connection is a collection of authentication
information which can include hostnames, logins, keys,
or other secret or sensitive information.
By default Cloud Composer will create connections
for BigQuery, Data Store, Cloud Storage,
as well as a generic GCP connection.
Each of these will contain a service account key used
to authenticate against the GCP API in question.
You can also create custom connections
using the Airflow Web UI.
Connections to Airflow's own SQL database use the cloud
SQL proxy created as part of Composer's initial setup.
But you can also add additional Cloud SQL
proxies to the GKE cluster to connect
to other Cloud SQL instances.
Once you've got your database connection set up,
Airflow provides some really useful tools in its web UI.
You can use the ad hoc query page to run SQL
queries against any connected database
and airflow even has some built in visualization,
quickly creating charts from SQL
results with just a few clicks.
Because Composer runs on Airflow you can benefit
from the many different ways to extend the
capabilities of Airflow under the hood.
You already have a fully supported Python installation
and package library at your disposal.
But if you need to add extra functionality
you have lots of options.
You can extend the local Python environment
by adding additional custom libraries.
Packages and dependencies can be specified
in the Composer environment configuration page.
You can also create your own Airflow plugins.
You can use this feature to, for example,
write your own custom operator or
customize the Airflow web UI.
There are other types of Airflow object beyond the scope of
this lecture that can also be customized using plugins.
If you want to create a custom environment for a
task in a workflow without installing
dependencies across all of your workers.
You can use the PythonVirtualEnvOperator
to set up isolated Python virtual envs
each with their own set of libraries.
And if you really need complete
control over a task environment
you can use the KubernetesPodOperator to run a task
inside a pod on the Cloud Composer GKE cluster.
You just need to be careful not to
compete for resources in your cluster
with Composer's own worker pods.
So this takes a bit of careful planning.
Okay, cloud gurus.
You should now have a good grasp of
Cloud Composer and the basics of what
you can achieve with Apache Airflow.
As the underlying product here is not
a Google original it's unlikely you'll get
too many Composer questions in the exam.
So we don't need an exam tips lesson for this chapter.
Just keep an eye out for mentions of Airflow
or scenarios where a really customizable
data workflow is required.
Thanks for watching Cloud Gurus and I'll see you next time.
```

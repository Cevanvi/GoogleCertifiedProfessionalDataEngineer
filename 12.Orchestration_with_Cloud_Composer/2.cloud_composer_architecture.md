## Lesson Transcript
```text
Hello, Cloud Gurus.
So now we understand what Airflow is,
let's learn how it is implemented and managed for us
in GCP by Cloud Composer.
Apache Airflow is implemented
as a microservices architecture.
It's deployed using numerous GCP resources
grouped together into a Cloud Composer environment.
You can have more than one Composer environment
inside a GCP project,
but each environment is an isolated installation of Airflow
and all of its component parts.
Some of these parts get deployed into a tenant project.
This is a Google-managed project
that you can't see or control.
In here, Composer places the Airflow database
running on Cloud SQL and the Airflow web server,
which provides its web UI running on App Engine Flex.
It will also configure the Identity-Aware Proxy
to control access to the web server.
Meanwhile, in your primary project,
Composer will create a Google Kubernetes Engine cluster.
In this cluster, it will create deployments for Redis,
the Airflow scheduler, and the Airflow workers,
along with a Cloud SQL proxy.
It will also create 2 Pub/Sub topics
for messaging between the various microservices
and a Cloud storage bucket for logs, plugins,
and most importantly, the DAGs themselves.
Cloud Composer automatically configures parameters
and environment variables for Apache Airflow,
but you can also customize some parameters,
although not all of them,
when you create the environment.
We'll take a look at some of those in a hands-on lab
coming up next in this chapter.
As we said before, workflow DAGs are written in Python.
And like any other Python program,
the DAG can consist of a single file
or multiple files with imports and dependencies.
This script represents all of the variables, operators,
and stages of the workflow
tied together with a DAG object and definition.
Each task in the workflow is designed as an instance
of an Airflow operator.
In this example,
we start with the dataproc_cluster create operator
to create a dataproc_cluster.
Then we use the dataproc_hadoop operator
to send it a job that we have previously defined,
in this case, it's a wordcount job.
Then we use the dataproc_cluster delete operator
to define the stage in the work flow
that will delete the cluster once it's no longer needed.
And finally, we specify the ordering
and dependency of the tasks
using these double greater than symbols.
For the real Python Gurus out there,
these look like bitwise operators,
but in Airflow,
they've been redefined as a sequence operator.
The scheduler will find any DAG object
that you have defined in any Python scripts
uploaded to the GCS bucket.
And if all the dependencies are met,
the workflow will be scheduled.
It's important to note
that when you delete a composer environment,
it doesn't clean up all of the resources it created.
Everything in the tenant project,
that was hidden away from you anyway,
will be removed along with the GKE cluster that was spun up.
However, you'll still be left with the Pub/Sub topics
and the GCS bucket to delete manually,
or you may wish to keep the bucket, of course.
This all sounds a bit magical at the moment,
so let's get hands on with a Cloud Composer lab.
Thanks for watching, Cloud Gurus,
and I'll see you in the next video.
```

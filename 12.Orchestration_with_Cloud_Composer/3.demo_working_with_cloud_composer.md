## Resources 

- [hadoop-wordcount.py](resources%2Fhadoop-wordcount.py)
- [War & Peace at Project Gutenburg](https://www.gutenberg.org/ebooks/2600) 
- [GCP Regions and Zones](https://cloud.google.com/compute/docs/regions-zones) 

## Lesson Transcript
```text
Hello, Cloud Gurus.
Welcome to this hands-on lab, Working with Cloud Composer.
In this lab,
we'll create our first Cloud Composer environment
and then we'll create our first Cloud Composer workflow.
To do this, We'll be writing our own DAG,
or Directed Acyclic Graph,
in Python using Airflow operators.
This will allow us to completely automate a Dataproc job,
including creating a cluster, running the job,
and deleting the cluster at the end.
All you need to follow along with this lab is a GCP project
and the Cloud Console.
We'll use the Cloud Shell Editor to write our DAG.
This is quite a Python-heavy lab,
but you can find all the code in the resources section.
So let's get started.
Well, here we are once again, in the Cloud Console.
Let's start by going to composer
in the big data section of the GCP menu.
If this is your first time using Cloud Composer,
you'll be prompted to enable the API.
As we learned in the previous lectures,
the microservices of Airflow
are all created inside a Cloud Composer environment.
So let's start by clicking Create Environment.
First we'll give our environment a name.
I'll call mine Composer Lab.
By default, The GKE cluster created by Cloud Composer
contains three nodes,
and this is the minimum number of nodes you can have.
Next, select a region near to you.
I'm going to use Europe West 2.
These are the only mandatory settings we need.
The zone will be chosen for us,
although we can choose to override it.
Each of the other settings for the GKE cluster
inside Cloud Composer can also be changed
or they will just use their default setting.
For example, the machine type is an N1-standard-1
and the nodes, by default,
have a 100 gigabyte persistent disk.
Let's change that to the minimum 20 gigabytes
to save a few pennies.
It's worth noting that some of these settings,
like the service account being used,
cannot be changed once an environment has been created.
Further down, we can optionally choose
the version of Airflow and Python
that will be running in our environment.
We can also click here to unhide
some advanced configuration options.
Here, we can change the network we use
or add custom parameters to our environment.
You can add configuration overrides here,
although check the docs, as I said before,
not all configurations are available to you.
You can also add environment variables
that will be presented to all the workers in the cluster.
And you can optionally add labels.
Perhaps you might do this
to help with the billing report later.
For now, just click Create.
It takes quite a long time to set up the environment here,
so pause the video and come back after a quick power nap.
Once the environment is up and running,
we can see it in the list here.
We'll click through to get some details in a moment,
but first, take note of some quick links that we get.
This is a link to the Airflow web server,
allowing us to directly access the Web UI.
And this link here will take us to the DAGs folder
in Cloud storage.
That's where we'll drop any Python DAGs that we write
to get picked up by the scheduler.
Let's click through to see our environment configuration.
We get some handy links here
to the GKE cluster that composer has created,
along with, again, links to the DAG folder
and the Airflow Web UI.
Elsewhere on this details page,
we can also view custom Airflow config overrides,
environment variables, and even custom PyPI packages,
if we'd installed them.
Now before we start playing with Airflow,
we need to create a GCS bucket
that will store the input and output files
for our Dataproc job.
Select Storage from the GCP menu
and create a new bucket.
I'm going to call mine Tim, A Cloud Guru Airflow Lab,
following the usual naming convention.
click Continue and let's make this bucket regional
and put it in the same region as the composer environment.
For me, that was europe-west-2,
then scroll down and click Create.
Inside the new bucket, create a folder called input.
Now, inside this folder, we'll need to put a text file
for our word count job to use.
I'm going to use the text of War and Peace
from Project Gutenberg that I've used in previous labs.
There's a link to this in the resources section,
or feel free to use any other texts that you'd like.
Upload the file.
Then select the file
and you'll see this neat little summary.
Click this icon here to copy the GCS link to the file
to your clipboard.
While we're snooping around the GCP console,
let's take a quick look at the resources
composer has created for us
to build the Airflow environment.
Under Kubernetes Engine, here's the GKE cluster.
And as we know, under the hood,
our cluster will be running on three compute engine VMs.
We can also see the pub sub topics that have been set up.
Okay, let's go back to Cloud Composer.
From the environment list page,
click the link to the Airflow web server.
You'll be prompted to authenticate
by the Identity-Aware Proxy,
then taken straight to the Airflow Web UI.
The default page shows all of the DAG workflows
managed by the system.
Out of the box, we have a monitoring workflow,
which will run every 5 minutes
to make sure the system is healthy.
Before we add our own DAG,
let's set some Airflow variables.
Note, these are separate from any environment variables
we set with Cloud Composer,
but will be available to any DAG that we run under Airflow.
Go to admin and variables.
We're going to create variables for our project,
desired compute zone and input and output files.
Click Create.
For the first variable, use the key input_file,
then paste in the value that you just copied from GCS.
Then click Save and add another.
Now create a variable called output_dir.
Paste in the same value, but delete the file name
and the input directory at the end, and add output instead.
Dataproc will create this directory for us
inside the GCS bucket when it writes the output of the job.
Click Save and add another.
Call this one gcp_project,
then enter the name of your GCP project.
Then click Save and add another one more time,
and finally, use the key gcp_zone
and enter a zone in the region
where you created your composer environment and bucket.
I'm going to use europe-west-2c.
There's a link to all of the GCP regions and zones
in the resources section for this video.
Then click Save.
You should now see a list of all of your Airflow variables.
Let's go back into the Cloud Console
and open the Cloud Shell and the Cloud Shell Editor,
where we'll write the Directed Acyclic Graph or DAG
for our workflow.
Feel free to pause the video as necessary
and type along with me
or grab the complete code from the resources section.
Start a new file called hadoop_wordcount.py.
First, we import some libraries
that we need to run the Airflow DAG.
Then we set some variables,
the input file and the output directory.
We set WordCount JAR to use one of the example Hadoop files
that we know will be bundled with the Spark installation
on the Dataproc cluster.
Then we set the arguments for the job.
Next, we write a quick hack to get a date for yesterday.
This might seem like an odd thing to do,
but you'll see why in a moment.
Now we set the default arguments for our DAG.
We set the start date
using the date of yesterday that we just created.
Setting the start date as something in the past
means that the DAG will be scheduled immediately,
as soon as it is picked up in the bucket.
We set the email notifications to false.
We allow the DAG to be retried once if it fails,
after waiting for 5 minutes.
And finally, we set the project ID
to the Airflow variable we previously configured.
Now we start to define the actual DAG.
First, we give it a name and set the recurring schedule.
Here we've told it to run every 24 hours.
And now here's our first operator.
Create Dataproc cluster
is an instance of the Dataproc cluster create operator.
We set some variables, such as the name of this task,
and the parameters of the Dataproc cluster itself.
Note that we set the cluster name to include a timestamp
so we can tell the clusters apart.
Next, we create an instance of the Dataproc Hadoop operator
to actually run the Hadoop job on our Dataproc cluster.
Then we define an operator to delete the Dataproc cluster.
Note the trigger rule all done,
which means that this operator
can proceed to delete the cluster,
regardless of the state of any Dataproc jobs.
Finally, we use Airflow sequence operator
to specify the ordering
of all of the operators we've just created in our DAG.
Create the cluster, run the Hadoop job,
and delete the cluster.
Now save the file.
Pop back to the GCP console,
where you should still have the composer page open.
Click the DAGs folder shortcut here.
This is the location in GCS
where DAG files should be uploaded.
Let's copy the name of this bucket.
Go back to the Cloud Shell Editor
and let's copy our new Python file
up to this bucket, using gsutil.
Paste in the name of the bucket
and don't forget to add /dags to the end.
Let's go back and refresh our bucket
to check the file is there.
Now let's jump into the Airflow UI.
Click on the DAGs tab.
The scheduler doesn't check right away
so it might take a few moments for your new DAG to appear.
I'll just reload the page.
And there it is.
Let's click on our new DAG
to take a look at what it's doing.
Here we can see a tree view of the tasks.
Right now, the create Dataproc cluster task is queued.
We can also see this as a graph view.
Each stage is color-coded to let us know its status.
The create Dataproc cluster stage is now running.
You can click on each task for more details,
or to look at its logs.
If we go back to the GCP console
and into the Dataproc section,
we can see the Dataproc cluster
that's being set up by Airflow.
Let's wait a few moments
for that cluster to finish provisioning.
Now let's go back to the Airflow graph of our DAG.
Back in the graph view,
we can see that the run Dataproc Hadoop task is running.
Let's give it a minute or so and reload the page.
And now the delete Dataproc cluster task is running.
Again, we'll wait a minute or so and reload the page.
And we can see that our DAG has completed successfully.
Let's go back to the cloud console
and we can see that our Dataproc cluster has been removed.
Now let's check in our GCS bucket
to see the output of our job.
And here are our completed word count files.
Now I know what you're thinking,
word counts aren't very exciting,
but this is just the most simple example of a data workflow
that can be scheduled and run for you by Airflow.
To clean up, let's go back to the composer page
and delete the environment.
Just tick the box next to the environment,
click Delete and Confirm.
Just like setting up the environment,
this takes quite a long time,
but you can leave it running in the background.
It will remove the hidden tenant services,
as well as the GKE cluster.
While it's doing that, let's go into Pub Sub.
And here, we'll manually remove these 2 topics.
This just leaves us with the cloud storage buckets.
As part of this lab, you'll have created your own bucket
and Airflow and Dataproc
will also have created their own buckets.
Of course, you can optionally keep these
if you want to preserve any data contained in them.
And that about wraps it up for this lab, Cloud Gurus.
If you have any questions, please let me know
in the course forums.
I'll see you next time.
```

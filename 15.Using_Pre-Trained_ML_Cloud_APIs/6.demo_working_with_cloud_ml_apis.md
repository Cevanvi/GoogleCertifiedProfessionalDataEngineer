## Resources

- [Cute dog picture](https://pixabay.com/photos/dog-pet-canine-animal-fur-snout-3277416/)
- [Bleeding hand picture](https://pixabay.com/photos/accident-bleed-bleeding-finger-743035/)
- [Lab code](resources/demo_working_with_cloud_ml_apis)
## Lesson Reference

1. In the GCP sandbox click on "APIs & Services" and then select "Library".

2. Search for and enable the following APIs:
   Cloud Vision API
   Cloud Functions API
   Cloud Build API

3.From the main navigation menu click on "Cloud Storage" and then select "Buckets".

Create 3, uniquely-named buckets:
<UNIQUE BUCKET NAME>-uploads
<UNIQUE BUCKET NAME>-approved
<UNIQUE BUCKET NAME>-flagged

4. Open the cloud shell and create an imagecheck directory.

mkdir imagecheck

5. change into the imagecheck directory.

cd imagecheck

6. Copy and paste the following link into your browser to download the code needed for the lesson.

https://acloudguru-content-attachment-production.s3-accelerate.amazonaws.com/1605056415442-Code.zip

7. Unzip the file you just downloaded.

8. At the top of the terminal window, click on the "Open Editor" button.

9. Make sure you're in the imagecheck folder. Then upload the 3 files you unzipped into that directory:
   requirements.txt
   config.json
   main.py

10. Click on the config.json file to open it. Replace the <YOUR_FLAGGED_BUCKET_NAME> and <YOUR_APPROVED_BUCKET_NAME>
    with your own bucket names that you created earlier. (Make sure you keep the quotation marks.)

11. From the File Editor menu, click on "File" and then select "Save All".

12. Go back to the Cloud Shell Terminal and enter the following command to deploy the cloud function, making sure to
    replace <UPLOADS BUCKET NAME> with the name of your own upload bucket name:

gcloud functions deploy check_images --max-instances 3 --runtime python39 --trigger-resource <UPLOADS BUCKET NAME>
--trigger-event google.storage.object.finalize --region <The Bucket Region> --no-gen2

13. It will take a few minutes for the function to deploy. After it's successfully deployed you can start uploading the
    provided images to your uploads bucket. After you load each image, wait a few seconds and then hit "refresh" to
    refresh the bucket. You will see that each picture has been relocated to either your approved bucket, or your
    flagged bucket.

14. Open up Cloud Functions dashboard and click on Logs tab to view the different logs for images youÃ¢â‚¬â„¢ve
    uploaded.

## Lesson Transcript

```text
Hello, cloud gurus.
In this lab, I'll show you how easy it is to leverage
one of Google's pre-trained machine learning models
with Cloud ML APIs.
So, in this lab, we're going to use the power
of the Cloud Vision API, a pre-trained machine
learning model that offers image labeling,
object detection, optical character recognition
and detection of unsafe content.
We'll call the API from a Cloud Function
that will be triggered whenever images are uploaded
to a Google Cloud storage bucket.
Our Cloud Function will use the API
to process the uploaded images, score them
for various unsafe qualities and flag them if necessary.
All you need to follow along with this lab is a GCP project
and access to the Cloud Console.
As per usual, we'll write code in the Cloud Shell Editor
or you can download the complete code
for this lab from the resources section.
Also, in the resources section are links
to two test images that we'll use in this lab.
Please note that one of them depicts
some blood on a person's hand.
I'm pretty sure it's faked,
but we needed something to catch the attention of the API.
If you'd rather skip this image, that's fine.
I'll let you know in the lab video when to look away.
Okay, cloud gurus, as soon as you're ready,
join me in the GCP Console.
Okay, cloud gurus, we're back in the Cloud Console
and the first thing we need to do to start working
with Cloud ML APIs is to make sure that they're enabled.
Let's go to APIs and services in the GCP menu.
Then, click enable APIs and services,
search for vision and then select the Cloud Vision API.
Click enable if you get the option here.
On my screen it says manage
because I've already got this API enabled.
So, now, we need to create three Cloud Storage buckets.
Let's go to Storage in the GCP menu and click create bucket.
The first bucket is going to be for uploads.
Let's imagine our users can upload images to our application
and the uploads get dumped into this bucket.
I'm going to call my bucket Tim, a cloud guru uploads.
I'd recommend you use a similar convention
for your bucket name.
Start with your project name
and end with the suffix uploads.
Then, click create.
Go back and we'll create another bucket,
this time for approved images.
If our function checks the image
and doesn't find anything to be concerned about,
it'll copy the image into this bucket.
I'm going to call mine Tim, a cloud guru approved.
Finally, we'll create a bucket for flagged images.
I'll call this one Tim, a cloud guru flagged.
As I said at the start of this lab,
we'll access the API by writing a cloud function.
This is a nice little introduction to how cloud functions
are often used to piece together various services
in the big data and ML toolbox.
So, to write our function, let's first open the Cloud Shell,
and then, the Cloud Shell Editor.
We'll start by creating a new directory for our function
called image check.
And let's move into that directory.
Now, we need to create three files.
We're going to write our function in Python.
So, first we create a requirement dot txt file
that specifies what libraries we need.
We need just two libraries for this function,
the Storage and Vision API libraries.
Let's save that file.
Now, we'll create a file called config dot JSON
to separate some configuration
from our actual application code.
In this file, we'll define the flagged bucket
and approved bucket names.
Make sure you use your own bucket names here, not mine.
Let's save that file.
Now, we'll write the main script of our application.
Create a new file called main dot py.
Feel free to type along with me.
Just pause the video when you need to
or you can download the complete script
from the resources section.
To start with, we import the necessary libraries
and open our config file.
Then, we create two new GCP clients,
one to access Cloud Storage
and one to access the Vision API.
Next, we start to create our check images function.
When the function is triggered by Cloud Storage,
it will receive a payload that contains information
about the file that has just been uploaded.
We capture that information in the variable data
and use it to do a few things.
First, we grab the URI, or the location of the image.
Then, we create a new image object using
the Vision API library, and we set the source
of that vision object to the URI we've just defined.
Next, we query the Cloud Vision API.
We do this by calling the safe search detection method
on the Vision client that we already created
and we capture the output of that
into a variable called response.
From that response, we take the safe search annotation
and store it in a variable called safe.
Now, the API returns values as numbers
from note to five based on how likely it is
that an image has matched a certain category
of unsafeness, such as adult or violent content.
But to make this easier to understand,
we create a set that maps these numbers to phrases,
unknown, very unlikely, unlikely, possible,
likely and very likely.
Now, we set up a for loop to check
all of the category responses we've had back
and to look for any occurrences of possible,
likely or very likely.
As you can see, we're checking in the categories
of adult, violence and racy.
If any of these results matches,
we set flagged to true for this image.
We'll also use a standard Python print statement here
to write out some output that we can look
at in the cloud logs viewer later.
Once we've made our decision,
we have to process the image accordingly.
So, we get the location of the image
from our data variable again,
but in a slightly different way
because this time we're actually going to grab
the binary data of the image
and store it in a variable called blob.
Then, based on how not safe for work the image is,
we switch buckets in our Cloud Storage client,
either to the flagged or the approved bucket.
Then we create a new blob object
in our chosen bucket and write the data.
Finally, we delete the original blob.
Let's save the file.
So, now, we've written our function, it's time to deploy it.
In the Cloud Shell make sure you are in the same directory
as the code that you've just written.
I'll just clear my terminal.
Then we'll type g cloud functions deploy check
underscore images minus minus runtime Python three seven
minus minus trigger resource,
and then the name of your uploads bucket
minus minus trigger event google dot storage
dot object dot finalize.
This command will deploy our function
using the Python 3.7 runtime.
It's important that the name of the function here
matches the name of the function that we've actually defined
inside our script.
Check underscore images.
The trigger will be the upload storage bucket
and the trigger event will be when an object is finalized.
That means when it's finished being written to the bucket.
Go ahead and hit return.
We'll be prompted to specify whether we want to allow
unauthenticated invocations of our new function.
All that means is do we want to make this function available
over the internet?
If we say yes, Cloud Functions will assign an HTTPS URL
to our function, which you can simply access in a browser,
but actually we don't need that.
Our function will be triggered by events
in our storage buckets not accessed via our browser,
so type N and hit return.
It will take a couple of minutes to deploy the function,
so pause the video here while you wait
for the command to complete.
When the command returns, you'll see a bunch
of information about your deployed function,
including its status, version, runtime and service account.
Let's go back to our GCP Console
and take a look at the Cloud Function section.
Let's select the function we've just deployed
and here we can see our deployed functions dashboard.
Over time, metrics will build up and be presented here.
There's none right now, as our function
hasn't been called yet.
If we click on the trigger tab, we can see that the trigger
is defined as our uploads bucket with the event type
of finalized create.
So, let's upload an image to our uploads bucket
and see what happens.
We'll start with something that we know
will be safe for work.
I'm going to use this picture of a rather cute dog
from the website, PixaBay.
You can find the link to this image
in the resources section.
Download the image, then go to the Cloud Storage browser
and into the uploads bucket.
Upload the picture of the dog.
Now, before we do anything else,
wait a few seconds and refresh the bucket.
You'll notice that the file has disappeared.
That means our function has already processed it
and moved it to a different bucket.
I think we can assume that that picture is safe for work.
So, let's move back up a level
and look in our approved bucket.
And there's our doggo.
Let's try another image.
Just to warn you, this next image is a very mild image
of a cut on someone's hand with some blood.
I'm pretty sure it's faked,
but if this bothers you, look away now.
You can find a link for this image in the resources section.
Once you've downloaded this image,
upload it to the uploads bucket.
You can look again now.
A few seconds later, refresh the bucket
and the image will have been processed.
Let's go back up one level, and I think we can assume
that this will be in our flagged bucket.
And here it is and I'm not going to open it, don't worry.
So, how did the Cloud Vision API help us to flag this image?
Let's go back into our Cloud Function.
Select our deployed function and click view logs.
The debug print line that we put in our script
shows us the name of the file we uploaded
and the results of the safe search annotations provided
by running it through the Cloud Vision API.
This shows us its determination of the likelihood
of it meeting any of these unsafe categories.
For that last image, it feels that it's possible
the image is medical in nature
and very likely that it depicts some sort of violence.
Results like this are very useful
if we're processing numerous user uploads
and we need to automate some sort of moderation
on the content that's being uploaded.
So, this is a really good example of how we can use
an off-the-shelf ML API to help us
and we can be assured that the accuracy of this API
is going to be quite high,
as it's trained by Google Images, no less.
Okay, cloud gurus, at this point,
I'm going to let you pick some of your own images
and see if you can get the Cloud Vision API
to trigger on any of the other categories.
The stock photography website we've used so far
should be able to provide you with some suitable material.
For example, a search for swimsuits
should turn up something still mostly safe for work
that will nevertheless gain the attention
of the API's model.
Don't forget that there are limits to the GCP free tier,
so don't get too carried away.
At the time of recording, that's about 1,000 lookups
for the Cloud Vision API
and there's a generous allotment
of compute time for cloud functions.
Refer to the GCP pricing online for further details.
When you're ready to clean up the lab,
go back to the Cloud Functions dashboard
and delete the function.
Then, go back to Cloud Storage
and delete the three GCS buckets
that we created for this lab.
Please let me know in the course forums
if you have any questions.
Thanks for watching, cloud gurus.
I'll see you next time.
```

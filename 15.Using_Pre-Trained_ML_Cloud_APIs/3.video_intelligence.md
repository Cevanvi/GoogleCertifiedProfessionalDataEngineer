## Lesson Transcript
```text
Welcome back, Gurus.
In this chapter, we'll be covering
some of the core capabilities of the video API.
So video sits together with vision
as part of the site building block.
The video component enables powerful content discovery
within videos.
Now a large number of concepts that apply to images
are also applicable to video.
The main difference is that with video,
the concept of time duration applies.
For this reason, we won't go into quite so much detail
on each one of the capabilities.
Cloud video intelligence API supports common video formats.
These include .MOVIE, .MPEG4, .MP4, and .AVI.
So let's look at some of the main capabilities
of the cloud video intelligence API.
First, it is able to detect labels.
This is very similar to what we had with images.
It annotates the videos with entities that are detected
within the video, and it is able to provide
a list of video segment annotations,
a list of frame annotations,
or a list of shot annotations upon request.
Next, we have shot change detection.
By default, the video intelligence API
examines videos by frame.
You do, however, have the ability to annotate a video
by shot or scene.
Therefore labeling of objects is relative
to the video scene.
Imagine people in a room where the camera is panning.
You could have 2 basic scenes, people and garden,
for example, if they're standing outside.
Next, there is the ability to detect explicit content.
A timestamp is provided where the explicit content
is detected within the video.
Then there is the ability to perform speech transcription.
Here spoken words within the video are transcribed
into blocks of text.
Speech transcription has many options
including filtering out profanity,
providing transcription hints,
putting in automatic punctuation,
as well as handling multiple speakers.
Next up, object tracking.
The video intelligence API is able to track
multiple objects within the video.
It provides the location of each object
within the various frames.
And each object detected will be associated with
a time segment with an offset.
And finally, there is text detection.
This is optical character recognition on text that occurs
within your videos.
The video intelligence API is able to provide
the actual text as well as the location of that text
within the video.
We moved through the vision intelligence API
far more quickly than we moved through the vision API.
You don't need to understand all the details
for each one of these capabilities.
You just need to have a broad understanding
of the capabilities that are available.
I'm going to finish this video by talking very briefly about
the Google Knowledge Graph Search API.
The Google Knowledge Graph Search API allows you to perform
searches on Google's Knowledge Graph.
This is how Google represents all entities
and the relationships between them.
This slide shows a really simple example with a few nodes
relating to Albert Einstein.
You can see how Einstein is linked to other entities
and concepts like mathematics, physics,
the E=MC squared equation,
the atomic bomb, and the atom.
Each entity within the Knowledge Graph is specified by
a unique machine generated identifier or MID.
Einstein has the identifier /M/OJCX.
No need to memorize that.
According to Google, some typical use cases
of the Knowledge Graph Search API are the following.
Firstly, getting a ranked list of the most notable entities
that match certain criteria.
Secondly, predictively completing entities
within a search box.
And thirdly, annotating or organizing content
using the Knowledge Graph entities.
The point here is all the objects that were detected
with the vision or video intelligence API
will be somewhere on the Google Knowledge Graph.
And you can use this search API to gain
even greater insights into your images and videos.
Okay, Gurus, that's it for video intelligence.
See you in the next video.
```

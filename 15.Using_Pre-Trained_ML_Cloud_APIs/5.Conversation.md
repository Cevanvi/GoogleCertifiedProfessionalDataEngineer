## Resources 

- [Cloud Speech-to-Text Concepts](https://cloud.google.com/speech-to-text/docs/concepts) 
- [Cloud Text-to-Speech basics](https://cloud.google.com/text-to-speech/docs/basics) 
- [Dialogflow](https://cloud.google.com/products/conversational-agents?hl=en) 


## Lesson Transcript
```text
Welcome back Gurus.
In this chapter, we'll be covering some
of the core capabilities in the area of the conversation,
AI Building Block.
So, the three aspects of conversation are Dialogflow,
Cloud Text-to-Speech,
as well as Cloud Speech-to-Text.
Dialogflow allows you to build virtual agents and chatbots.
The Text-to-Speech API allows you to convert text
to human-like speech using WaveNet voices.
And Speech-to-Text,
allows you to convert speech to text, automatically.
So we'll start with Dialogflow.
Dialogflow is a natural language interaction platform.
It can be used in mobile and web applications,
devices as well as various types of bots.
It is able to analyze text or audio inputs
and it can respond using text or speech.
Intents are central to Dialogflow.
An intent categorizes an end user's intention.
In essence, an intent is something
that Dialogflow is able to understand and respond to.
So, for example,
a simple banking telephonic system may have 2 intents.
Firstly, account balance inquiry and request call-back.
In a conversational context,
if you ask people what they would like to do,
they will not use the exact wording
for account balance inquiry.
If they want the balance,
we'll need to map what they actually say
to the account balance inquiry intent.
So, they may say things like, "What is my balance?"
"How much money is there in my accounts?"
All of these need to map to the correct intent.
In this example, we simply have 2 intents.
In most applications, there will be many more.
Here, intent is really just a category
and so, speech segments are effectively being categorized.
Each intent is really a category or class
and so, speech segments are being classified.
This is known as intent classification.
When working with intents, we need to use training phrases,
these phrases are used to train the model.
Our training phrases should be examples
of what the end users may actually say.
It's important to note that you don't have
to provide every possible way of saying something.
This is because
of Dialogflow's machine learning capabilities.
So, these Training phrases will map onto our intent.
Now, we are able to also extract parameters from the phrases
that are mapped onto the intent.
Each parameter has a type called an entity type,
which determines how data is used.
Parameters are structured data
and so, we can perform logical operations on them.
Let's consider a few phrases that are related
to requesting the weather forecast intent.
These, for example,
would all be mapped to the forecast intent.
And where possible, we could extract a time parameter
such as now or tomorrow.
And, we can extract a location parameter, where possible.
Here: Seattle.
So, let's look at how we could use intents in Dialogflow.
We have an end user,
the end user will provide an input phrase.
This will be sent to some type of agent.
The agent performs intent classification,
mapping our input phrase onto one of our intents.
From the intent, we would extract the relevant parameters.
We can also link actions to intents.
This would be where our system goes off
and does certain things,
such as retrieving an account balance.
The parameters as well as the output of the action
can be sent to create a response.
The response is speech or text
that is returned to the end user.
It's important to note that in order for Dialogflow
to understand the input phrases,
the context of those phrases is important.
Now, let's look at the Speech-to-Text API.
It does pretty much what you'd expect it to do.
It takes audio as input and converts it to text.
The Cloud Speech-to-Text recognition engine
supports a variety of languages and dialects.
So, the input audio is converted to text
and we can have an audio file as input or an audio stream.
The first is for batch processing
and the second is for real-time stream processing.
So, both batch and stream processing modes are available.
Cloud Speech-to-Text has 3 main methods
to perform speech recognition.
These are listed below.
Firstly, we have Synchronous Recognition
then, there is Asynchronous Recognition
and finally, there is Streaming Recognition.
Synchronous Recognition can be used with REST and gRPC.
As you would expect,
Synchronous Recognition only returns a response
after all the audio has been processed.
Because it is synchronous, you are limited to smaller files.
Asynchronous Recognition can be used with REST or gRPC.
Asynchronous Recognition,
initiates a long running operation in the background.
With Asynchronous Recognition,
we need to poll for the results.
Streaming Recognition can only be used with gRPC.
In this case, a bi-directional stream is established.
And here, results are produced
while the audio is actually being captured and processed.
So, Cloud Speech-to-Text can use one
of several machine learning models
to transcribe your audio data.
These models have been trained
for specific audio types and sources.
So, there is the video model for transcribing audio
in video clips that contains multiple speakers.
There is the phone call model,
this is for transcribing audio from phone calls.
There is the command and search model,
this is to analyze clips
that contain commands or search requests.
And finally, there is the default model.
Let's look at an example Speech-to-Text response.
The response contains an alternative section,
which contains a number of possible transcriptions.
Each alternative identified will have a transcript,
which contains the actual transcribed text.
Each alternative will also have a confidence.
The confidence contains a value between 0 and 1.
And as usual,
it indicates how confident Cloud Speech-to-Text is,
that the transcription is correct.
Let's finish off looking at the Text-to-Speech API.
Text-to-Speech uses a synthetic voice to create audio
from text that is presented to it.
It creates natural sounding human speech as playable audio.
In addition to normal text,
Cloud Text-to-Speech API is able
to convert Speech Synthesis Markup Language,
or SSML to audio.
SSML allows you to control the way
in which text is converted to speech.
That is how it is read.
So, you can insert pauses, for example.
You are able to include sounds.
You can say things as cardinals,
so here, the word "ten" would be produced.
You can say as ordinals,
so here, the word "tenth" would be produced.
And you can say as characters,
so here, the output would be 1-0.
You can also have things like, phrase substitution,
where World Wide Web Consortium will be used
instead of W3C.
There are many other tags available.
Okay, Cloud Gurus,
I hope you've learned a lot in this video.
I'll see you in the next one.
```

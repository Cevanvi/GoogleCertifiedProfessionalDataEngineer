## Lesson Transcript
```text
Hello Cloud Gurus.
In this video,
we're going to learn all about data processing pipelines.
Pipelines are frequently built by data engineers
to enable data to get from A to B,
or maybe that should be from A to E or even F
as there are always a few stages in between.
Pipelines are a fundamental part
of any data processing system.
At a high level,
there are four main stages to a data pipeline,
ingestion, storage, processing, and visualization.
We're going to dedicate an entire chapter to storage.
So for this video,
let's take a look at some concepts for the other 3.
Let's start with data ingestion.
This is the process by which we move data
into our data processing system.
The data itself may come from numerous sources
and be in many different formats,
the final destination of our data processing system,
maybe a data warehouse or a data lake
as discussed in our last video.
But ingestion is always the first stage in getting it there.
As we also learned in our concepts lecture,
data is usually ingested in 1 of 2 ways.
As a batch,
where we take a large amount of data,
define the parameters for its ingestion
and load it all in one go,
or with streaming ingestion,
when new data is continually being loaded into the system.
And again,
it's worth noting,
that some systems simply do this with very small batches
rather than individual items of data.
This is all still very abstract at the moment.
So to help you conceptualize this part of the data pipeline,
let's have a quick think about the types of data
we might want to ingest.
We could be moving a large amount of archive data
into a larger data analytic system.
For example,
importing relational data into BigQuery.
We could be collecting logs from multiple systems
in our stack so that we can aggregate them
and analyze them later.
Or perhaps we're streaming events from real-time devices.
This could be anything from medical devices
to wind turbines, to your family car.
And these are just 3 suggestions.
In reality,
there are hundreds more data sources to ingest from.
There are several technical challenges to bear in mind
when designing the ingestion stage of your data pipeline.
Big data by its very nature is big.
So, you need to consider network design,
the technical mechanisms for moving data,
be it HTTP, FTP, or something else,
and how these things will affect the time it takes
to load your data.
You need to choose the correct compute and storage options
for the type of data you're ingesting.
Otherwise, you may end up with a solution
that may be too expensive or too slow.
Your choice of batch or streaming ingestion
will also affect these outcomes.
As we progress through the course,
we'll learn more about designing these systems
and help you make the right choices here.
But there are also some non-technical challenges
that you should bear in mind.
The data you are ingesting should have value.
It should be useful to the business storing or using it.
Otherwise, it's just wasting money to store data
for its own sake.
Moving data can make it vulnerable.
This means you need to consider the security implications
of your data processing design
to reduce the possibility of a data breach.
You may also be working in an industry
where compliance and regulation apply,
in which case, data breaches could be disastrous
and regulations may mandate
additional transformational steps to take with your data.
Don't worry.
We have entire chapters on these issues later in the course.
So, we've covered the theory around ingesting data.
Although we'll deal with some practical examples
of this as well later in the course.
We can also assume we've stored the data in a sensible way.
Moving on,
let's talk about how we process the data
that has been stored
in order to do something useful with it.
Recall the concept of ETL or extract transform and load
from our core concepts lecture.
Traditionally, the process of ETL
has been used for data ingestion to take data from a source,
and then manipulate it to fit the parameters
of the destination system.
This might mean for example,
the data needs to be changed to match a particular schema
before it can be stored.
With newer cloud-based systems for big data,
ETL is sometimes now replaced with ELT,
or extract load and transform,
where data can be loaded directly into a data lake
and transformed later as required.
In reality,
you may end up using both types of approach
or a compromise between the 2.
There are some common transformations
that you may apply to your data
to make sure it's as valuable and as useful as possible.
For example,
formatting the data to match the destination storage system,
labeling data perhaps with metadata regarding its source
or other parameters,
filtering out bad data,
which could mean data that you don't want or need
or data that has become corrupted during transit
or validating the data
to make sure that it meets certain other requirements
before accepting it for storage.
These are still high-level concepts,
and you'll see plenty of examples of these
as you start to work with the GCP products and services
that we explore in this course.
But let's talk through just one example for now.
Imagine you have a lamp connected to a smart home system.
It may contain data such as this,
it's ID, how brightly it's lit,
the status of its bulb,
maybe the local Wi-Fi hub it's connected to.
We can format this data
so that it fits the data storage system
that we want to store this in.
In this example,
let's say it's a wide column NoSQL database.
At the labeling stage,
perhaps we add some metadata retrieved
from another system based on the source of our lamp data
to add our customer's ID.
Next, we perform some internal filtering
to remove the data we don't want or need.
Perhaps we don't actually need
the name of the customer's Wi-Fi network
and storing it might be problematic.
Finally,
we can validate that this data is acceptable.
Later in the course,
you'll learn how transformations like these and others,
can be achieved with Cloud Dataflow. Finally,
once we've ingested, stored, and processed our data,
it's time to do something with it.
For most types of big data,
this means some sort of process of visualization.
Data is often visualized using various dashboard tools
to provide live optics on the current state
of an organization's metrics.
In this example,
we're looking at an AdWords report generated in Data Studio.
Data Studio can also produce interactive reports
that can be easily accessed by business analysts
to quickly gain insights from data.
Of course,
visualization is just the end of one type of pipeline.
Often, data is being used to provide decision-making
for other parts of a system,
perhaps for an e-commerce recommendation system,
or maybe the data is being used to build
a machine learning model,
perhaps to do some classification or prediction work.
As we work through the various technologies
you need to learn for this course,
you'll see multiple examples of data pipelines
and what they can produce.
Okay Cloud Gurus,
for now we've covered the major high-level concepts
and we're ready to move on to the next chapter.
I'll see you next time.

```

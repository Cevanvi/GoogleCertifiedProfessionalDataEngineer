## Lesson Transcript
```text
Welcome back, Cloud Gurus.
In this video, we'll be taking a look
at some of the options you have
when building a neural network
using TensorFlow with the Keras specification.
We'll cover the construction and specification of layers,
activators, optimizers, and loss measures.
We'll finish by looking at some callback options you have.
Let's get going.
It's easy to add different layers to your neural network.
The Add layer allows you to add a list of inputs.
A Dense layer is a fully connected neural network layer.
A Flatten layer will flatten the input provided,
something like taking a matrix or tensor
and converting it to a vector.
A Two-Dimensional Convolution is a spatial convolution
over an image.
You can easily add an Average Pooling layer
and you can add a Max Pooling layer.
On the right is an example of a portion
of a convolutional neural network with a first convolution
consisting of a 2-Dimensional Convolutional layer,
followed by a 2 by 2 Max Pooling layer.
There 2 additional convolutions.
There are many built-in activation functions.
Let's look at 3 we've already encountered.
You can add a Relu activation function,
a Softmax activation function,
or a Hyperbolic Tangent activation function.
Here, you can see how a Relu activation function
is added to the 2-dimensional convolutional layer.
Optimizers represent the algorithm
that is used to optimize or minimize the loss function.
Some common optimizes include the Adam optimizer,
which implements the Adam algorithm.
There is also the RMS Prop optimizer
and the Stochastic Gradient Descent optimizer.
You can see in the code snippet,
how we add the RMS Prop optimizer when compiling the model.
Here, we've also given the learning rate.
The loss function measures the loss or difference
between a set of predictions and the associated labels.
There is the Mean Squared Error,
the Binary Cross Entropy function,
which is useful for binary classification,
nd there is the Categorical Cross Entropy.
There are many other types of loss functions
that can be used in different scenarios.
In this example,
we work with the Binary Cross Entropy loss function.
Finally, there are a number of callback methods
that can be implemented.
The methods are included in the callback class.
These allow you to execute code at specific points
in the training and prediction process.
We can implement callbacks at a number of points
within the process.
So firstly, during Batch processing
at the beginning or the end of the batch.
We can also implement callbacks during Epoch processing,
so at the beginning or the end of an epoch.
You could use a callback to implement early stopping
to prevent over-fitting.
And finally, you can also implement
callbacks during prediction.
There are a number of other points
where callbacks can be implemented.
Okay, Cloud Gurus, that's it for the video
on building neural networks with TensorFlow and Keras.
I hope you found this video interesting.
If you found it a little confusing,
there's no need to be concerned.
All of these concepts will be demonstrated and explained
in the associated labs.
Keep being awesome Cloud Gurus.
```

## Resources 

- [Introduction to convolutional neural networks](https://cs231n.stanford.edu/slides/2016/winter1516_lecture7.pdf) 
- [Overview of deep learning](https://devblogs.nvidia.com/deep-learning-nutshell-core-concepts/) 
- [Weights and layers](https://machinelearningmastery.com/neural-networks-crash-course/) 
- [Activation functions](https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6) 

## Lesson Transcript
```text
Welcome back, Cloud Gurus.
In this video,
I'll be introducing artificial neural networks,
the structures that allow deep learning.
We'll first take a look at what tensors are,
and then we'll see how they are used within neural networks.
I'll take you through a short Python program
that uses the TensorFlow Keras API,
so you can get a feel for how easy it can be
to create a neural network.
We'll also cover a few other aspects
of deep learning and neural networks.
Let's jump in.
We'll start off by looking
at Scalars, Vectors, Matrices, and Tensors.
A Scalar is simply a number.
It has 0 dimensions.
On a a slightly more technical level, a Scalar is invariance
under transformations of the coordinate system.
A vector is a list of numbers stacked in a column.
If you remember from school maths or physics,
vectors are quantities
that have both magnitude and direction.
The example shown here is a 2x1 dimensional vector,
where the number of rows is given,
and then the number of columns is given.
Vectors have the general form M x 1,
which means you can have many rows but only a single column.
A matrix, on the other hand, is a collection of numbers
that are ranged into rows and columns.
Matrices are able to interact with or change vectors.
In this example here, we have a 2x2 matrix
with 2 rows and 2 columns.
The general structure of a matrix would be M times N,
meaning you can have many rows and many columns.
A tensor is similar to a matrix
but it allows for an arbitrary number of dimensions.
You can think of a tensor as a multi-dimensional matrix.
In this example, we have a 2 x 2 x 4 tensor.
You can think of this as four 2 x 2 matrices stacked
on top of each other.
So, why is TensorFlow called TensorFlow?
This is because tensors represent the flow of information
between the layers of a neural network.
In addition, the input to a neural network
is represented as a tensor.
We could of tensors as having the general form M x N x P,
but as I mentioned we can have an arbitrary number
of dimensions.
Just to finish off, it's worth nothing
that a vector is a special case of a matrix.
Vectors and matrices can be considered
as special cases of a tensor.
Leaving complexities aside,
a scaler can be considered as a special case of a vector.
That is a 1 x 1 vector.
So as I mentioned in the chapter on machine learning,
a deep neural network is simply another way
to create a model.
A model allows you to ask questions with,
and it provides answers, hopefully, the right answers.
So we can train a neural network
to answer questions like this.
Is this a picture of a cat or a dog?
A neural network should classify this as a cat.
This is an example of
an artificial neural network performing classification.
We can also an artificial neural network
to answer questions like this.
How much do expect this new house to sell for?
And some $ value would be predicted.
This is an example
of an artificial neural network performing a regression.
Deep artificial neural networks often perform better
than other types of models. Although they can be harder
to build and train.
Neural networks are composed of a number
of different layers.
Firstly, we have the input layer, this is the layer
that allows us to feed data into the model.
So here, we feed in our image of a cat.
Then there is also an output layer.
This represent the way we want the neural network
to provide answers.
So, for example, in a classification problem,
we'll have a neuron for each of the available classes.
So in this example, we have 5 classes.
Dog, cat, bird, snake, and giraffe.
Here, our network will classify each image
as being either a dog, a cat, a bird, a snake,
or a giraffe.
In between the input and the output layer,
there are number of so called hidden layers.
The number of hidden layers will vary based
on the type of problem you are trying to solve.
Remember, that the number of hidden layers is
an example of model hyperparameter.
It is customary to refer to the input layer as layer 0.
And then have subsequent layers, sequentially numbered
until we reach the output layer.
In this case, our output layer is layer 6.
Since we started at 0, we actually have 7 layers.
Each layer will also have a specified number
of neurons or nodes.
In this example, our input layer has 5 nodes.
And our first layer has 4 nodes.
We need to represent our input data in such a way
that it can be fed into the artificial neural network.
So, for example, a 2 dimensional gray scale white image
can be represented as an N times M matrix.
Specifically, a matrix would represent the number
of pixels in the horizontal and vertical plain.
Each pixel would be represented as a value
within the matrix.
With a byte image, 0 would represent black,
and 255 represents white.
Values between 0 and 255 represents gray pixels.
Remember normalization here, if we values
between 0 and 255, it makes sense to divide all values
by 255 to get elements in an inclusive range
from 0 to 1.
We can have the scenario where every neuron of 1 layer
is connected to every neuron in the following layer.
This is what it refers to as a fully-connected layer.
Fully connected layers are also known as dense layers.
There are other types of layers where every neuron
in one layer is not connected to every neuron
in the adjacent layer.
These are partially connected layers.
These types of layers can play many different roles
within an artificial neural network.
Let's take a look at a super simple Python program.
This Python program builds a neural network
to perform a regression.
Firstly, we import the TensorFlow library along
with the numpy library. Numpy is a Python library
for mathematical operations
that supports multi-dimensional matrices or tensors.
In this step, we are actually building the neural network.
We have a dense or fully connected layer.
In this case, our input is a tensor with only one dimension.
The output layer has a single neuron denoted by the
number of units.
Next, we compile the model,
which essentially prepares it for training.
Here we specify what algorithm will be used
to optimize the model on the training data.
SGD stands for Stochastic Gradient Descent,
which is a variant of gradient descent we discussed in
machine learning chapter.
You also specify the loss. That is how the loss is
to be measured. This is a function that needs
to be minimized.
Here we're setting up our training data.
In this simple example, our data represents the function
f of x equals 2 x minus 1.
Each training sample has a single input feature
that is the train x array.
Each sample also has a single associated label.
The corresponding entry in the trained Y array.
So let's take one. If we take one, we multiply by two,
and we minus one, we have one, which is the first entry
in the train Y array.
Let's look at 3 in the train X array.
If we take 3 multiplied by 2 and subtract 1, we have 5.
This is the third entry in the train Y array.
We use model.fit to actually train the model.
We specify the number of epochs, that is the number of
times our training data will be used to train the model.
And finally, we use our trained model to make a prediction.
We use the predict method to determine
what our output value, expected value should be
if we feed in an X value of 6.
We know that 6 times 2 minus 1 is equal to 11.
However, the predicted value is 10.999995
So the prediction is not exact.
Remember, that the model is a learned approximation
arrived at making very small adjustments.
It's not an analytical solution to our problem.
So this was a really easy example.
We know the underlying function
that represents the relationship
between our features and labels.
In the real world, you will not know this relationship.
And there will be many more features,
making the relationship
between inputs and outputs pretty complex.
I use this example to try and demonstrate the syntax
and flow of the program.
This neural network can definitely be improved.
So we looked at the layers of the neural networks.
Let's focus in on one specific neuron.
We want to understand what structure the neuron has,
and what job it performs.
Each neuron will normally have an input vector X,
and an output value Y.
For simplicity's sake, we are just showing this input vector
as having 2 components, X1 and X2.
The neuron will also be associated with a weight vector.
The number of weights in the weight vector will correspond
to the number of inputs in the input vector.
There is a multiplication operation where
each weight is multiplied by the corresponding input value.
Once all the weights have been multiplied
by the corresponding input values,
these values are summed.
So here have a X1 times W1 plus X2 times W2.
Remember, that if we had more X values for our inputs
we'd have more terms in this equation.
And finally, a function is applied to these summed values.
This gives our output Y.
The multiplication and sum operations can be
represented as the dot product between the weight
and input vector.
This can be conveniently depicted as
the weight vector transposed multiplied by the input vector.
You don't need to know this for the certification exam.
It's just for your interest.
The function,
that we applied to those summed values,
is known as the activation function.
In the last slide, I mentioned the activation function.
Activation functions are generally non-linear functions
that introduce non-linearities into the network.
This simply allows neural networks
to learn more complex relationships
between the features and the labels.
If there were no activation function in our network,
then we would only be able to work with straight lines.
And of course, this is very limited.
In this example, showing here,
there is no way
to have a straight line separating the different colors.
We need something like an oval, which is highly non-linear.
There are number of different types of activation functions.
Three common ones are the rectified linear units or RELU,
there is the sigmoid or logistic activation function,
and there is the hyperbolic tangent activation function.
I've provided you with the actual equations
for these activation functions.
Again, this is for your interest.
You would never need to know this
for the certification test.
Let's finish off by looking at a specific type
of activation function in a little more detail.
This is the softmax function.
Remember the output layer from our previous example,
where we categorized images as being of a dog, a cat,
a bird, a snake or a giraffe.
When we feed in an image, we'd have something
like the values depicted here as outputs for these neurons.
These are known as logits.
In this case, the logit depicting the dog
has the highest value, and so we would say that
the image was most likely a picture of a dog.
However, we want to find a way to transform these values
so that they represent a set of probabilities.
Remember, probabilities sum to 1.
So, want a transformation that takes the logic values
and converts them to probabilities,
which do sum to 1.
Here we see that the probability that the image is a picture
of a dog is 0.95 or 95%.
You'll see that the probabilities now all sum to 1.
The function that we used to transform the logic values
to probabilities is the softmax function.
That's it for this video Cloud Gurus.
I hope you found it informative.
We'll be taking a look
at artificial neural network architectures next.
See you in the next video.
```

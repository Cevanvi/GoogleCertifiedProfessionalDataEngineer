## Lesson Transcript
```text
Hello Cloud Gurus, in this video
we are going to break down the rather lengthy, eclectic,
and some may say deliberately vague exam guide
that Google have published for the data engineer exam.
Warning, the following video contains walls of text.
The exam guide itself is 4 domains, 15 sections,
and each section contains multiple learning objectives.
There's unfortunately no visually engaging way
of going through this material other than line by line.
But the exam guide is out there, so we should analyze
the text in full.
For each objective, I'll give you my opinion
of what the exam expects from you,
which should hopefully make you more prepared to pass it.
Okay, let's jump straight into it.
The first of our 4 domains in the exam guide
is designing data processing systems.
Let's have a think about what Google mean by that.
A data processing system is something
that for a defined set of inputs produces
a defined set of outputs.
There are thousands of inputs and outputs
you could apply this to, one example could be
the customer transactions are inputs
and shopping trends are outputs.
It really could mean anything, but at a high level
we're talking about designing a system
that takes in data of some sort and turns it into data
that is useful to us.
So under this, we have our first section,
selecting the appropriate storage technologies.
The exam guide asks us to consider the following,
mapping storage systems to business requirements,
data modeling, tradeoffs involving latency, throughput,
transactions, distributed systems, and schema design.
They're starting us off gently here,
this is pretty self-explanatory.
A big part of being a data engineer
is choosing the correct storage systems,
which obviously need to meet business
as well as technical requirements.
Google want you to be aware of the basics of data modeling,
because this will inform the decision you make
about which system to use.
These tradeoffs between latency and throughput,
are little hints to a decision tree designed by Google
for choosing between storage systems.
We covered this and how to make these decisions
in our chapter on storage and database systems.
Note here that there's no particular order
to these bullet points they can jump around,
and as we work our way through you may feel like
some are being repeated.
That just seems to be the nature of the way
the exam guide was written.
The next section is designing data pipelines.
So we need to grasp what a pipeline is
in the context of a data processing system.
If you're familiar with CI pipelines
in the software industry,
it's not an entirely dissimilar concept,
the idea is to get from A to B.
The starting point A might be your raw data,
and the endpoint B might be your data
organized, categorized, and analyzed.
The steps that take you along the way comprise the pipeline.
Google asks us to consider basically
a bunch of their products.
Data publishing and visualization, e.g., BigQuery,
batch and streaming data, e.g., Cloud Dataflow,
Cloud Dataproc, Apache Beam, Apache Spark,
and Hadoop ecosystem, Cloud Pub/Sub, Apache Kafka,
online interactive versus batch predictions,
job automation and orchestration, e.g., Cloud Composer.
They throw a lot of these at us all in one go,
that's because this is a very broad learning objective
that basically says you should know
at a high level at least, what all of these products can do
and where they might fit in a data pipeline.
We covered most of these with their own chapter,
so by now you should have an understanding
of how they can be used to design a data pipeline.
Note that Google also highlight batch
and streaming models of ingestion,
so remind yourself of which services
are best for each of these approaches.
It's interesting to note here, that Google call out
some products from the wider ecosystem,
Beam, Spark, and Kafka for example.
There was a theme in the exam that a lot of solutions
will be based on ingesting data from
or migrating users away from these open source systems.
Next up, designing a data processing solution.
Solution is one of those words
that can mean different things,
arguably, this is the same thing as designing
a data processing system.
But when Google says solution,
what they mean is solution architecture.
So they ask us to consider choice of infrastructure,
system availability and fault tolerance,
use of distributed systems, capacity planning, hybrid cloud,
and edge computing, architecture options,
e.g., message brokers, message queues, middleware,
service-oriented architecture, serverless functions,
at least once, in-order, and exactly once event processing.
There's quite a few high-level concepts
thrown into this one.
And as I say, Google are asking you to think
about the overall architecture
of a data processing solution.
You may need to consider a choice of compute platform
and build up from there, and if you do,
you'll need to think about availability and fault tolerance.
Availability can be achieved
using Google's distributed regions and zones,
which is where distributed systems come into play.
And capacity planning means you'll need to think about
how you would calculate, how much to compute
to throw it a problem.
These architecture options are also some useful pointers,
we'll talk about message brokers and queues
in our chapter on Pub/Sub, which can be used
to loosely couple services together,
including serverless functions
to form a data processing system.
This last point on event processing
almost seemed slightly out of place here,
as it's less of a broad systems architecture topic,
but again, it's another gentle nudge towards Pub/Sub
and the different models of subscriber
and publisher available.
At the end of this domain we have the final section,
migrating data warehousing and data processing.
Considerations include, awareness of current state
and how to migrate a design to a future state,
migrating from on-premise to cloud,
Data Transfer Service, Transfer Appliance,
Cloud Networking, and validating a migration.
So on the surface, this one seems pretty lightweight,
it's simply about how to migrate data
from where it is now, probably a physical data center
or a different cloud provider, to where we want it to be,
obviously Google cloud.
There are various transfer services here,
which we covered in our storage and databases chapter,
but there's a bit more to it than that.
Recall how Google have already name-dropped
some of the open source products in the big data ecosystem,
like Spark and Kafka.
You'll need to know how to migrate
from these things as well,
into their corresponding GCP products
especially if your case study tells you that your customer
is seeking managed solutions wherever possible.
Okay, we're into our second domain,
building and operationalizing data processing systems.
Where Google expects us to get more hands-on
with the systems we've designed and actually build them.
You'll see them use this word a lot, "operationalizing"
and I checked it's not made up,
it's basically making you aware that you will need
to operate the system that you have built.
So the first section is building
and operationalizing storage systems.
And Google wants us to consider,
effective use of managed services,
Cloud Bigtable, Cloud Spanner, Cloud SQL,
BigQuery, Cloud Storage, Cloud Datastore,
Cloud Memorystore, storage costs and performance,
and lifecycle management of data.
These topics are all based around storage,
choosing the best storage option
for the data you're dealing with
and managing the lifecycle of that data.
There are decisions and compromises you can make
that will affect cost,
such as the classification of Cloud Storage,
as well as performance, which is how you'll choose
between similar products.
The key here is to read the exam question very carefully,
of the answers you are given to choose from,
several may seem correct, but only one will match every clue
you are given in the question.
We covered making these decisions
in our storage and databases chapter.
It's worth noting here, that references to Cloud Datastore
in the exam may not yet have been updated
to Cloud Firestore, but the knowledge you have
on these should be interchangeable.
Next up in this domain we have this section,
building and operationalizing pipelines.
You may have noticed a pattern here,
section 1.2 was really about pipelines in theory,
and in section 2.2, we're now talking
about pipelines in practice.
Considerations include data cleansing, batch and streaming,
transformation, data acquisition and import
and integrating with new data sources.
So again, this is all about how we get data
into our platform and into a format we can use.
Data cleansing means making sure the data
is in a sensible form that we can read,
this covers not just the format of the data itself,
but if any data is missing
or if any data needs to be transformed.
We need to think about building systems
that can handle batches of data that may be pushed ad hoc
or at a scheduled time, as well as streaming systems
that are designed to continually ingest data.
These last 2 points are about how
when you add new data sources you should think about
how you acquire a backfill of existing data
to import into your system.
How you approach each of these tasks
will depend on the tool you are using.
But again, we'll cover the options
when we get to those tools in their respective chapters.
Okay, we're onto building
and operationalizing processing infrastructure.
When we theorize this stuff in section 1.3,
it was a solution,
now we're actually building it, it's infrastructure.
Considerations include, provisioning resources,
monitoring pipelines, adjusting pipelines,
and testing and quality control.
These are written as rather broad objectives.
A data engineer doesn't need to be an infrastructure expert,
but you do need to know the benefits
of certain design choices for provisioning resources,
such as managed versus unmanaged, and the pros and cons
of geo locating certain resources.
For monitoring and adjusting pipelines,
these are going to be specific to the products
and services you're using--data flow for example.
Testing and quality control are again high-level concepts,
but they're good things to bear in mind
when answering questions in the exam.
For example, how can you ensure
that the processing infrastructure you're building
is going to maintain the integrity of your data?
Again, it's an abstract question,
but it might just become relevant in the exam
and point you towards the correct answer.
We're into domain 3 now,
operationalizing machine learning models.
It sounds scary, but the keyword is operationalizing.
Funnily enough, Google doesn't expect its data engineers
to be PhD machine learning scientists,
although you will need to know
some machine learning fundamentals.
And yes, I'm aware robots and machine learning
are 2 different things,
I just thought this was a pretty cool picture.
The first section is quite simple
to ease you into this domain,
leveraging pre-built ML models as a service.
Considerations include, ML APIs,
e.g., Vision API and Speech API,
customizing MLA APIs, e.g., AutoML Vision, also ML text
and conversational experiences, e.g., Dialogflow.
Each of the ML APIs provided by Google,
are pre-built machine learning models
that have been designed and trained to perfection,
that you can simply consume as an API.
These models can be further customized with AutoML,
and Dialogflow can be used to create interactive voice
and text-based services like voice assistance and chat bots.
For each of these you'll need an understanding
of when to utilize the product
and their basic features and capabilities.
Okay fear not Cloud Gurus, we're past halfway now
onto deploying an ML pipeline.
Considerations include, ingesting appropriate data,
retraining of machine learning models,
Cloud Machine Learning Engine, BigQuery ML, Kubeflow,
Spark ML, and continuous evaluation.
It's important that data used to make predictions
should have a similar structure and distribution
to data that was used to train the models.
Over time, there are often changes that occur to data,
and this can result in significant degradation
in the performance of ML models.
This changing in the underlying structure of data
is called concept drift.
To overcome this, models need to be continuously evaluated
and retrained when performance has degraded
to an unacceptable level.
A number of tools and technologies can be leveraged
to help manage the end-to-end pipeline,
which can and should include retraining.
These include, BigQuery, AI platform,
TensorFlow Extended and Kubeflow pipelines.
Next up, choosing the appropriate training
and serving infrastructure.
Considerations include, distributed versus single machine,
use of edge compute and hardware accelerators,
e.g GPU, and TPU.
Hardware plays an important role in training ML models
and in serving those models.
The optimal hardware for training ML models
depends very much on the model that is being trained.
TPUs can significantly speed up training for some models.
Models can also be trained on multiple machines
in order to expedite the process
which can take days or weeks on a single machine.
The AI platform supports
distributed training with containers.
Train models can be deployed to on-premises machines,
or they can be deployed to edge services.
This can make ML fast and cost effective.
Finally in this domain, measuring, monitoring,
and troubleshooting machine learning models.
Considerations include, machine learning terminology,
e.g., features, labels, models, regression, classification,
recommendation, supervised and unsupervised learning,
evaluation metrics, impact of dependencies
on machine learning models, common sources of error,
e.g., assumptions about data.
Yup, that's a lot, but without a basic understanding
of these concepts, it's not really possible
to understand ML, each of these topics is deep,
but it's not difficult to gain a sufficient level
of understanding to make ML understandable
or to pass the certification.
Make sure you get the key concepts
and the big picture will emerge.
Now our final domain, ensuring solution quality.
Most of this domain will deal with best practices approaches
to using the tools we have now learned how to use.
The first section is Designing For Security And Compliance.
Considerations include, identity and access management,
e.g., Cloud IAM, data security,
encryption and key management.
Ensuring privacy, e.g., Data Loss Prevention API.
Legal compliance, e.g., Health Insurance Portability
and Accountability Act,
Children's Online Privacy Protection Act,
FedRAMP, General Data Protection Regulation.
Hopefully you've just finished the chapter
on security and regulation,
but you need to think about identity and access management
and every part of the exam.
You need to think about the security principle
of least privilege and assigning only
the necessary IAM roles for every GCP product and service.
Moving on, we have Ensuring Scalability And Efficiency,
broadly speaking that means,
can a solution grow to meet demand
and does it operate in the most efficient way possible?
This could also be read to mean,
is a solution design wasting any money.
Considerations include, building and running test suites,
pipeline monitoring, e.g., Stackdriver,
assessing, troubleshooting,
and improving data representations
and data processing infrastructure,
and resizing and autoscaling resources.
This is another set of high-level principles,
we'll quickly go through them one by one.
It's curious that Google mentioned test suites here,
but it's probably more for awareness than anything else.
These are going to be a function
of whatever software framework is being used,
but that generally isn't a consideration
you need to worry about when designing infrastructure
at the level expected by this exam.
For pipeline monitoring
you should be familiar with Stackdriver.
You'll use this to monitor and check logs
for various compute jobs and pipelines.
This next point, assessing, troubleshooting,
and improving data representations
and processing infrastructure,
to be honest, it's just a rewording of objectives
that have already been stated.
I'm not sure if Google were just trying
to pad out this section, possibly they are just nudging you
to take a second look at your design choices
to make sure they are as efficient as possible.
And while you're checking your infrastructure,
make sure it meets this requirement of scalability as well.
If demand on a particular workload can fluctuate,
have you picked a product or service that can auto scale
or otherwise be resized easily to meet that demand?
Conversely, are you over provisioning for workloads
that may be guaranteed to remain stable?
Next, Ensuring Reliability and Fidelity.
Google are really sticking to the high level concepts here,
but they do go together.
If your system becomes unreliable, it's likely to affect
the fidelity of your data,
that is, it's exactness or accuracy.
Let's go through the considerations,
performing data preparation and quality control,
e.g., Cloud Dataprep, verification and monitoring,
planning, executing, and stress testing data recovery,
fault tolerance, rerunning failed jobs,
performing retrospective re-analysis,
and choosing between ACID, idempotent
eventually consistent requirements.
Data preparation is a subject they touched on
early in the exam guide.
They've also mentioned verification and monitoring before.
Stress testing is an important topic
you'll need to consider as a data engineer.
When you're designing a system, either for data ingestion,
processing, or analytics, you need to consider
how that system will respond
to things like faults and outages.
Can data be backfilled for example?
Can any components be implemented
in a fault tolerant manner?
Finally, this last point brings us
almost right back to database design.
You need to consider based on your data model
and requirements, which sort of database
or storage solution is going to suit your needs.
As they may each have different distributed
consistency models or may
or may not support ACID transactions.
And finally, ensuring flexibility and portability.
Considerations include, mapping to current
and future business requirements,
designing for data and application portability,
e.g., multi-cloud, data residency requirements,
data staging, cataloging, and discovery.
Again, Google really wants to hammer home
that business requirements carry as much weight
as technical requirements.
These may include things like a desire
to remain cloud agnostic or embrace multi-cloud.
There may be data residency requirements,
which bring us back to some
of those regulatory compliance frameworks,
which means you need to think again
about the physical location of your data and workloads,
and if that may impact latency or performance.
Finally, data staging, cataloging, and discovery.
Well, these topics cover a wide range
of different activities,
but in the context of the data engineer cert requirements,
you should be able to plan how you would discover the data
an organization has, how it could be cataloged
and then prepared for import into GCP.
Well done Cloud Gurus for making it this far.
Now if you're still with me after those walls of text,
you may be feeling a little overwhelmed.
Don't despair, hopefully this picture of a sleeping puppy
will help calm you.
But it's okay, the domains in this exam guide
might feel like they add up to a vast knowledge
required to pass the exam.
But believe me when I say, that you already know
most of these concepts
from studying the chapters in this course.
This has just been an academic way
of presenting the concepts.
And hopefully as we went through the objectives,
your brain started suggesting some solutions to you,
like "hey, I recognize that,"
or "hey, I know how I would solve that," thanks brain.
Just keeping awesome Cloud Gurus,
and I know you can pass that exam.
```

## Resources 

- [GCP Solutions: Big Data](https://cloud.google.com/architecture#/technology=Big_data_and_analytics) 

## Lesson Transcript
```text
Hello, Cloud Gurus.
To help us prepare for the exam,
we're going to start looking at some of Google's
official published reference architectures
available on the amazing GCP solutions website,
which you can find a link to in the resources section.
Talking through these references will hopefully help you put
everything you've learned into context
and see how real world systems are built.
In this first video,
we'll concentrate on reference architectures for big data.
So here we are on the GCP Solutions website.
To find reference architectures for big data,
we'll scroll down to the smart analytics section
and click Big Data.
Let's start by taking a look
at the data warehouse modernization design.
The purpose of this design, as you may have guessed,
is to create a modern data warehouse
capable of large scale streaming and batch data processing
and storage, that will support analysis
with a variety of popular analytics
and reporting tools.
In this example, we have a variety of data sources.
Back office systems, point of sale systems,
user-facing applications, weather data, and social media.
Depending on the design of these systems,
Google presents 2 different methods
to begin the ingestion of their data.
Option 1 is used when one of these systems
doesn't have a direct integration with GCP.
In this case, we build an ingestion system
and deploy it with App Engine.
This could be something as simple as an HTTP endpoint
that we can configure our external systems, to send data to.
our App Engine endpoint can then write the data
to one of these storage systems or databases,
depending on the type of data it's receiving.
For wide column NoSQL data we can write to Bigtable.
For globally scalable SQL data,
we can write to Cloud Spanner.
Although Cloud SQL might be another option
for smaller deployments.
If we're handling JSON documents, we have cloud Datastore,
and if it's just binary or unstructured data,
we can use cloud Storage.
Alternatively, it might be that our external systems
can push messages directly to cloud Pub/Sub.
In this case, we buffer messages in Pub/Sub,
then use a pipeline in cloud Dataflow
to process the messages
before pushing them to one of the storage
or database systems, we just discussed.
Our pipeline could optionally perform some transformation
or ordering of messages to make them more appropriate
for our chosen storage system.
From here, we need to get our data
from our simple storage systems
into our data warehouse, BigQuery.
We've seen several examples of how this can be done directly
or using further Data Flow pipelines.
In this reference architecture,
we are running a further Spark job on the data
before ingesting it into BigQuery.
Our Spark job is managed by Cloud DataProc
and it's orchestrated by Cloud Composer.
That means we can automate when want the job to run
perhaps based on a regular interval or some other trigger.
Composer can also automate queries directly in BigQuery,
perhaps to produce shared views.
Finally, we get everything into BigQuery.
From here we have many options for offering this data
up to our analysts.
First, we can have users interact directly
via the BigQuery UI itself,
or we can explore the data in Data Studio
or create notebooks that use the data in DataLab.
BigQuery itself is also supported as a source
for many third-party data analysis and visualization tools.
Other custom or legacy tools can be connected to BigQuery,
via its rest API or database drivers.
Let's go back and look next at the Data Lake
reference architecture.
Here, the solution is designed to process stream
and batch data, providing opportunities for data, mining
and exploration, then offer the refined data
for machine learning, training and prediction.
The Data Lake approach supports all of these activities.
Our data sources in this example include sensors,
on-premises devices,
the capture of user activity within applications,
as well as an online transactional processing system.
Perhaps some legacy database.
The data is gathered in one of several ways.
It can be stored directly in cloud storage
or ingested in batches using transfer appliance,
transfer service, or just good old GSutil.
For stream processing,
we see the common combination of Pub/Sub
and Cloud Data Flow.
In this example, our Data Flow pipeline,
as well as storing its transformed output in cloud storage
is also writing to a real-time store in cloud Big Table
or cloud Spanner, depending on the type of data.
For data mining and exploration,
we can use cloud Datalab and cloud Dataprep.
As we've seen Datalab can be used to create
interactive Jupyter notebooks that can aid in the study
of ingested data
and even the creation and training
of preliminary machine learning models.
That's why we also see cloud Datalab
here in the ML training section,
where we can interact with Cloud ML Engine
to train prediction models.
prediction jobs can then be run via Cloud ML Engine
writing data out to cloud Bigtable.
Cloud Dataprep can be used to clean up, organize,
categorize, or otherwise prepare data
to make it easier to use.
This could include changing the format of some data,
reorganizing a schema or filling in missing data.
Not shown in this diagram is how a bespoke data prep recipe
can be exported as a Data Flow pipeline
and then automated with the Data Flow service.
If we need to export data
to a more traditional data warehouse,
we can ingest directly from cloud storage to BigQuery
or process data into a structured form
using Hive and Spark jobs in cloud Data Proc.
Let's take a look at one more.
The real-time inventory solution.
This one is interesting because it contains
quite a few custom components.
You don't need to know how to create these for the exam
just where dropping in a custom component
like something running in AppEngine makes sense
to an overall solution here.
Here our solution needs to support our retail store,
where our points of sale system needs a real time view
of inventory,
as well as the ability to make changes to that inventory,
as sales are processed.
Separately to this front of store activity,
we need to integrate the data with other Google services,
such as AdWords
to make sure we are maximizing our marketing investments
and provide data to other proprietary back office systems,
perhaps for logistics and supply invoicing.
Our point of ingestion is a custom App Engine application,
which provides the backend to our point of sale system,
capturing changes as they occur.
These are then streamed through cloud Pub/Sub,
which provides us with scalability and resilience.
From here, we have 2 subscribers.
One is a further custom App Engine application,
which takes the inventory update messages
and interacts with our proprietary back office applications.
The other is a cloud Data Flow pipeline,
which is responsible for processing the events,
for storing them in a time series database
in cloud Big Table,
and then writing them to a Big Query warehouse
for later analysis.
The Data Flow pipeline also updates inventory counts
in a cloud SQL database,
which is also read by our services application.
There's a more detailed view of the Data Flow pipeline here.
Once the data is collected, windowing is applied
so that aggregate events can be written to BigQuery
and inventory counts can be updated in cloud SQL.
At the same time,
the time series data is written out to Big Table.
So this is a great example of how you can take a data source
and manipulate it to suit multiple use cases.
In this solution, we've used the data
to power the point of sale system itself
with real-time inventory updates,
as well as provide guaranteed,
aggregated inventor information
to the financial systems in our back office.
We've also taken the same data
to store a time series history,
which we could potentially use to train a prediction model,
a very common scenario in retail environments.
And we provided the data in BigQuery,
making it easier for our data analysts
to perform their own investigations,
which in turn could help them provide input
for those prediction models.
Okay, Cloud Gurus, we'll leave it at those 3,
but I encourage you to browse through the other solutions
in your own time.
Join me in the next video,
where we'll look at some of the reference architectures
for AI and machine learning.
```

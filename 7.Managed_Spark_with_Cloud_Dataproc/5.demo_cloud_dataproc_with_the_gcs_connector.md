## Resources 

- [Project Gutenberg](https://www.gutenberg.org/ebooks/2600) 
- [wordcount](resources%2Fwordcount.py)

## Lesson Transcript
```text
Hello Cloud Gurus,
and welcome to our next Dataproc lab.
In this lab, we're going to demonstrate
one of the great advantages Dataproc has
over standalone Spark clusters
using the GCS connector for storage
instead of HDFS on the cluster nodes themselves.
So in this lab, we'll create a new cloud Dataproc cluster,
but this time, rather than use an example,
we'll write our own PySpark job,
and submit it via the command line.
And rather than using our cluster's own storage,
we'll use the GCS connector
to read and write data from Cloud Storage.
To follow along with this lab,
you'll need a GCP project,
access to the cloud console,
and we'll use the Cloud Shell
and Cloud Shell Editor
to write and submit our PySpark job.
I'm going to be using a text from Project Gutenberg,
a free resource of public domain books and other works.
Follow the link in the Resources section
to download the plain text version
of "War and Peace" by Leo Tolstoy.
Or feel free to choose a different book if you'd like.
Once you've got the file ready,
join me in the GCP console.
So back in the GCP console,
let's first create a GCS bucket.
We'll select Storage from the menu.
And click Create Bucket.
Name your bucket something unique.
Again, it's always a good idea to use your project name
as a prefix.
I'm going to call mine tim-acg-dataproc-texts.
Make a note of this bucket name,
as you'll need it in a moment.
Click Continue and choose to make your bucket regional.
Then select the closest region to you.
For me, that's europe-west2.
Remember which region you've chosen here,
as we'll use it again in a moment
when we create our cluster.
Then scroll down and click Create.
Inside the new bucket, create a folder called input.
Then go into this folder
and upload the text file
that you've downloaded from Project Gutenberg.
Now it's time to create our Dataproc cluster.
Select Dataproc in the Big Data section of the menu.
The Dataproc API should already be enabled here
if you've completed the previous lab,
but if you're prompted to enable the API, just do so.
Click Create cluster.
This time, we'll give our cluster a friendly name,
labcluster.
Pick the same region here
that you used to create the GCS bucket a moment ago.
So for me that was europe-west2.
We'll change the cluster type to Single Node.
We'll leave everything else as the default,
and click Create.
Note we don't have to take any extra steps
to install the Cloud Storage connector,
it's included by default.
We just have to choose to use it when we submit jobs.
While that's creating, let's open the Cloud Shell.
And the Cloud Shell Editor.
Now we'll write our Spark job in Python.
Create a new file called word-count.py.
You can pause the video here to catch up with my typing
or copy the file from the Resources section.
This is a very simple Python script.
It takes two arguments,
an input location and an output location.
It then creates a Spark context,
and uses this to perform some map and reduce functions.
Then it saves the output to the location specified.
Let's save that file.
Now we've written our PySpark job,
we're ready to submit it.
Let's quickly check that our cluster is up and running
by going back to the Dataproc window.
Looks good.
Let's go back to the Cloud Shell.
To submit the job, we'll use the gcloud command line.
The command is gcloud dataproc jobs submit.
Then the type of the job, which is pyspark.
Then the name of the script that we've just written,
word-count.py.
Then our cluster name, --cluster=labcluster.
Then the region you built your cluster in,
-region=, and again, for me, it's europe-west2.
Then two dashes on their own,
and that means that everything after this point
will be submitted as arguments to our script.
So first we give it the name of the input file.
Make sure you use your bucket name and input file name here.
And finally, the output directory name in the same bucket.
We haven't created this output directory yet,
but Dataproc will do that for us.
When we hit Return, the job will be submitted.
After a few moments,
the console will start to stream the output of the job
as it runs on the cluster.
We can also go back into the cloud console
and see the output in the Jobs section of the Dataproc page.
Now, don't panic if you see any errors here,
this might be due to a bug in HDFS,
but we can safely ignore it
as we're using Cloud Storage for everything.
It shouldn't take long for the job to complete,
but we don't see the results in the job output
because we asked for them to be written to GCS.
Let's go back to our Cloud Shell and take a look.
I'll just clear the terminal.
We can list the contents of our output directory
with gsutil ls.
As we can see, the output is partitioned into two files.
Let's download them to our Cloud Shell terminal
with gsutil cp.
We can join the two files together
with the Unix cat command.
Now the file part 00000 contains all of the output.
Let's see what the most common words in the text were.
I'll just clear the terminal.
We can do this with the Unix sort command.
This will sort the output file
containing all of the word counts
so that we can see what the most commonly used words are
in the text.
Hmm, lots of military references
and lots of things galloping.
It's not the most exciting thing you can do with Dataproc,
but we've used Spark to complete a word count
on a novel that's over half a million words long
without having to set up or configure a cluster.
And now we've finished,
we can simply delete our Dataproc cluster.
Then we'll delete our Cloud Storage bucket,
as it's a billable resource.
Although feel free to hang onto it if you want to.
Great job, Cloud Gurus.
If you've got any questions about this lab,
please let me know in the course forums.
I'll see you in the next video.
```

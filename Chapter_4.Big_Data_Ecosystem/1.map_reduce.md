## Resources 

- [MapReduce research paper](resources%2Fmapreduce.pdf) 

## Lesson Transcript
```text
Hello Cloud Gurus.
Welcome to this chapter where we're going to look into
MapReduce, Hadoop, and related technologies.
These are some of the fundamentals of big data that
originated in the early days of Google and spawned much of
the open source,
big data ecosystem in use today while they're not GCP
products that will be explored in depth in the exam.
It's good to have a working knowledge of these tools,
especially for any questions that involve migrating from
open source on-premises systems into the cloud.
So to start with in this lecture,
we're going way back to 2004 to learn about MapReduce.
So let's start by asking what is MapReduce?
It's possible that this is a term you've heard of every now
and again, without really ever finding out what it means.
Well, it's actually 2 things.
It's a programming model.
That means it's a designed for how you can write computer
programs, using 2 specific functions, map,
and reduce and how they are combined to perform a task.
We'll deep dive on these in a moment,
but it's also an implementation of that model. Specifically,
it's a distributed implementation or a way of running those
MapReduce programs across clusters of hundreds or thousands
of computers.
It was created at Google to solve problems they were having
when trying to analyze large data sets for various purposes.
Prior to MapReduce, there were multiple approaches employed,
and most of them combined the actual logical processing they
were trying to accomplish along with the complex work of
distributing that processing across many computers,
MapReduce provided a common framework that could be used for
a large variety of problems, but also abstracted away
all of the distributed systems management work. Like the
good scholars, they are Google wrote up the R and D.
They had put into developing that produce,
which was published in 2004,
sixth symposium on operating systems,
design and implementation.
So at a high level,
you can think of the MapReduce model like this.
The map function takes an input from the user and produces a
set of intermediate key value pairs.
The keys in these pairs are elements of the input we've
given our function.
And in a simple example, the value is just one,
because every time that element is mapped, we count it,
it intermediate just means we're not done yet.
We're going to pass these key value pairs to the reduced
function.
The purpose of the reduced function then is to merge the
intermediate values
we pass to it with other values that have the same key.
In most cases,
this means we're going to end up with a smaller set of
values hence the term reduce.
It's easier to explain this with a demonstration.
Let's see a representation of a simple problem to solve
counting the words in a document.
Let's imagine we have an input file,
which contains 3 lines of text.
It's not the most interesting document in the world,
but that's not the point. To run this through our MapReduce
program, we first split the file into separate lines.
Then we split each line into separate words and output the
key value pair.
The key is the word and the value is 1 because it
represents 1 occurrence of the word.
This is our map function.
We are mapping this word by producing this key value pair.
The clever part comes next.
When we reduce all those mapped key value pairs to combine
the ones with matching keys.
This gives us our final result, which is our word count.
All of these arrows are making my eyes hurt,
so let's remove them, but here's our final result.
We have 2 cats, 2 dogs, 2 birds,
and surprisingly 3 frogs.
But this is just one example of how the MapReduce model
can be used to solve a problem. Let's see some more.
In this example, we'll create an inverted word index.
That means we'll create an index of key value pairs where
the key is a word
and the value is a list of all the documents
where that word appears. We start with our documents,
which are split and read one by one.
Each word in each document is mapped.
And the intermediate key value pair is the word followed by
the reference to its originating document.
These are then reduced to give us our final index.
This is a much simplified version of a real problem.
Google had to solve as part of the web indexing required for
Google search.
Another large dataset problem Google had to solve was how
web pages all linked to each other.
Let's take, for example, 2 web pages.
Each webpage contains links to other web pages.
We can map these links to give us our intermediate key value
pairs where the key is the webpage being linked to.
And the value is the origin of that link.
We then reduce those pairs down to give us our final index
where each element is a link and a list of the web pages
that contain that link.
You can start to imagine how the output of a MapReduce job
like this could in fact become the input for a further
MapReduce job.
And that's exactly what Google did.
What you're looking at here is a very simplified version of
the first step of their page rank algorithm for ranking
search results.
So, as we said before,
the MapReduce model gave programmers a standardized
framework to design solutions to their data's processing
problems,
but it also abstracted away all of the computing management,
which previously they had to worry about themselves.
The MapReduce system took care of parallelizing and
executing jobs across hundreds,
or even thousands of machines,
depending on the total computing power required to solve a
problem.
All cluster management was taken care of behind the scenes,
including partitioning jobs,
scheduling them, and dealing with networking storage or
computing faults, Google's MapReduce system split all the
jobs in small chunks up to about 64 MBs of data in the
original implementation,
which were then scheduled on the cluster
using a master and worker model worker jobs were considered
atomic.
The master would periodically health check all worker nodes.
And if a worker node or job failed,
it could simply be reassigned to a different worker before
any output from that chunk was reassembled rated performance
was improved by using local disc instead of network attached
disks for intermediate files on each worker node before the
final results were written to partition files on Google's
own global file system.
At the time of publication of the research paper,
Google had just completed a total rewrite of Google search
and web indexing using MapReduce in 2004,
that index used about 20 terabytes of data.
I shutter to think what it might be up to now. Okay
Cloud Gurus, now we know about the origins of MapReduce.
Join me in the next video where we'll learn about the open
source implementation of MapReduce with Hadoop.

```
